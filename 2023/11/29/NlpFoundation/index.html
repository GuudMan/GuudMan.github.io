<!DOCTYPE html><html lang="zh-cn" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>NlpFoundation</title><meta name="keywords" content="NLP"><meta name="author" content="AI4Future"><meta name="copyright" content="AI4Future"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1、基础如何将计算机不认识的文本特征转化为数字特征 常见的特征分类有类别特征（categorical feature）和数值特征（numeric feature）。 类别特征一般是有限集合， 没有大小之分， 是并列权重。数值特征如：年龄， 工资等， 有数值大小之分。计算机只能处理数值型特征，因此需要将非数值特征转化为计算机能识别的数字， 如下表中的gender和nationality。gend">
<meta property="og:type" content="article">
<meta property="og:title" content="NlpFoundation">
<meta property="og:url" content="https://guudman.github.io/2023/11/29/NlpFoundation/index.html">
<meta property="og:site_name" content="Ai4Future">
<meta property="og:description" content="1、基础如何将计算机不认识的文本特征转化为数字特征 常见的特征分类有类别特征（categorical feature）和数值特征（numeric feature）。 类别特征一般是有限集合， 没有大小之分， 是并列权重。数值特征如：年龄， 工资等， 有数值大小之分。计算机只能处理数值型特征，因此需要将非数值特征转化为计算机能识别的数字， 如下表中的gender和nationality。gend">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg">
<meta property="article:published_time" content="2023-11-29T10:03:47.000Z">
<meta property="article:modified_time" content="2023-11-29T10:14:36.133Z">
<meta property="article:author" content="AI4Future">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://guudman.github.io/2023/11/29/NlpFoundation/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: AI4Future","link":"Link: ","source":"Source: Ai4Future","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://fastly.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://fastly.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NlpFoundation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-11-29 18:14:36'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Ai4Future" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="onerror=null;src='https://gitee.com/guudman/blog_images/raw/master/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">AI4Future</div><div class="author-info__description">Not Only Look Once</div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/GuudMan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2663017379@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/gedan"><i class="fa-fw fas fa-music"></i><span> 歌单</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk"><i class="fa-fw fa fa-heartbeat"></i><span> 时光</span></a></div><div class="menus_item"><a class="site-page" href="/shuoba"><i class="fa-fw fas fa-comment-dots"></i><span> 说吧</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/google"><span> 镜像</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://json.xbyzs.cf"><span> Json格式化</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://draw.xbyzs.cf"><span> Draw画布</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://enkey.xbyzs.cf"><span> EnKey</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ai4Future</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/gedan"><i class="fa-fw fas fa-music"></i><span> 歌单</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk"><i class="fa-fw fa fa-heartbeat"></i><span> 时光</span></a></div><div class="menus_item"><a class="site-page" href="/shuoba"><i class="fa-fw fas fa-comment-dots"></i><span> 说吧</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/google"><span> 镜像</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://json.xbyzs.cf"><span> Json格式化</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://draw.xbyzs.cf"><span> Draw画布</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://enkey.xbyzs.cf"><span> EnKey</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">NlpFoundation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-29T10:03:47.000Z" title="Created 2023-11-29 18:03:47">2023-11-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:14:36.133Z" title="Updated 2023-11-29 18:14:36">2023-11-29</time></span></div><div class="meta-secondline"></div></div></div><article class="post-content" id="article-container"><meta name="referrer" content="no-referrer">

<h4 id="1、基础"><a href="#1、基础" class="headerlink" title="1、基础"></a>1、基础</h4><p>如何将计算机不认识的文本特征转化为数字特征</p>
<p>常见的特征分类有类别特征（categorical feature）和数值特征（numeric feature）。 类别特征一般是有限集合， 没有大小之分， 是并列权重。数值特征如：年龄， 工资等， 有数值大小之分。计算机只能处理数值型特征，因此需要将非数值特征转化为计算机能识别的数字， 如下表中的gender和nationality。gender为二元特征， nationality为多元的类别特征。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128093427069.png" alt="image-20231128093427069" style="zoom:67%;"></p>
<p>国籍表示成1-197之间的整数(全球大概有197个国家)， 但这些整数只是一个类别， 它们之间无法比较大小， 因为这些整数只是类别而已， 并不是真正的数值， 所以需要对国籍做one-hot Embedding。需要注意的是我们这里用数字表示的类别特征从1开始， 因为0要用来保留未知或缺失的国籍，数据库中经常会有缺失数据， 这些缺失的国籍就用0来表示。</p>
<p>对于性别， 用0表示女性， 1表示男性。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128095539327.png" alt="image-20231128095539327"></p>
<p>上面提到， 1-197只代表一个类别， 它们之间无大小之分， 因此需要对国籍进一步做one-hot Embedding。</p>
<h5 id="类别特征转化数字流程"><a href="#类别特征转化数字流程" class="headerlink" title="类别特征转化数字流程"></a>类别特征转化数字流程</h5><p>上面我们将每个国籍与一个数字进行了映射， 因此会得到这样的一个字典{‘US’-1, ‘China’:2, ‘India’:3, ‘German’: 4,…}， 针对197种国籍， 每个用数字代表的国籍又可以映射为197×1的向量， 使用上面的字典查找国籍的index（如index(China) = 2）, 则该197×1的向量的第二维为1， 其他为0。</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;US&quot;:1-&gt;[1, 0, 0, 0,...]</span><br><span class="line">&quot;China&quot;:2-&gt;[0, 1, 0, 0,...]</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>nationality转成one-hot向量后得到的表格如下。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128104946062.png" alt="image-20231128104946062" style="zoom:67%;"></p>
<p>针对表格中人的三个特征（age, gender, nationality）转化为数字后， 可以得到1+1+197=199维的向量，向量中每个数值的含义如下图。</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(35, Male, Chine) = (35, 1, 0, 1, 0, ...)</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128104608977.png" alt="image-20231128104608977" style="zoom:67%;"></p>
<p>为什么要使用one-hot向量来表示呢， </p>
<p>如果不使用one-hot向量而用1， 2， 3， …分别代表US, China, India…， 则在此列运算时，”US” + “China” = “India”， 这显然不合理， 而用one-hot向量表示时， “US” + “China”  = [1, 0, 0, …] + [0, 1, 0, …] = [1, 1, 0,…]。 向量[1, 1, 0, 0…]表示既包含US也包好China的类别信息， 可以更好的表示二者类别特征相加的意思，显然one-hot表示更为合理。</p>
<h5 id="文本特征处理"><a href="#文本特征处理" class="headerlink" title="文本特征处理"></a>文本特征处理</h5><p>在自然语言处理中， 数字都是文本， 文本可以分割成很多单词， 需要把单词表示成数值向量， 每个单词就是一个类别， 如果字典中有1万个单词， 那么就有1万个类别。很显然， 单词就是categorical features, 用categorical features的方法把单词变成数值向量。</p>
<p>文本处理的第一步就是把文本处理成单词， 一段话、一篇文章或一本书都可以表示成一个字符串， 可以把文本分成很多单词，这个操作叫作tokenization。</p>
<p>第一步：文本-&gt;单词： 分词。 tokenization就是把文本变成单词列表。</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">before:</span><br><span class="line">S = &quot;... to be or not to be...&quot;.</span><br><span class="line">after:</span><br><span class="line">L = [..., to, be, or, not, to, be, ...]</span><br></pre></td></tr></table></figure>
<p>第二步： 计算词频， 也就是每个单词出现的次数， 可以用一个hash表来计数， 开始时hash表是空的， 然后按照下面的方式更新hash表。</p>
<p>如果单词w不在hash表中，  这就说明到目前为止文本中只出现过一次单词w， 所以把w加入hash表中， 让它的词频等于1。假如单词w在hash表中， 说明之前文本中就已经出现过单词w了， 需要将它的词频加1。</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">总结如下：</span><br><span class="line">若词w不在hash表中，则add(w, 1)</span><br><span class="line">否则dict[w] = dict[w] + 1</span><br></pre></td></tr></table></figure>
<p>如下图所示</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128111523105.png" alt="image-20231128111523105" style="zoom:67%;"></p>
<p>第三步： 排序， 将字典中的词频从大到小排序</p>
<p>表中最前面的是词频最高的词， 表最后是词频最低的。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128112721467.png" alt="image-20231128112721467" style="zoom:67%;"></p>
<p>然后把词频换成index， 转化完成的hash表称为词汇表vocabulary。词频最高的index是1。</p>
<p>统计词频的目的是保留常用词， 去掉词频词。比如你可以设置保存最常用的前10k个单词。为什么要去掉地频词呢， 有如下三点原因。</p>
<p>1、很多地频词是name entities, 在大多数的应用中， name entities没有任何意义。</p>
<p>2、拼写过程难免出现错误， 低频词还有可能是拼写错误造成的。</p>
<p>3、不希望词汇表vocabulary太大， vocabulary越大， one-hot向量维度就越高。</p>
<p>第一步的tokenization把文本分割成单词的列表， 第二步建立了一个字典（也可为hash表）， 将每个单词映射到一个正整数。下面开始第三步， 对单词做one-hot encoding。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128115000838.png" alt="image-20231128115000838" style="zoom:67%;"></p>
<p>如果有需要的话， 可以将每个index转换成一个one-hot向量。one-hot向量的维度就是字典vocabulary的维度。</p>
<p>上面提到， 字典中的低频词会被删除， 所以有些词在字典中找不到。假如有个错误拼写的单词bi， 这个单词在字典中找不到， 做one-hot encoding时可以忽略这些词， 也可以将其编码为0.</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128115214398.png" alt="image-20231128115214398" style="zoom:67%;"></p>
<h4 id="2、文本处理与词嵌入"><a href="#2、文本处理与词嵌入" class="headerlink" title="2、文本处理与词嵌入"></a>2、文本处理与词嵌入</h4><p>应用的数据集是IMDB<a href="[Sentiment Analysis (stanford.edu">(数据集下载地址)</a>](<a target="_blank" rel="noopener" href="http://ai.stanford.edu/~amaas/data/sentiment/))电影评论数据集，">http://ai.stanford.edu/~amaas/data/sentiment/))电影评论数据集，</a> 该数据集包含了5万条偏向明显的评论， 其中2.5万条作为训练集， 2.5万条作为测试机。label为pos和neg，是一个二分类问题。</p>
<p>1、文本变成序列， text to sequence， 也就是tokenization， 把文本分割成单词， 一个token就是一个单词， 有些应用中， token可以是一个符号。</p>
<p>会有很多考虑， 1、是否将大写变成小写， 正常情况下， 大小写意义相同， 但有些特例，比如Apple表示苹果公司， 而apple表示水果。2、有些应用会去掉停用词stop word。 3、会存在拼写错误的情况， 因此需要typo correction文本纠错。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128175224685.png" alt="image-20231128175224685" style="zoom:67%;"></p>
<p>2、建立字典。首先统计词频， 去掉地频词， 然后每个单词对应一个正整数。 有了字典， 可以将每个单词映射成一个整数， 这样一句话可以用正整数的列表表示， 这个列表就是sequences。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128175445939.png" alt="image-20231128175445939" style="zoom:67%;"></p>
<p>3、Encoding编码</p>
<p>将字典中的单词映射到索引，索引列表就是一个sequence。如果有必要的话， 还可以进一步做one-hot encoding, 将单词表示成one-hot向量。在IMDB电影评论的例子中， 数据是50k条， 每条电影评论可是表示成一个字符串， tokenization（句子切分成单词列表）之后， 每条电影评论都被转化成一个sequence。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129092836076.png" alt="image-20231129092836076" style="zoom:67%;"></p>
<p>这样的话每一条评论都被转化成一个sequence序列。 注意每个单词的索引是在25k条评论tokenization之后放在一个列表中， 经过排序， 截断之后的索引。排名越靠前， 该单词被使用的频率越高。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129093349986.png" alt="image-20231129093349986" style="zoom:67%;"></p>
<p>但是电影评论有长有短</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129095156849.png" alt="image-20231129095156849" style="zoom:67%;"></p>
<p>这样就造成一个问题， 训练数据没有对齐，每条sequence都有不同的长度， 做机器学习训练时需要把数据存储在张量中， 这就要求吧序列对齐， 让每条序列都有相同的长度。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129095701552.png" alt="image-20231129095701552" style="zoom:67%;"></p>
<p>解决方法就是可以固定长度w， 假如这个序列太长， 可切掉前面的单词，当然也可以保留前面的单词。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129095832639.png" alt="image-20231129095832639" style="zoom: 67%;"></p>
<p>如果序列太短的话， 就用padding把长度增加到w。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129095939431.png" alt="image-20231129095939431" style="zoom:67%;"></p>
<p>这样处理之后， 所以序列的长度都是w。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129100018776.png" alt="image-20231129100018776" style="zoom:67%;"></p>
<p>对齐之后的sequence就可以存储到一个矩阵中， 文本处理的过程就是每个词用一个正整数来表示， 下一步就是word embedding。</p>
<p>keras中文本处理流程实例。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129102921411.png" alt="image-20231129102921411"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : func.py</span></span><br><span class="line"><span class="string"># @Time       ：2023/11/29 9:54</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 剔除html标签</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rm_tags</span>(<span class="params">text</span>):</span><br><span class="line">    re_tag = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;[^&gt;]+&gt;&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> re_tag.sub(<span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_file</span>(<span class="params">filetype</span>):  <span class="comment"># 读取文件</span></span><br><span class="line">    path = <span class="string">r&quot;./aclImdb&quot;</span></span><br><span class="line">    file_list = []</span><br><span class="line"></span><br><span class="line">    positive_path = os.path.join(path, filetype, <span class="string">&#x27;pos&#x27;</span>)  <span class="comment"># 正面评价的文件路径</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(positive_path):</span><br><span class="line">        file_list += [os.path.join(positive_path, f)]  <span class="comment"># 存储到文件列表中</span></span><br><span class="line"></span><br><span class="line">    negative_path = os.path.join(path, filetype, <span class="string">&#x27;neg&#x27;</span>)  <span class="comment"># 负面评价的文件路径</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(negative_path):</span><br><span class="line">        file_list += [os.path.join(negative_path, f)]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;read&#x27;</span>, filetype, <span class="string">&#x27;files&#x27;</span>, <span class="built_in">len</span>(file_list))  <span class="comment"># 打印文件个数</span></span><br><span class="line"></span><br><span class="line">    all_labels = ([<span class="number">1</span>] * <span class="number">12500</span> + [<span class="number">0</span>] * <span class="number">12500</span>)  <span class="comment"># 前12500值正面都为1， 后12500是负面都为0</span></span><br><span class="line">    all_texts = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(file_list)</span></span><br><span class="line">    <span class="comment"># 读取所有文件</span></span><br><span class="line">    <span class="keyword">for</span> fi <span class="keyword">in</span> file_list:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fi, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file_input:</span><br><span class="line">            <span class="comment"># 先读取文件， 使用join连接所有字符串， 然后使用rm_tags删除tag最后存入列表all_texts</span></span><br><span class="line">            all_texts += [rm_tags(<span class="string">&quot; &quot;</span>.join(file_input.readlines()))]</span><br><span class="line">    <span class="keyword">return</span> all_labels, all_texts</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_text:  list[string], 一条一条的评论放在一个列表中</span></span><br><span class="line">y_train, train_text = read_file(<span class="string">&quot;train&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;train_text: \n&quot;</span>, train_text[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了方便打印， 这里只查看前2条评论的转化情况</span></span><br><span class="line">train_text = train_text[:<span class="number">2</span>]</span><br><span class="line">vocabulary = <span class="number">1000</span></span><br><span class="line">tokenizer = Tokenizer(num_words=vocabulary)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit_on_texts的过程就是将文本切分成一个一个单词， 建立单词与index之间的索引</span></span><br><span class="line">tokenizer.fit_on_texts(train_text)</span><br><span class="line"><span class="comment"># word_index： 单词与索引的映射字典。dict[(string, int)]</span></span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;word_index: \n&quot;</span>, word_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sequence_train: 将所有的评论中的每个单词映射成list列表， 然后将所有的列表放在一个大列表中。list[list[int]]</span></span><br><span class="line">sequences_train = tokenizer.texts_to_sequences(train_text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;sequence_train: \n&quot;</span>, sequences_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(sequences_train[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(sequences_train[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本对齐</span></span><br><span class="line">word_num = <span class="number">200</span></span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(sequences_train, maxlen=word_num)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(x_train[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(x_train[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">train_text: </span></span><br><span class="line"><span class="string"> Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as &quot;Teachers&quot;. My 35 years in the teaching profession lead me to believe that Bromwell High&#x27;s satire is much closer to reality than is &quot;Teachers&quot;. The scramble to survive financially, the insightful students who can see right through their pathetic teachers&#x27; pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I&#x27;m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn&#x27;t!</span></span><br><span class="line"><span class="string">word_index: </span></span><br><span class="line"><span class="string"> &#123;&#x27;the&#x27;: 1, &#x27;to&#x27;: 2, &#x27;a&#x27;: 3, &#x27;on&#x27;: 4, &#x27;of&#x27;: 5, &#x27;or&#x27;: 6, &#x27;as&#x27;: 7, &#x27;who&#x27;: 8, &#x27;is&#x27;: 9, &#x27;it&#x27;: 10, &#x27;that&#x27;: 11, &#x27;what&#x27;: 12, &#x27;bolt&#x27;: 13, &#x27;i&#x27;: 14, &#x27;and&#x27;: 15, &#x27;for&#x27;: 16, &#x27;like&#x27;: 17, &#x27;he&#x27;: 18, &#x27;where&#x27;: 19, &#x27;bromwell&#x27;: 20, &#x27;high&#x27;: 21, &#x27;in&#x27;: 22, &#x27;once&#x27;: 23, &#x27;if&#x27;: 24, &#x27;be&#x27;: 25, &#x27;streets&#x27;: 26, &#x27;you&#x27;: 27, &quot;it&#x27;s&quot;: 28, &#x27;with&#x27;: 29, &#x27;his&#x27;: 30, &#x27;other&#x27;: 31, &#x27;school&#x27;: 32, &#x27;such&#x27;: 33, &#x27;teachers&#x27;: 34, &#x27;can&#x27;: 35, &#x27;see&#x27;: 36, &#x27;their&#x27;: 37, &#x27;when&#x27;: 38, &#x27;everything&#x27;: 39, &#x27;homeless&#x27;: 40, &#x27;bet&#x27;: 41, &#x27;rich&#x27;: 42, &#x27;do&#x27;: 43, &#x27;comedy&#x27;: 44, &#x27;at&#x27;: 45, &#x27;about&#x27;: 46, &#x27;life&#x27;: 47, &#x27;my&#x27;: 48, &#x27;years&#x27;: 49, &#x27;me&#x27;: 50, &#x27;students&#x27;: 51, &#x27;all&#x27;: 52, &#x27;student&#x27;: 53, &#x27;one&#x27;: 54, &#x27;think&#x27;: 55, &quot;isn&#x27;t&quot;: 56, &#x27;has&#x27;: 57, &#x27;an&#x27;: 58, &#x27;but&#x27;: 59, &#x27;help&#x27;: 60, &#x27;street&#x27;: 61, &#x27;were&#x27;: 62, &#x27;did&#x27;: 63, &#x27;from&#x27;: 64, &#x27;work&#x27;: 65, &#x27;matter&#x27;: 66, &#x27;people&#x27;: 67, &#x27;while&#x27;: 68, &#x27;worrying&#x27;: 69, &#x27;next&#x27;: 70, &#x27;given&#x27;: 71, &#x27;live&#x27;: 72, &#x27;without&#x27;: 73, &#x27;luxuries&#x27;: 74, &#x27;home&#x27;: 75, &#x27;mel&#x27;: 76, &#x27;making&#x27;: 77, &quot;he&#x27;s&quot;: 78, &#x27;by&#x27;: 79, &#x27;molly&#x27;: 80, &#x27;before&#x27;: 81, &#x27;losing&#x27;: 82, &#x27;her&#x27;: 83, &#x27;used&#x27;: 84, &#x27;being&#x27;: 85, &#x27;they&#x27;: 86, &#x27;money&#x27;: 87, &#x27;maybe&#x27;: 88, &#x27;cartoon&#x27;: 89, &#x27;ran&#x27;: 90, &#x27;same&#x27;: 91, &#x27;time&#x27;: 92, &#x27;some&#x27;: 93, &#x27;programs&#x27;: 94, &#x27;35&#x27;: 95, &#x27;teaching&#x27;: 96, &#x27;profession&#x27;: 97, &#x27;lead&#x27;: 98, &#x27;believe&#x27;: 99, &quot;high&#x27;s&quot;: 100, &#x27;satire&#x27;: 101, &#x27;much&#x27;: 102, &#x27;closer&#x27;: 103, &#x27;reality&#x27;: 104, &#x27;than&#x27;: 105, &#x27;scramble&#x27;: 106, &#x27;survive&#x27;: 107, &#x27;financially&#x27;: 108, &#x27;insightful&#x27;: 109, &#x27;right&#x27;: 110, &#x27;through&#x27;: 111, &#x27;pathetic&#x27;: 112, &quot;teachers&#x27;&quot;: 113, &#x27;pomp&#x27;: 114, &#x27;pettiness&#x27;: 115, &#x27;whole&#x27;: 116, &#x27;situation&#x27;: 117, &#x27;remind&#x27;: 118, &#x27;schools&#x27;: 119, &#x27;knew&#x27;: 120, &#x27;saw&#x27;: 121, &#x27;episode&#x27;: 122, &#x27;which&#x27;: 123, &#x27;repeatedly&#x27;: 124, &#x27;tried&#x27;: 125, &#x27;burn&#x27;: 126, &#x27;down&#x27;: 127, &#x27;immediately&#x27;: 128, &#x27;recalled&#x27;: 129, &#x27;classic&#x27;: 130, &#x27;line&#x27;: 131, &#x27;inspector&#x27;: 132, &quot;i&#x27;m&quot;: 133, &#x27;here&#x27;: 134, &#x27;sack&#x27;: 135, &#x27;your&#x27;: 136, &#x27;welcome&#x27;: 137, &#x27;expect&#x27;: 138, &#x27;many&#x27;: 139, &#x27;adults&#x27;: 140, &#x27;age&#x27;: 141, &#x27;far&#x27;: 142, &#x27;fetched&#x27;: 143, &#x27;pity&#x27;: 144, &#x27;homelessness&#x27;: 145, &#x27;houselessness&#x27;: 146, &#x27;george&#x27;: 147, &#x27;carlin&#x27;: 148, &#x27;stated&#x27;: 149, &#x27;been&#x27;: 150, &#x27;issue&#x27;: 151, &#x27;never&#x27;: 152, &#x27;plan&#x27;: 153, &#x27;those&#x27;: 154, &#x27;considered&#x27;: 155, &#x27;human&#x27;: 156, &#x27;going&#x27;: 157, &#x27;vote&#x27;: 158, &#x27;most&#x27;: 159, &#x27;just&#x27;: 160, &#x27;lost&#x27;: 161, &#x27;cause&#x27;: 162, &#x27;things&#x27;: 163, &#x27;racism&#x27;: 164, &#x27;war&#x27;: 165, &#x27;iraq&#x27;: 166, &#x27;pressuring&#x27;: 167, &#x27;kids&#x27;: 168, &#x27;succeed&#x27;: 169, &#x27;technology&#x27;: 170, &#x27;elections&#x27;: 171, &#x27;inflation&#x27;: 172, &quot;they&#x27;ll&quot;: 173, &#x27;end&#x27;: 174, &#x27;up&#x27;: 175, &#x27;month&#x27;: 176, &#x27;had&#x27;: 177, &#x27;entertainment&#x27;: 178, &#x27;sets&#x27;: 179, &#x27;bathroom&#x27;: 180, &#x27;pictures&#x27;: 181, &#x27;wall&#x27;: 182, &#x27;computer&#x27;: 183, &#x27;treasure&#x27;: 184, &#x27;goddard&#x27;: 185, &quot;bolt&#x27;s&quot;: 186, &#x27;lesson&#x27;: 187, &#x27;brooks&#x27;: 188, &#x27;directs&#x27;: 189, &#x27;stars&#x27;: 190, &#x27;plays&#x27;: 191, &#x27;man&#x27;: 192, &#x27;world&#x27;: 193, &#x27;until&#x27;: 194, &#x27;deciding&#x27;: 195, &#x27;make&#x27;: 196, &#x27;sissy&#x27;: 197, &#x27;rival&#x27;: 198, &#x27;jeffery&#x27;: 199, &#x27;tambor&#x27;: 200, &#x27;thirty&#x27;: 201, &#x27;days&#x27;: 202, &#x27;succeeds&#x27;: 203, &#x27;wants&#x27;: 204, &#x27;future&#x27;: 205, &#x27;project&#x27;: 206, &#x27;more&#x27;: 207, &#x27;buildings&#x27;: 208, &quot;bet&#x27;s&quot;: 209, &#x27;thrown&#x27;: 210, &#x27;bracelet&#x27;: 211, &#x27;leg&#x27;: 212, &#x27;monitor&#x27;: 213, &#x27;every&#x27;: 214, &#x27;move&#x27;: 215, &quot;can&#x27;t&quot;: 216, &#x27;step&#x27;: 217, &#x27;off&#x27;: 218, &#x27;sidewalk&#x27;: 219, &#x27;nickname&#x27;: 220, &#x27;pepto&#x27;: 221, &#x27;vagrant&#x27;: 222, &#x27;after&#x27;: 223, &#x27;written&#x27;: 224, &#x27;forehead&#x27;: 225, &#x27;meets&#x27;: 226, &#x27;characters&#x27;: 227, &#x27;including&#x27;: 228, &#x27;woman&#x27;: 229, &#x27;name&#x27;: 230, &#x27;lesley&#x27;: 231, &#x27;ann&#x27;: 232, &#x27;warren&#x27;: 233, &#x27;ex&#x27;: 234, &#x27;dancer&#x27;: 235, &#x27;got&#x27;: 236, &#x27;divorce&#x27;: 237, &#x27;pals&#x27;: 238, &#x27;sailor&#x27;: 239, &#x27;howard&#x27;: 240, &#x27;morris&#x27;: 241, &#x27;fumes&#x27;: 242, &#x27;teddy&#x27;: 243, &#x27;wilson&#x27;: 244, &#x27;are&#x27;: 245, &#x27;already&#x27;: 246, &quot;they&#x27;re&quot;: 247, &#x27;survivors&#x27;: 248, &#x27;not&#x27;: 249, &#x27;reaching&#x27;: 250, &#x27;mutual&#x27;: 251, &#x27;agreements&#x27;: 252, &#x27;fight&#x27;: 253, &#x27;flight&#x27;: 254, &#x27;kill&#x27;: 255, &#x27;killed&#x27;: 256, &#x27;love&#x27;: 257, &#x27;connection&#x27;: 258, &#x27;between&#x27;: 259, &quot;wasn&#x27;t&quot;: 260, &#x27;necessary&#x27;: 261, &#x27;plot&#x27;: 262, &#x27;found&#x27;: 263, &#x27;stinks&#x27;: 264, &quot;brooks&#x27;&quot;: 265, &#x27;observant&#x27;: 266, &#x27;films&#x27;: 267, &#x27;prior&#x27;: 268, &#x27;shows&#x27;: 269, &#x27;tender&#x27;: 270, &#x27;side&#x27;: 271, &#x27;compared&#x27;: 272, &#x27;slapstick&#x27;: 273, &#x27;blazing&#x27;: 274, &#x27;saddles&#x27;: 275, &#x27;young&#x27;: 276, &#x27;frankenstein&#x27;: 277, &#x27;spaceballs&#x27;: 278, &#x27;show&#x27;: 279, &#x27;having&#x27;: 280, &#x27;something&#x27;: 281, &#x27;valuable&#x27;: 282, &#x27;day&#x27;: 283, &#x27;hand&#x27;: 284, &#x27;stupid&#x27;: 285, &quot;don&#x27;t&quot;: 286, &#x27;know&#x27;: 287, &#x27;should&#x27;: 288, &#x27;give&#x27;: 289, &#x27;instead&#x27;: 290, &#x27;using&#x27;: 291, &#x27;monopoly&#x27;: 292, &#x27;this&#x27;: 293, &#x27;film&#x27;: 294, &#x27;will&#x27;: 295, &#x27;inspire&#x27;: 296, &#x27;others&#x27;: 297&#125;</span></span><br><span class="line"><span class="string">sequence_train: </span></span><br><span class="line"><span class="string"> [[20, 21, 9, 3, 89, 44, 10, 90, 45, 1, 91, 92, 7, 93, 31, 94, 46, 32, 47, 33, 7, 34, 48, 95, 49, 22, 1, 96, 97, 98, 50, 2, 99, 11, 20, 100, 101, 9, 102, 103, 2, 104, 105, 9, 34, 1, 106, 2, 107, 108, 1, 109, 51, 8, 35, 36, 110, 111, 37, 112, 113, 114, 1, 115, 5, 1, 116, 117, 52, 118, 50, 5, 1, 119, 14, 120, 15, 37, 51, 38, 14, 121, 1, 122, 22, 123, 3, 53, 124, 125, 2, 126, 127, 1, 32, 14, 128, 129, 45, 21, 3, 130, 131, 132, 133, 134, 2, 135, 54, 5, 136, 34, 53, 137, 2, 20, 21, 14, 138, 11, 139, 140, 5, 48, 141, 55, 11, 20, 21, 9, 142, 143, 12, 3, 144, 11, 10, 56], [145, 6, 146, 7, 147, 148, 149, 57, 150, 58, 151, 16, 49, 59, 152, 3, 153, 2, 60, 154, 4, 1, 61, 11, 62, 23, 155, 156, 8, 63, 39, 64, 157, 2, 32, 65, 6, 158, 16, 1, 66, 159, 67, 55, 5, 1, 40, 7, 160, 3, 161, 162, 68, 69, 46, 163, 33, 7, 164, 1, 165, 4, 166, 167, 168, 2, 169, 170, 1, 171, 172, 6, 69, 24, 173, 25, 70, 2, 174, 175, 4, 1, 26, 59, 12, 24, 27, 62, 71, 3, 41, 2, 72, 4, 1, 26, 16, 3, 176, 73, 1, 74, 27, 23, 177, 64, 3, 75, 1, 178, 179, 3, 180, 181, 4, 1, 182, 3, 183, 15, 39, 27, 23, 184, 2, 36, 12, 28, 17, 2, 25, 40, 11, 9, 185, 186, 187, 76, 188, 8, 189, 8, 190, 7, 13, 191, 3, 42, 192, 8, 57, 39, 22, 1, 193, 194, 195, 2, 196, 3, 41, 29, 3, 197, 198, 199, 200, 2, 36, 24, 18, 35, 72, 22, 1, 26, 16, 201, 202, 73, 1, 74, 24, 13, 203, 18, 35, 43, 12, 18, 204, 29, 3, 205, 206, 5, 77, 207, 208, 1, 209, 4, 19, 13, 9, 210, 4, 1, 61, 29, 3, 211, 4, 30, 212, 2, 213, 30, 214, 215, 19, 18, 216, 217, 218, 1, 219, 78, 71, 1, 220, 221, 79, 3, 222, 223, 28, 224, 4, 30, 225, 19, 13, 226, 31, 227, 228, 3, 229, 79, 1, 230, 5, 80, 231, 232, 233, 58, 234, 235, 8, 236, 237, 81, 82, 83, 75, 15, 83, 238, 239, 240, 241, 15, 242, 243, 244, 8, 245, 246, 84, 2, 1, 26, 247, 248, 13, 56, 78, 249, 84, 2, 250, 251, 252, 17, 18, 23, 63, 38, 85, 42, 19, 28, 253, 6, 254, 255, 6, 25, 256, 68, 1, 257, 258, 259, 80, 15, 13, 260, 261, 2, 262, 14, 263, 47, 264, 2, 25, 54, 5, 76, 265, 266, 267, 19, 268, 2, 85, 3, 44, 10, 269, 3, 270, 271, 272, 2, 30, 273, 65, 33, 7, 274, 275, 276, 277, 6, 278, 16, 1, 66, 2, 279, 12, 28, 17, 280, 281, 282, 81, 82, 10, 1, 70, 283, 6, 4, 1, 31, 284, 77, 3, 285, 41, 17, 52, 42, 67, 43, 38, 86, 286, 287, 12, 2, 43, 29, 37, 87, 88, 86, 288, 289, 10, 2, 1, 40, 290, 5, 291, 10, 17, 292, 87, 6, 88, 293, 294, 295, 296, 27, 2, 60, 297]]</span></span><br><span class="line"><span class="string"># 对齐前</span></span><br><span class="line"><span class="string">138</span></span><br><span class="line"><span class="string">425</span></span><br><span class="line"><span class="string"># 对齐后</span></span><br><span class="line"><span class="string">200</span></span><br><span class="line"><span class="string">200</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Process finished with exit code 0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>以上的过程总结如下：以一条评论为例。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129110647827.png" alt="image-20231129110647827"></p>
<p>对于测试数据集同样完成tokenization-&gt;encoding-&gt;alignment这三步。但是需要注意的是， 训练集中的字典与测试集中的字典必须相同， 否则可能会出现同样的单词在测试集中的索引与训练集中的所有不同。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129111035359.png" alt="image-20231129111035359" style="zoom:67%;"></p>
<h4 id="3、Word-Embedding词嵌入-word-to-vector"><a href="#3、Word-Embedding词嵌入-word-to-vector" class="headerlink" title="3、Word Embedding词嵌入  word to vector"></a>3、Word Embedding词嵌入  word to vector</h4><p>将单词进一步表示成向量， 之前每个单词都用数字表示， 该如何把这些特征表示成向量呢。显然可以用如下图所示的one-hot编码。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129111713626.png" alt="image-20231129111713626" style="zoom:67%;"></p>
<p>但是如果字典中有1万个单词， 那么这个one-hot维度就太大了， 因此需要做word embedding， 将这些高维向量映射成低维向量。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129112205707.png" alt="image-20231129112205707" style="zoom:67%;"></p>
<p>上图中<strong>P</strong>是参数矩阵， 它的参数可以在训练过程中从训练数据中学习。<strong>ei</strong>是字典中第i个单词的one-hot向量。</p>
<p>矩阵<strong>P</strong>的大小是d×v， d是词向量的维度， 由用户自主定义。矩阵乘法的结果是<strong>Xi</strong>， <strong>Xi</strong>就是词向量， 维度是d。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129112644337.png" alt="image-20231129112644337" style="zoom:67%;"></p>
<p>训练好的词向量展示在坐标中， 发现相似的词在坐标中的距离比较近。</p>
<p>keras中提供的Embedding层， 用户需要指定vocabulary的大小和词向量的维度以及每个sequence的长度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Flatten, Dense, Embedding</span><br><span class="line"></span><br><span class="line">embedding_dim = <span class="number">8</span></span><br><span class="line">vocabulary = <span class="number">100000</span></span><br><span class="line">word_num = <span class="number">20</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Embedding(vocabulary, embedding_dim, input_length=word_num))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129114342009.png" alt="image-20231129114342009" style="zoom: 67%;"></p>
<p>上图中， 处理数据时， 词汇表的大小是10k，以及每个电影评论中保留最后20个词（不足就补齐）， 设置词向量的维度等于8。</p>
<p>Embedding层的输出是20x8的句子， 也就是word_num × Embedding_dim</p>
<p>embedding层的参数量等于80k， 80k是这样计算的，Embedding层中有一个参数矩阵p， 矩阵的行数等于vocabulary， 所以矩阵有10k行， 矩阵的列数d是词向量的维度embedding_dim设置为8， 所以矩阵的大小是10k×8=80k， 所以Embedding层一共有80k个参数。</p>
<p>上面已经完成了文本处理和word embed， 每条电影评论保留最后的20个单词， 每个单词用一个8维的词向量表示。现在用logistics regression来做二分类。</p>
<p>Logistic regression for binary classification， 判断电影评论是正面的还是负面的， 用这几行就可实现一个分类器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Flatten, Dense, Embedding</span><br><span class="line"></span><br><span class="line">embedding_dim = <span class="number">8</span></span><br><span class="line">model = Sequential()  <span class="comment"># 就把网络层按照顺序搭建起来</span></span><br><span class="line"><span class="comment"># 添加Embedding层， Embedding层的输出是20x8的句子， 每条电影评论中有20个单词， 每个单词用8维的向量表示</span></span><br><span class="line">model.add(Embedding(vocabulary, embedding_dim, input_length=word_num))</span><br><span class="line"><span class="comment"># 展平操作</span></span><br><span class="line">model.add(Flatten())</span><br><span class="line"><span class="comment"># 全连接层， 输出1维， 用sigmoid激活函数， 这一层的输出是介于0-1之间的浮点数。0代表负面评价， 1代表正面评价</span></span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"><span class="comment"># summary函数打印模型的概要</span></span><br><span class="line">model.summary()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129142507422.png" alt="image-20231129142507422" style="zoom:67%;"></p>
<p>下面是编译模型， 然后训练损失函数来拟合模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line">epochs = <span class="number">50</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=optimizer.RMSprop(lr=<span class="number">0.0001</span>), loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line">history = model.fit(x_train, y_train, epochs=epochs, </span><br><span class="line">                   batch_size=<span class="number">32</span>, validation_data=(x_valid, y_valid))</span><br></pre></td></tr></table></figure>
<p>epochs的意思是把2万条数据扫一遍叫作igepochs， 50个epochs的意思是把训练数据全部扫50遍。</p>
<p>训练得到的准确率和损失曲线如下。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129150432649.png" alt="image-20231129150432649" style="zoom:67%;"></p>
<p>上面的过程总结如下：</p>
<p>针对每条电影评论， 首先进行tokenization， 将电影评论分割成一个一个的单词， 然后把每个单词编码成一个一个数字， 这样一条电影评论就可以用正整数的序列来表示了， 这个正整数的序列叫作sequence， sequence就是神经网络中Embedding的输入。</p>
<p>由于电影评论长短不一， 得到的sequences长短也不一样， 无法存储在一个矩阵中， 解决方法就是alignment对齐， 假设长度大于20个， 就只保留最后20个， 假设长度小于20， 就用0补齐，将长度补到20个， 这样每个sequence都是20个单词长度， 后面就是word embedding.</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129144713366.png" alt="image-20231129144713366" style="zoom:67%;"></p>
<p>把长度为20的sequences输入embedding层， embedding层把每个单词映射到8维的词向量， 所以每个长度为20的句子用一个20x8的矩阵来表示</p>
<p>然后用flatten将20x8的矩阵展平， 变成160维的向量， 最后用logistics regression分类器做分类。</p>
<p>embedding层有一个参数矩阵， 大小是1万×8， 1万就是词典里面的单词个数， 8是词向量的维度， 每个单词被映射为8维的词向量， 这个logistics regression分类器有161个参数， 输入是160维的向量， 所以分类器有160维的参数向量， 分类器还有一个bias偏置量， 所以一共有161个参数。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129144737270.png" alt="image-20231129144737270" style="zoom:67%;"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">AI4Future</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://guudman.github.io/2023/11/29/NlpFoundation/">https://guudman.github.io/2023/11/29/NlpFoundation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post_share"><div class="social-share" data-image="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://fastly.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/12/01/RNN/"><img class="prev-cover" src="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg" onerror="onerror=null;src='https://gitee.com/guudman/blog_images/raw/master/article.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">RNN</div></div></a></div><div class="next-post pull-right"><a href="/2023/11/27/RepVGG/"><img class="next-cover" src="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg" onerror="onerror=null;src='https://gitee.com/guudman/blog_images/raw/master/article.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">RepVGG</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2023/12/01/RNN/" title="RNN"><img class="cover" src="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-01</div><div class="title">RNN</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E5%9F%BA%E7%A1%80"><span class="toc-number">1.</span> <span class="toc-text">1、基础</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%B1%BB%E5%88%AB%E7%89%B9%E5%BE%81%E8%BD%AC%E5%8C%96%E6%95%B0%E5%AD%97%E6%B5%81%E7%A8%8B"><span class="toc-number">1.1.</span> <span class="toc-text">类别特征转化数字流程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86"><span class="toc-number">1.2.</span> <span class="toc-text">文本特征处理</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="toc-number">2.</span> <span class="toc-text">2、文本处理与词嵌入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E3%80%81Word-Embedding%E8%AF%8D%E5%B5%8C%E5%85%A5-word-to-vector"><span class="toc-number">3.</span> <span class="toc-text">3、Word Embedding词嵌入  word to vector</span></a></li></ol></div></div><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='https://gitee.com/guudman/blog_images/raw/master/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">AI4Future</div><div class="author-info__description">Not Only Look Once</div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/GuudMan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2663017379@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">微信公众号: AI4Future</div></div></div></div></main><footer id="footer" style="background: #FFFFFF"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By AI4Future</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/chenxz21/hexo-theme-bcxm">Bcxm</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script type="text/javascript" src="https://fastly.jsdelivr.net/npm/leancloud-storage@4.10.0/dist/av-min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>