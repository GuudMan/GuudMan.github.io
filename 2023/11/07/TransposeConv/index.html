<!DOCTYPE html><html lang="zh-cn" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>TransposeConv</title><meta name="keywords" content="ComputureVision"><meta name="author" content="AI4Future"><meta name="copyright" content="AI4Future"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1、简介首先回顾一下普通卷积， 下图是以stride&#x3D;1， padding&#x3D;0， kernel_size&#x3D;3为例， 假设输入特征图大小为4×4(假设输入输出都是单通道), 通过卷积后的特征图大小为2×2， 一般使用卷积的情况中， 要是特征图变小（stride&gt;1)， 要么保持不变(stride&#x3D;1)， 当然也可以通过四周padding让特征图变大但是没有意义。   2、转置卷积转置卷积主">
<meta property="og:type" content="article">
<meta property="og:title" content="TransposeConv">
<meta property="og:url" content="https://guudman.github.io/2023/11/07/TransposeConv/index.html">
<meta property="og:site_name" content="Ai4Future">
<meta property="og:description" content="1、简介首先回顾一下普通卷积， 下图是以stride&#x3D;1， padding&#x3D;0， kernel_size&#x3D;3为例， 假设输入特征图大小为4×4(假设输入输出都是单通道), 通过卷积后的特征图大小为2×2， 一般使用卷积的情况中， 要是特征图变小（stride&gt;1)， 要么保持不变(stride&#x3D;1)， 当然也可以通过四周padding让特征图变大但是没有意义。   2、转置卷积转置卷积主">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg">
<meta property="article:published_time" content="2023-11-07T02:02:47.000Z">
<meta property="article:modified_time" content="2023-11-29T10:06:24.840Z">
<meta property="article:author" content="AI4Future">
<meta property="article:tag" content="ComputureVision">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://guudman.github.io/2023/11/07/TransposeConv/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: AI4Future","link":"Link: ","source":"Source: Ai4Future","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://fastly.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://fastly.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'TransposeConv',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-11-29 18:06:24'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Ai4Future" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="onerror=null;src='https://gitee.com/guudman/blog_images/raw/master/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">AI4Future</div><div class="author-info__description">Not Only Look Once</div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/GuudMan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2663017379@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/gedan"><i class="fa-fw fas fa-music"></i><span> 歌单</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk"><i class="fa-fw fa fa-heartbeat"></i><span> 时光</span></a></div><div class="menus_item"><a class="site-page" href="/shuoba"><i class="fa-fw fas fa-comment-dots"></i><span> 说吧</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/google"><span> 镜像</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://json.xbyzs.cf"><span> Json格式化</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://draw.xbyzs.cf"><span> Draw画布</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://enkey.xbyzs.cf"><span> EnKey</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ai4Future</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/gedan"><i class="fa-fw fas fa-music"></i><span> 歌单</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk"><i class="fa-fw fa fa-heartbeat"></i><span> 时光</span></a></div><div class="menus_item"><a class="site-page" href="/shuoba"><i class="fa-fw fas fa-comment-dots"></i><span> 说吧</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/google"><span> 镜像</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://json.xbyzs.cf"><span> Json格式化</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://draw.xbyzs.cf"><span> Draw画布</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://enkey.xbyzs.cf"><span> EnKey</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">TransposeConv</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-07T02:02:47.000Z" title="Created 2023-11-07 10:02:47">2023-11-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:06:24.840Z" title="Updated 2023-11-29 18:06:24">2023-11-29</time></span></div><div class="meta-secondline"></div></div></div><article class="post-content" id="article-container"><meta name="referrer" content="no-referrer">

<h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>首先回顾一下普通卷积， 下图是以stride=1， padding=0， kernel_size=3为例， 假设输入特征图大小为4×4(假设输入输出都是单通道), 通过卷积后的特征图大小为2×2， 一般使用卷积的情况中， 要是特征图变小（stride&gt;1)， 要么保持不变(stride=1)， 当然也可以通过四周padding让特征图变大但是没有意义。</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/conv1.gif" alt="conv1"></p>
<hr>
<h4 id="2、转置卷积"><a href="#2、转置卷积" class="headerlink" title="2、转置卷积"></a>2、转置卷积</h4><p>转置卷积主要起到上采样的作用， 但转置卷积不是卷积的逆运算(一般卷积操作是不可逆的), 它只能恢复到原来的大小， 数值与原来的不一样。转置卷积的运算步骤可以归纳为以下几点：</p>
<p>在输入特征图元素间填充s-1行， 列0(其中s是转置卷积的步距)</p>
<p>在输入特征图四周填充p-1行， 列0（其中k表示转置卷积的kernel_size大小， p为转置卷积的padding， 注意这里的padding和卷积操作有些不同)</p>
<p>将卷积核参数上下、左右翻转</p>
<p>做正常卷积运算(填充0， 步距1)</p>
<p>下面假设输入的特征图为2×2（假设输入输出为单通道)， 通过转置卷积后的的得到4×4大小的特征图。 这里使用的转置卷积核大小k=3， stride=1， padding=0的情况(忽略偏置bias)</p>
<p>首先在元素间填充s-1=0行，列0</p>
<p>然后再特征图四周填充k-p-1=2行， 列0</p>
<p>接着对卷积核参数进行上下， 左右翻转</p>
<p>最后做正常卷积(填充0， 步距1)  transpose_conv.jpg</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/transpose_conv.jpg" alt="transpose_conv"></p>
<p>下面展示了不同转置卷积中不同s和p的情况  transpose_conv_s2_p0_k3.gif  transpose_conv_s1_p0_k3.gif  transpose_conv_s2_p1_k3.gif</p>
<table style="text-align: center;">
    <tr>
    <td><img src="https://gitee.com/guudman/blog_images/raw/master/transpose_conv_s1_p0_k3.gif" width="200">
    <div>s=1, p=0, k=3 </div>
    <div>间隙填充s-1=0,四周填充k-p-1=2</div>
    </td>
   <td>
     <img src="https://gitee.com/guudman/blog_images/raw/master/transpose_conv_s2_p0_k3.gif" width="200">    
     <div>s=2, p=0, k=3</div> 
     <div>间隙填充s-1=1,四周填充k-p-1=2</div>
    </td>
   <td>
        <img src="https://gitee.com/guudman/blog_images/raw/master/transpose_conv_s2_p1_k3.gif" width="200">
       <div> s=2, p=1, k=3</div>
       <div>间隙填充s-1=1,四周填充k-p-1=1</div>
    </td>
    </tr>
</table>

<p>转置卷积操作后特征图的大小可以通过如下公式进行计算：</p>
<script type="math/tex; mode=display">
H_{out}=(H_{in} - 1) \times stride[0] -2 \times padding[0] + kernel\_size[0]</script><script type="math/tex; mode=display">
W_{out}=(W_{in} - 1) \times stride[1] -2 \times padding[1] + kernel\_size[1]</script><p>其中stride[0]表示高度方向的stride， padding[0]表示高度方向的padding， kernel_size[0]表示高度方向的kernel_size, 同理[1]表示宽度方向上的。 通过上面的公式可以看出， padding越大，输出特征矩阵的高宽越小。 你可以理解为正向传播过程中进行了padding然后得到了特征图， 现在使用转置卷积还原到原来的高宽后要把原来的padding减掉。 </p>
<hr>
<h4 id="3、pytorch中转置卷积参数"><a href="#3、pytorch中转置卷积参数" class="headerlink" title="3、pytorch中转置卷积参数"></a>3、pytorch中转置卷积参数</h4><p>pytorch官方关于转置卷积的ConvTranspose2d的文档。 </p>
<p>Applied a 2D transposed convolution operator over an input image composed of several input planes. This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (althought it is not an actual deconvolution operation).   image-20231107093032555.png</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231107093032555.png" alt="image-20231107093032555"></p>
<p>官方的介绍中还有如下几个参数：</p>
<p>output_padding: 在计算得到的输出特征图的高、宽各方向各填充几行或列0(注意：这里只是在上下以及左右的一侧one side填充，并不是两侧头填充)</p>
<p>groups: 当使用到组卷积时才会用到的参数， 默认为1即为普通卷积</p>
<p>bias： 是否使用偏置， 默认为True使用</p>
<p>dilation： 当使用空洞卷积（膨胀卷积）时才会使用到的参数， 默认为1即为普通卷积。</p>
<p>输出特征图高、宽计算</p>
<script type="math/tex; mode=display">
H_{out}=(H_{in} - 1) \times stride[0] -2 \times padding[0] \\ + dilation[0] \ times (kernel_size[0] - 1) + output_padding[0] + 1</script><script type="math/tex; mode=display">
W_{out}=(W_{in} - 1) \times stride[1] -2 \times padding[1] + \\
dilation[1] \ times (kernel_size[1] - 1) + output_padding[1] + 1</script><hr>
<h4 id="4、pytorch转置卷积实验"><a href="#4、pytorch转置卷积实验" class="headerlink" title="4、pytorch转置卷积实验"></a>4、pytorch转置卷积实验</h4><p>下面使用pytorch模拟s=1, p=0, k=3的转置卷积操作</p>
<p><img src="https://gitee.com/guudman/blog_images/raw/master/transpose_conv_s1_p0_k3.gif" alt="conv_s1k0p3"></p>
<p>在代码中transpose_conv_official函数是通过官方的转置卷积进行计算， transpose_conv_self是按照上面的介绍对输入特征图进行填充并通过卷积得到的结果。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : convTranspose_demo.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transposed_conv_official</span>():</span><br><span class="line">    feature_map = torch.as_tensor([[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                                   [<span class="number">2</span>, <span class="number">1</span>]], dtype=torch.float32).reshape([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">    <span class="built_in">print</span>(feature_map)</span><br><span class="line">    trans_conv = nn.ConvTranspose2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>,</span><br><span class="line">                                    kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    trans_conv.load_state_dict(&#123;<span class="string">&quot;weight&quot;</span>:</span><br><span class="line">                                    torch.as_tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                                                     [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                                     [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">                                                    dtype=torch.float32)</span><br><span class="line">                               .reshape([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>])&#125;)</span><br><span class="line">    <span class="built_in">print</span>(trans_conv.weight)</span><br><span class="line">    output = trans_conv(feature_map)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output, \n&quot;</span>, output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transposed_conv_self</span>():</span><br><span class="line">    feature_map = torch.as_tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                   [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                   [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                   [<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                   [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                   [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                   ], dtype=torch.float32).reshape([<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line">    <span class="built_in">print</span>(feature_map)</span><br><span class="line">    conv = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>,</span><br><span class="line">                     kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    conv.load_state_dict(&#123;<span class="string">&quot;weight&quot;</span>: torch.as_tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                                                     [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                                                     [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]], dtype=torch.float32).reshape([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>])&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(conv.weight)</span><br><span class="line">    output = conv(feature_map)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output \n&quot;</span>, output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    transposed_conv_official()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;--------&#x27;</span>)</span><br><span class="line">    transposed_conv_self()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>终端输出：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">feature<span class="built_in">_</span>map </span><br><span class="line"> tensor([[[[1., 0.],</span><br><span class="line">          [2., 1.]]]])</span><br><span class="line">trans<span class="built_in">_</span>conv.weight </span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([[[[1., 0., 1.],</span><br><span class="line">          [0., 1., 1.],</span><br><span class="line">          [1., 0., 0.]]]], requires<span class="built_in">_</span>grad=True)</span><br><span class="line">output, </span><br><span class="line"> tensor([[[[1., 0., 1., 0.],</span><br><span class="line">          [2., 2., 3., 1.],</span><br><span class="line">          [1., 2., 3., 1.],</span><br><span class="line">          [2., 1., 0., 0.]]]], grad<span class="built_in">_</span>fn=&lt;ConvolutionBackward0&gt;)</span><br><span class="line">--------</span><br><span class="line">self feature<span class="built_in">_</span>map </span><br><span class="line"> tensor([[[[0., 0., 0., 0., 0., 0.],</span><br><span class="line">          [0., 0., 0., 0., 0., 0.],</span><br><span class="line">          [0., 0., 1., 0., 0., 0.],</span><br><span class="line">          [0., 0., 2., 1., 0., 0.],</span><br><span class="line">          [0., 0., 0., 0., 0., 0.],</span><br><span class="line">          [0., 0., 0., 0., 0., 0.]]]])</span><br><span class="line">conv.weight </span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([[[[0., 0., 1.],</span><br><span class="line">          [1., 1., 0.],</span><br><span class="line">          [1., 0., 1.]]]], requires<span class="built_in">_</span>grad=True)</span><br><span class="line">output </span><br><span class="line"> tensor([[[[1., 0., 1., 0.],</span><br><span class="line">          [2., 2., 3., 1.],</span><br><span class="line">          [1., 2., 3., 1.],</span><br><span class="line">          [2., 1., 0., 0.]]]], grad<span class="built_in">_</span>fn=&lt;ConvolutionBackward0&gt;)</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">AI4Future</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://guudman.github.io/2023/11/07/TransposeConv/">https://guudman.github.io/2023/11/07/TransposeConv/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post_share"><div class="social-share" data-image="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://fastly.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/11/08/SwinTransformer/"><img class="prev-cover" src="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg" onerror="onerror=null;src='https://gitee.com/guudman/blog_images/raw/master/article.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">SwinTransformer</div></div></a></div><div class="next-post pull-right"><a href="/2023/11/05/VisionTransformer/"><img class="next-cover" src="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg" onerror="onerror=null;src='https://gitee.com/guudman/blog_images/raw/master/article.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">VisionTransformer</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2023/11/02/AlexNet%E8%A7%A3%E6%9E%90/" title="AlexNet解析"><img class="cover" src="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-02</div><div class="title">AlexNet解析</div></div></a></div><div><a href="/2023/11/04/BatchNormalization/" title="BatchNormalization"><img class="cover" src="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-04</div><div class="title">BatchNormalization</div></div></a></div><div><a href="/2023/11/12/ConvNeXt/" title="ConvNeXt"><img class="cover" src="https://gitee.com/guudman/blog_images/raw/master/top_image.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-12</div><div class="title">ConvNeXt</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">1、简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF"><span class="toc-number">2.</span> <span class="toc-text">2、转置卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E3%80%81pytorch%E4%B8%AD%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E5%8F%82%E6%95%B0"><span class="toc-number">3.</span> <span class="toc-text">3、pytorch中转置卷积参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E3%80%81pytorch%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.</span> <span class="toc-text">4、pytorch转置卷积实验</span></a></li></ol></div></div><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='https://gitee.com/guudman/blog_images/raw/master/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">AI4Future</div><div class="author-info__description">Not Only Look Once</div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/GuudMan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2663017379@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">微信公众号: 预留，暂未开通</div></div></div></div></main><footer id="footer" style="background: #FFFFFF"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By AI4Future</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/chenxz21/hexo-theme-bcxm">Bcxm</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script type="text/javascript" src="https://fastly.jsdelivr.net/npm/leancloud-storage@4.10.0/dist/av-min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>