<!DOCTYPE html><html lang="zh-cn" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Ai4Future</title><meta name="keywords" content="成长，分享，专注"><meta name="author" content="AI4Future"><meta name="copyright" content="AI4Future"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Not Only Look Once">
<meta property="og:type" content="website">
<meta property="og:title" content="Ai4Future">
<meta property="og:url" content="https://guudman.github.io/index.html">
<meta property="og:site_name" content="Ai4Future">
<meta property="og:description" content="Not Only Look Once">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://guudman.github.io/img/avatar.jpg">
<meta property="article:author" content="AI4Future">
<meta property="article:tag" content="成长，分享，专注">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://guudman.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://guudman.github.io/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: AI4Future","link":"Link: ","source":"Source: Ai4Future","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://fastly.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://fastly.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Ai4Future',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2023-12-02 16:23:41'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Ai4Future" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="onerror=null;src='https://gitee.com/guudman/blog_images/raw/master/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">AI4Future</div><div class="author-info__description">Not Only Look Once</div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/GuudMan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2663017379@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/gedan"><i class="fa-fw fas fa-music"></i><span> 歌单</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk"><i class="fa-fw fa fa-heartbeat"></i><span> 时光</span></a></div><div class="menus_item"><a class="site-page" href="/shuoba"><i class="fa-fw fas fa-comment-dots"></i><span> 说吧</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/google"><span> 镜像</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://json.xbyzs.cf"><span> Json格式化</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://draw.xbyzs.cf"><span> Draw画布</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://enkey.xbyzs.cf"><span> EnKey</span></a></li></ul></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ai4Future</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/gedan"><i class="fa-fw fas fa-music"></i><span> 歌单</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk"><i class="fa-fw fa fa-heartbeat"></i><span> 时光</span></a></div><div class="menus_item"><a class="site-page" href="/shuoba"><i class="fa-fw fas fa-comment-dots"></i><span> 说吧</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/google"><span> 镜像</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://json.xbyzs.cf"><span> Json格式化</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://draw.xbyzs.cf"><span> Draw画布</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://enkey.xbyzs.cf"><span> EnKey</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/02/make-RNN-more-efficient/" title="make_RNN_more_efficient">make_RNN_more_efficient</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-12-02T07:56:23.000Z" title="Created 2023-12-02 15:56:23">2023-12-02</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-12-02T08:22:40.189Z" title="Updated 2023-12-02 16:22:40">2023-12-02</time></span></div><div class="content">

1、stacked RNN3个技巧提升RNN的效果。
第一个是Stacked RNN。stack RNN为多层RNN， 可以把很多全连接层堆叠起来， 构成一个multilayer percepter， 也可把很多卷积层堆叠起来， 构成一个深度卷积网络。同样的道理也可以把很多RNN堆叠起来构成一个多层RNN网络。神经网络的每一步都会更新状态h， 新算出来的h有两个copy， 一份送到下一个时刻， 另外一份作为输出。这一层输出的状态h成为了上一层的输入。再解释一下， 最底层的RNN的输入是词向量x， 这层RNN会输出每一步的状态向量h， 这些输出的状态向量又成为了第二次RNN的输入。

第二次RNN有自己的模型参数， 会更新和输出自己的状态向量h， 第二层输出的状态向量h又成为了第三层RNN的输入。

一共有三层RNN， 最上层的状态向量是最终的输出， 可以用最后一个状态ht看成是从最底层的输入I love the movie so much中提取的特征向量。
使用keras实现多层LSTM， 这里用了三层LSTM层。第一层的输出会成为第二层的输入， 所以第一层的return_sequ ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/02/LSTM/" title="LSTM">LSTM</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-12-02T07:49:02.000Z" title="Created 2023-12-02 15:49:02">2023-12-02</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-12-02T08:23:08.762Z" title="Updated 2023-12-02 16:23:08">2023-12-02</time></span></div><div class="content">

1、简介LSTM是一种RNN模型， 是对Simple RNN的改进， LSTM可以避免梯度消失的问题， 可以有更长的记忆。原论文 Long Short-term Memory (researchgate.net)](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory))是1997年发表的。下图是LSTM原论文中的截取的图。

LSTM也是一种循环神经网络， 原理跟Simple RNN差不多。

每当读取一个新的输入x就会更新状态h， lstm的结构比simple RNN要复杂很多。Simple RNN只有一个参数矩阵， LSTM有4个参数矩阵， 下面看一下LSTM内部的具体结构。
2、LSTM结构LSTM最重要的设计是这个传输带， 即为向量C。过去的信息通过传输带送到下一个时刻，不会发生太大的变化， LSTM就是通过传输传输带来避免梯度消失。

LSTM中有很多gate可以有选择的让信息通过， 先来看一下forget gate遗忘门。遗忘门由sigmoid function和Element ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/01/RNN/" title="RNN">RNN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-12-01T09:50:00.000Z" title="Created 2023-12-01 17:50:00">2023-12-01</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-12-01T10:04:34.438Z" title="Updated 2023-12-01 18:04:34">2023-12-01</time></span></div><div class="content">

1、简介RNN循环神经网络在nlp领域有些过时了， 训练数据足够多时， RNN的效果不如transformers模型， 但在小规模问题上， RNN还是很有用。机器学习中经常用到的文本、语音等时序数据， 思考一下， 如何对时序数据建模。在上面的基础部分讲到把一段文字整体输入到一个logistics regression模型， 让模型做二分类， 这属于one-to-one模型。 一个输入对应一个输出， 全连接神经网络和卷积网络都是one-to-one模型，但人类并不会把一整段文字全部输入到大脑中。 人类阅读时会从左到右阅读一段文字， 阅读时逐渐在大脑中积累文本的信息， 阅读一段话后脑中积累了整段文字的大意。one-to-one模型要求一个输入对应一个输出， 比如输入一张图片， 输出每一类的概率值。one-to-one模型很适合图片的问题， 但不太适合文本的问题。
对于文本问题， 输入和输出长度并不固定。 一句话可长可短， 所以输入的长度并不固定， 输出的长度也不固定。 比如把英文翻译成汉语， 英语可能有10个单词， 但翻译成的汉语可能有10个也可能有8个， 输出汉语的字数并不固定。由于 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/29/NlpFoundation/" title="NlpFoundation">NlpFoundation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-29T10:03:47.000Z" title="Created 2023-11-29 18:03:47">2023-11-29</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:14:36.133Z" title="Updated 2023-11-29 18:14:36">2023-11-29</time></span></div><div class="content">

1、基础如何将计算机不认识的文本特征转化为数字特征
常见的特征分类有类别特征（categorical feature）和数值特征（numeric feature）。 类别特征一般是有限集合， 没有大小之分， 是并列权重。数值特征如：年龄， 工资等， 有数值大小之分。计算机只能处理数值型特征，因此需要将非数值特征转化为计算机能识别的数字， 如下表中的gender和nationality。gender为二元特征， nationality为多元的类别特征。

国籍表示成1-197之间的整数(全球大概有197个国家)， 但这些整数只是一个类别， 它们之间无法比较大小， 因为这些整数只是类别而已， 并不是真正的数值， 所以需要对国籍做one-hot Embedding。需要注意的是我们这里用数字表示的类别特征从1开始， 因为0要用来保留未知或缺失的国籍，数据库中经常会有缺失数据， 这些缺失的国籍就用0来表示。
对于性别， 用0表示女性， 1表示男性。

上面提到， 1-197只代表一个类别， 它们之间无大小之分， 因此需要对国籍进一步做one-hot Embedding。
类别特征转化数字流 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/27/RepVGG/" title="RepVGG">RepVGG</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-27T10:55:52.000Z" title="Created 2023-11-27 18:55:52">2023-11-27</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:56.537Z" title="Updated 2023-11-29 18:05:56">2023-11-29</time></span></div><div class="content">

简介VGG网络是2014年牛津大学提出的， 在2014到2016年， VGG网络可以说是当时很火并广泛应用的backbone， 后面由于新网络的提出， 精度上VGG比不上ResNet， 速度和参数数量VGG比不过MobileNet等轻量级网络， 慢慢的VGG开始淡出人们的视线， 当VGG已经被大家遗忘时， 2021年清华，旷视等机构共同基础了RepVGG网络。
论文中， 作者提到了structural re-parameterization technique方法，即结构重参数化。实际上就是在训练时， 使用一个类似ResNet-style的多分支模型，而推理时转化成VGG-style的单路模型。 如下图， B表示RepVGG训练时采用的网络结构， 而在推理时采用图（C)的网络结构。


RepVGG Block详解其实关于RepVGG模型就是在不断堆叠Rep VGG Block。下面介绍一下RepVGG Blocks中的结构， 如下图针对训练时采用的RepVGG Block结构。其中（a)是进行下采样stride=2时使用的RepVGG Block结构，图(b)是正常的(strid ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/24/YOLOX/" title="YOLOX">YOLOX</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-24T09:55:08.000Z" title="Created 2023-11-24 17:55:08">2023-11-24</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:06:41.100Z" title="Updated 2023-11-29 18:06:41">2023-11-29</time></span></div><div class="content">

简介前面讲介绍过yolov5， 这里再来介绍YOLOX, YOLOx是旷视科技在2021年发表的一篇论文， 当时对标的网络就是YOLOv5。YOLOx从YOLOv5中引入如下三点：decoupled head, anchor-free以及advanceed label assigning strategy（SimOTA)。
在自己的项目中YOLOv5和YOLOx到底如何选择呢， 如果你的数据集分辨率不是很高， 比如640x640， 二者都可以试试， 如果你的图像分辨率很高， 比如1280x1280， 那么建议使用yolov5， 因为yolov5官方仓库提供了更大尺度的预训练权重， 而YOLOx当前只有640x640的预训练权重。 


网络结构下图是根据源码绘制的YOLOX-L网络结构， 因为它是基于YOLO v5构建的， 所以Backbone以及PAN部分和YOLO v5是一模一样的， 注意这里说的YOLO v5对应的是tag：v5.0版本， 而我们之前的yolov5对应的是tag:v6.1版本， 所以在backbone部分有细微区别。

yolox与yolo v5在结构上有什么 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/24/FCOS/" title="FCOS">FCOS</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-24T09:46:37.000Z" title="Created 2023-11-24 17:46:37">2023-11-24</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:22.167Z" title="Updated 2023-11-29 18:05:22">2023-11-29</time></span></div><div class="content">

简介论文原文：论文原文)
在之前的一些目标检测网络中， 比如Faster RCNN, SSD, yolov2~v5都是基于Anchor进行预测，即在原图上生成一堆密密麻麻的Anchor Boxes， 然后网络基于这些Anchor去预测它们的类别、中心点偏移量以及宽高缩放因子得到网络预测输出的目标， 最后通过NMS(no-max-suppression非极大值抑制)即可得到最终预测目标。针对基于Anchor网络存在的问题， 原作者总结如下四点：
1、检测器的性能和Anchor的size以及aspect ratio相关， 比如在RetinaNet中改变Anchor(论文中说这是个超参数hyper-parameters)能够产生约4%的AP变化， 也就是说Anchor需要设置的合适才可以。
2、一般Anchor的size和aspect ratio（ 宽度/高度 的比值）都是固定的， 所以很难处理哪些形状变化很大的目标（比如一本书横着放于竖着放久不一样）。而且迁移到其他任务中时，如果新的数据集目标和预训练数据集中的目标形状差别很多， 一般需要重新设计Anchor。
3、为了达到更高的召回率 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/19/HRNet/" title="HRNet">HRNet</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-19T06:41:20.000Z" title="Created 2023-11-19 14:41:20">2023-11-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:35.998Z" title="Updated 2023-11-29 18:05:35">2023-11-29</time></span></div><div class="content">

简介论文原文： 论文原文)
HRNet是由中国科学技术大学和亚洲微软研究院在2019年共同发表的， 这篇文章中的HRNet（High-Resolution Net）是针对2D人体姿态估计任务提出的， 并且该网络主要针对单一人体姿态估计。人体姿态估计主要应用在人体行为识别、人机交互、动画制作等。

主要由两种深度学习方法处理Human Pose Estimation任务。
1、基于regression的方式， 即直接预测每个关键点的位置坐标
2、基于heatmap的方式， 即针对每个关键点预测一张热度图（预测出现在每个位置上的分数）

HRNet下图是根据源码绘制的HRNet-w32的模型结构简图， 文章中还提到了HENet-w48版本， 二者的区别在于每个模块所采用的通道个数不同， 网路的整体结构都一样， 而该论文的核心思想就是不断地去融合不同尺度上的信息， 也就是论文中说的Exchange Blocks。

从上图可以看出，HRNet首先通过两个卷积核大小为3x3步距为2的卷积层（后面接着BN以及ReLU）共下采样了4倍， 然后通过Layer1模块， 这里的Layer1其实和之前 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/17/Mask-R-CNN/" title="Mask_R-CNN">Mask_R-CNN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-17T09:53:10.000Z" title="Created 2023-11-17 17:53:10">2023-11-17</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:40.588Z" title="Updated 2023-11-29 18:05:40">2023-11-29</time></span></div><div class="content">

简介Mask R-CNN是2017年发表的文章， 一作是何恺明， 该论文也获得了ICCV 2017年最佳论文奖。并且网络提出后，又霸榜了MS COCO的各项任务，包括目标检测、实例分割以及人体关键点检测任务。

Mask R-CNN是在Faster R-CNN的基础上增加了一个用于预测目标分割Mask的分支（即可预测目标的Bounding Boxes信息、类别信息以及分割Mask信息）

Mask R-CNN不仅能够同时进行目标检测与分割， 还能很容易扩展到其他任务， 比如预测人体关键点信息。

Mask R-CNN的结构也很简单， 就是通过RoIAlign（在原Fast R-CNN中是RoIPool）得到RoI基础上并行添加一个Mask分支（小型的FCN), 见下图，之前Faster R-CNN是在RoI基础上接上一个Fast R-CNN检测头，即图中class, box分支， 现在又并行了一个Mask分支。

注意带和不带FPN结构的Mask R-CNN在Mask分支上略有不同， 对于带有FPN结构的Mask R-CNN它的class， box分支和Mask分支并不是共用一个R ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/17/YoloV5/" title="YoloV5">YoloV5</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-17T09:46:42.000Z" title="Created 2023-11-17 17:46:42">2023-11-17</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:06:36.976Z" title="Updated 2023-11-29 18:06:36">2023-11-29</time></span></div><div class="content">

简介YOLOv5项目的作者是Glenn Jocher并不是原Darknet项目的作者Joseph Redmon。并且这个项目至今都没有发表过正式的论文。YOLOV5仓库早在2020年5月就已创建， 如今已迭代多个版本。本文是针对V6.1版本展开的， 下表是V6.1版本中贴出的关于不同大小模型以及输入尺度对应的mAP、推理速度、参数数量以及理论计算的FLOPs。




Model
size
mAPval
mAPval
Speed
Speed
Speed
params
FLOPs





pixels
0.5:0.95
0.5
CPU b1(ms)
V100 b1(ms)
V100 b32(ms)
(M)
@640 (B)


YOLOv5n
640
28
45.7
45
6.3
0.6
1.9
4.5


YOLOv5s
640
37.4
56.8
98
6.4
0.9
7.2
16.5


YOLOv5m
640
45.4
64.1
224
8.2
1.7
21.2
49


YOLOv5l
640
49
67.3
430
10.1
2.7
46.5
109.1


YOLOv ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/#content-inner">4</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='https://gitee.com/guudman/blog_images/raw/master/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">AI4Future</div><div class="author-info__description">Not Only Look Once</div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/GuudMan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2663017379@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">微信公众号: AI4Future</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">32</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div></div></div></div></div></main><footer id="footer" style="background: #FFFFFF"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By AI4Future</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/chenxz21/hexo-theme-bcxm">Bcxm</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script type="text/javascript" src="https://fastly.jsdelivr.net/npm/leancloud-storage@4.10.0/dist/av-min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>