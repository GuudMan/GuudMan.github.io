<!DOCTYPE html><html lang="zh-cn" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Ai4Future</title><meta name="keywords" content="成长，分享，专注"><meta name="author" content="AI4Future"><meta name="copyright" content="AI4Future"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Not Only Look Once">
<meta property="og:type" content="website">
<meta property="og:title" content="Ai4Future">
<meta property="og:url" content="https://guudman.github.io/page/2/index.html">
<meta property="og:site_name" content="Ai4Future">
<meta property="og:description" content="Not Only Look Once">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://guudman.github.io/img/avatar.jpg">
<meta property="article:author" content="AI4Future">
<meta property="article:tag" content="成长，分享，专注">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://guudman.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://guudman.github.io/page/2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: AI4Future","link":"Link: ","source":"Source: Ai4Future","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://fastly.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://fastly.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Ai4Future',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2023-12-08 08:57:47'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Ai4Future" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="onerror=null;src='https://gitee.com/guudman/blog_images/raw/master/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">AI4Future</div><div class="author-info__description">Not Only Look Once</div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/GuudMan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2663017379@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/gedan"><i class="fa-fw fas fa-music"></i><span> 歌单</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk"><i class="fa-fw fa fa-heartbeat"></i><span> 时光</span></a></div><div class="menus_item"><a class="site-page" href="/shuoba"><i class="fa-fw fas fa-comment-dots"></i><span> 说吧</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/google"><span> 镜像</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://json.xbyzs.cf"><span> Json格式化</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://draw.xbyzs.cf"><span> Draw画布</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://enkey.xbyzs.cf"><span> EnKey</span></a></li></ul></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ai4Future</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/gedan"><i class="fa-fw fas fa-music"></i><span> 歌单</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk"><i class="fa-fw fa fa-heartbeat"></i><span> 时光</span></a></div><div class="menus_item"><a class="site-page" href="/shuoba"><i class="fa-fw fas fa-comment-dots"></i><span> 说吧</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/google"><span> 镜像</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://json.xbyzs.cf"><span> Json格式化</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://draw.xbyzs.cf"><span> Draw画布</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://enkey.xbyzs.cf"><span> EnKey</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/24/FCOS/" title="FCOS">FCOS</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-24T09:46:37.000Z" title="Created 2023-11-24 17:46:37">2023-11-24</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:22.167Z" title="Updated 2023-11-29 18:05:22">2023-11-29</time></span></div><div class="content">

简介论文原文：论文原文)
在之前的一些目标检测网络中， 比如Faster RCNN, SSD, yolov2~v5都是基于Anchor进行预测，即在原图上生成一堆密密麻麻的Anchor Boxes， 然后网络基于这些Anchor去预测它们的类别、中心点偏移量以及宽高缩放因子得到网络预测输出的目标， 最后通过NMS(no-max-suppression非极大值抑制)即可得到最终预测目标。针对基于Anchor网络存在的问题， 原作者总结如下四点：
1、检测器的性能和Anchor的size以及aspect ratio相关， 比如在RetinaNet中改变Anchor(论文中说这是个超参数hyper-parameters)能够产生约4%的AP变化， 也就是说Anchor需要设置的合适才可以。
2、一般Anchor的size和aspect ratio（ 宽度/高度 的比值）都是固定的， 所以很难处理哪些形状变化很大的目标（比如一本书横着放于竖着放久不一样）。而且迁移到其他任务中时，如果新的数据集目标和预训练数据集中的目标形状差别很多， 一般需要重新设计Anchor。
3、为了达到更高的召回率 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/19/HRNet/" title="HRNet">HRNet</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-19T06:41:20.000Z" title="Created 2023-11-19 14:41:20">2023-11-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:35.998Z" title="Updated 2023-11-29 18:05:35">2023-11-29</time></span></div><div class="content">

简介论文原文： 论文原文)
HRNet是由中国科学技术大学和亚洲微软研究院在2019年共同发表的， 这篇文章中的HRNet（High-Resolution Net）是针对2D人体姿态估计任务提出的， 并且该网络主要针对单一人体姿态估计。人体姿态估计主要应用在人体行为识别、人机交互、动画制作等。

主要由两种深度学习方法处理Human Pose Estimation任务。
1、基于regression的方式， 即直接预测每个关键点的位置坐标
2、基于heatmap的方式， 即针对每个关键点预测一张热度图（预测出现在每个位置上的分数）

HRNet下图是根据源码绘制的HRNet-w32的模型结构简图， 文章中还提到了HENet-w48版本， 二者的区别在于每个模块所采用的通道个数不同， 网路的整体结构都一样， 而该论文的核心思想就是不断地去融合不同尺度上的信息， 也就是论文中说的Exchange Blocks。

从上图可以看出，HRNet首先通过两个卷积核大小为3x3步距为2的卷积层（后面接着BN以及ReLU）共下采样了4倍， 然后通过Layer1模块， 这里的Layer1其实和之前 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/17/Mask-R-CNN/" title="Mask_R-CNN">Mask_R-CNN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-17T09:53:10.000Z" title="Created 2023-11-17 17:53:10">2023-11-17</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:40.588Z" title="Updated 2023-11-29 18:05:40">2023-11-29</time></span></div><div class="content">

简介Mask R-CNN是2017年发表的文章， 一作是何恺明， 该论文也获得了ICCV 2017年最佳论文奖。并且网络提出后，又霸榜了MS COCO的各项任务，包括目标检测、实例分割以及人体关键点检测任务。

Mask R-CNN是在Faster R-CNN的基础上增加了一个用于预测目标分割Mask的分支（即可预测目标的Bounding Boxes信息、类别信息以及分割Mask信息）

Mask R-CNN不仅能够同时进行目标检测与分割， 还能很容易扩展到其他任务， 比如预测人体关键点信息。

Mask R-CNN的结构也很简单， 就是通过RoIAlign（在原Fast R-CNN中是RoIPool）得到RoI基础上并行添加一个Mask分支（小型的FCN), 见下图，之前Faster R-CNN是在RoI基础上接上一个Fast R-CNN检测头，即图中class, box分支， 现在又并行了一个Mask分支。

注意带和不带FPN结构的Mask R-CNN在Mask分支上略有不同， 对于带有FPN结构的Mask R-CNN它的class， box分支和Mask分支并不是共用一个R ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/17/YoloV5/" title="YoloV5">YoloV5</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-17T09:46:42.000Z" title="Created 2023-11-17 17:46:42">2023-11-17</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:06:36.976Z" title="Updated 2023-11-29 18:06:36">2023-11-29</time></span></div><div class="content">

简介YOLOv5项目的作者是Glenn Jocher并不是原Darknet项目的作者Joseph Redmon。并且这个项目至今都没有发表过正式的论文。YOLOV5仓库早在2020年5月就已创建， 如今已迭代多个版本。本文是针对V6.1版本展开的， 下表是V6.1版本中贴出的关于不同大小模型以及输入尺度对应的mAP、推理速度、参数数量以及理论计算的FLOPs。




Model
size
mAPval
mAPval
Speed
Speed
Speed
params
FLOPs





pixels
0.5:0.95
0.5
CPU b1(ms)
V100 b1(ms)
V100 b32(ms)
(M)
@640 (B)


YOLOv5n
640
28
45.7
45
6.3
0.6
1.9
4.5


YOLOv5s
640
37.4
56.8
98
6.4
0.9
7.2
16.5


YOLOv5m
640
45.4
64.1
224
8.2
1.7
21.2
49


YOLOv5l
640
49
67.3
430
10.1
2.7
46.5
109.1


YOLOv ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/14/Yolov4/" title="Yolov4">Yolov4</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-14T10:54:43.000Z" title="Created 2023-11-14 18:54:43">2023-11-14</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:06:32.621Z" title="Updated 2023-11-29 18:06:32">2023-11-29</time></span></div><div class="content">

简介论文原文：
yolov1原文)
yolov2原文)
yolov3原文)
yolov4原文)
YOLOV4是2020年AlexEY Bochkovskiy等人发表在CVPR上的一篇文章， 并不是Darknet的原始作者Joseph Redmon发表的， 但这个工作已经被Jose Redmon大佬认可了。如果将YOLOV4和原始的YOLOV3相比较确实有很大的提升， 但和Utralytics版的YOLOV3 SPP相比提升确实不大， 但毕竟Ultralytics的YOLOv3 SPP以及YOLOv5都没有发表过正式的论文， 所以这里先聊聊Alexey Bochkovskit的YOLOV4。


YOLOv4的亮点
阅读原论文，会发信啊作者就是将当年所有的常用技术罗列了一遍，然后做了一堆消融实验。网络结构
在论文3.4章节中介绍了YOLOV4网络的具体结构
Backbone: CSPDarknet53
Neck: SPP, PAN
Head: YOLOv3
相比之前的yolov3, 改进了以下Backbone, 在Darknet53中引入了CSP模块（来自CSPNet）。在Neck部 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/12/Grad-CAM/" title="Grad-CAM">Grad-CAM</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-12T07:23:48.000Z" title="Created 2023-11-12 15:23:48">2023-11-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:30.799Z" title="Updated 2023-11-29 18:05:30">2023-11-29</time></span></div><div class="content">

1、简介论文原文： Grad_CAM
对于常用的深度学习网络（例如CNN， 普遍认为是个黑盒， 可解释性并不强）， 它为什么会这么预测， 它的关注点在哪里， 我们并不知道， 很多科研人员想方设法的探索其内在的联系， 也有很多相关的论文。 这篇Grad-CAM并不是最新的论文， 但是很有参考意义。通过Grad-CAM我们能够绘制出如下的热力图（对于给定类别， 网络到底关注哪些区域）。Grad-CAM（Gradient-weighted Classes Activation Mapping）是CAM（Class Activation Mapping）的升级版， Grad-CAM比CAM更具一般性。但CAM比较致命的问题是需要修改网络结构并重新训练， 而Grad-CAM完美避开了这些问题， 本文不对CAM进行讲解。

Grad-CAM能够帮助我们分析网络对于某个类别的关注区域， 那么我们通过网络关注的区域能够反过来分析网络是否正确学习到正确的特征或者信息。例如， 作者训练了一个二分类网络， Nurse和Doctor， 如下图， 第一列是预测时输入的原图， 第二列是Biased model ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/12/ConvNeXt/" title="ConvNeXt">ConvNeXt</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-12T01:02:56.000Z" title="Created 2023-11-12 09:02:56">2023-11-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:02.892Z" title="Updated 2023-11-29 18:05:02">2023-11-29</time></span></div><div class="content">

1、简介论文原文： ConvNeXt)
自从Vit（Vision Transformer)在CV领域大放异彩， 越来越多的研究人员开始涌入Transformer中， 2021年在CV领域的文章绝大多数都是基于Transformer的， 比如2021年的ICCV的best paper （Swin transformer）， 而卷积神经网络已经开始慢慢淡出舞台中央。 卷积网络要被Transformer取代 了吗， 也许会在不久的将来。2022年一份月， Facebook AI Research和UC Berkeley一起发表了一篇文章A ConvNet for the 2020s, 在文章中提出了ConVNeXt纯卷积神经网络， 它对标的就是2021年非常火的Swin Transformer， 通过一系列实验对比， 在相同的FLOPs下， ConvNeXt相比Swin Transformer拥有更快的推理速度以及更高的准确率， 在ImageNet 22k上ConvNeXt-XL达到了87.8%的准确率。

仔细阅读这篇文章， 你会发现ConvNeXt使用的都是现有的结构和方法， 无任何 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/10/DeepLab-V3/" title="DeepLab_V3">DeepLab_V3</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-10T10:11:19.000Z" title="Created 2023-11-10 18:11:19">2023-11-10</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:16.917Z" title="Updated 2023-11-29 18:05:16">2023-11-29</time></span></div><div class="content">

1、简介论文原文： 论文原文)

DeepLab V3是2017年发表在CVPR上的文章， 与DeepLab V2相比， 个人感觉有如下的三种变化：1）引入Multi-grid， 2）改进ASPP结构， 3）把CRFs后处理给移除掉了。

DeepLabV3两种模型结构文章中, 穿插讲解两种模型的实验, 这两种模型分别是cascded model和ASPP model。在cascded model中没有使用ASPP模块， 在ASPP model中没有使用cascaded blocks模块。注意， 虽然文中提出了两种结构， 但作者说ASPP model比cascaded model略好点， 包括在Github上开源的一些代码， 大部分也是用的ASPP model。

Both our best cascaded model (in Tab. 4) and ASPP model in（Tab. 6) (in both cases without Dense CRF post-processing or MS-COCO pre-training) already outperform D ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/09/DeepLab-V2/" title="DeepLab_V2">DeepLab_V2</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-09T11:34:38.000Z" title="Created 2023-11-09 19:34:38">2023-11-09</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:13.172Z" title="Updated 2023-11-29 18:05:13">2023-11-29</time></span></div><div class="content">

1、简介论文原文：论文原文](https://arxiv.org/pdf/1606.00915.pdf))
这是一篇2016年发表在CVPR上的文章， 其实对比模型结构图， 发现DeepLab v1与Deep Lab v2相比，其实就是换了一个backbone（VGG-&gt;ResNet， 换个backbone大概能涨3个点）， 然后引入了一个新的模块ASPP（Atros Spatial Pyramid Pooling）， 其他的没有太大区别。

DCNNs应用在语义分割任务中的问题和上一篇文章一样， 在文章的引言部分作者提出了DCNNs应用在语义分割任务中遇到的问题。 
分辨率被降低（主要由于下采样stride&gt;1的层导致）
目标的多尺度问题
DCNNs的不变性（invariance)会降低定位精度

文中对应的解决方法针对分辨率被降低的问题， 一般就是将最后的几个Maxpooling层的stride设置成1（如果是通过卷积下采样的， 比如resnet， 同样将stride设置成1即可)， 然后再配合使用膨胀卷积。 

In order to overcome this  ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/09/DeepLab-V1/" title="DeepLab_V1">DeepLab_V1</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-09T11:31:54.000Z" title="Created 2023-11-09 19:31:54">2023-11-09</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:08.559Z" title="Updated 2023-11-29 18:05:08">2023-11-29</time></span></div><div class="content">

1、简介论文原文： 论文原文](https://arxiv.org/pdf/1412.7062.pdf))
这篇文章最早发表于2014年， 是Google和UCLA等共同的杰作， 也是一篇很经典的论文。 DeepLab系列的第一篇论文， 因为已经过了很久，所以只是做简单的记录。 


2、语义分割中存在的问题在论文的引言中首先抛出了两个问题（针对语义分割任务）：信号下采样导致分辨率降低和空间不敏感问题。 
对于第一个问题信号下采样， 作者说主要是采用Maxpooling导致的， 为了解决这个问题， 作者引入了’atrous’ (with holes) algorithm（空洞卷积/膨胀卷积/扩张卷积）
对于第二个问题空间不敏感, 作者说分类器自身的问题（分类器本身具备一定空间不变性）。为解决这个问题作者采用了fully-connected CRF(Conditional Random Field)方法， 这个方法只在DeepLabv1-v2中使用到了， 从V3之后即不去使用了， 而且这个方法很耗时。

DeepLabV1的优势相比于之前的一些网络， 本文提出的网络具有以下优势：

 ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/#content-inner">3</a><a class="page-number" href="/page/4/#content-inner">4</a><a class="extend next" rel="next" href="/page/3/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='https://gitee.com/guudman/blog_images/raw/master/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">AI4Future</div><div class="author-info__description">Not Only Look Once</div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/GuudMan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2663017379@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">微信公众号: AI4Future</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">36</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div></div></div></div></div></main><footer id="footer" style="background: #FFFFFF"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By AI4Future</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/chenxz21/hexo-theme-bcxm">Bcxm</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script type="text/javascript" src="https://fastly.jsdelivr.net/npm/leancloud-storage@4.10.0/dist/av-min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>