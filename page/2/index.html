<!DOCTYPE html><html lang="zh-cn" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Ai4Future</title><meta name="keywords" content="成长，分享，专注"><meta name="author" content="AI4Future"><meta name="copyright" content="AI4Future"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Not Only Look Once">
<meta property="og:type" content="website">
<meta property="og:title" content="Ai4Future">
<meta property="og:url" content="https://guudman.github.io/page/2/index.html">
<meta property="og:site_name" content="Ai4Future">
<meta property="og:description" content="Not Only Look Once">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://guudman.github.io/img/avatar.jpg">
<meta property="article:author" content="AI4Future">
<meta property="article:tag" content="成长，分享，专注">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://guudman.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://guudman.github.io/page/2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: AI4Future","link":"Link: ","source":"Source: Ai4Future","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://fastly.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://fastly.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Ai4Future',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2023-12-01 18:11:46'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Ai4Future" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="onerror=null;src='https://gitee.com/guudman/blog_images/raw/master/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">AI4Future</div><div class="author-info__description">Not Only Look Once</div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/GuudMan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2663017379@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/gedan"><i class="fa-fw fas fa-music"></i><span> 歌单</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk"><i class="fa-fw fa fa-heartbeat"></i><span> 时光</span></a></div><div class="menus_item"><a class="site-page" href="/shuoba"><i class="fa-fw fas fa-comment-dots"></i><span> 说吧</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/google"><span> 镜像</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://json.xbyzs.cf"><span> Json格式化</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://draw.xbyzs.cf"><span> Draw画布</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://enkey.xbyzs.cf"><span> EnKey</span></a></li></ul></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ai4Future</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/gedan"><i class="fa-fw fas fa-music"></i><span> 歌单</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk"><i class="fa-fw fa fa-heartbeat"></i><span> 时光</span></a></div><div class="menus_item"><a class="site-page" href="/shuoba"><i class="fa-fw fas fa-comment-dots"></i><span> 说吧</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/google"><span> 镜像</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://json.xbyzs.cf"><span> Json格式化</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://draw.xbyzs.cf"><span> Draw画布</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://enkey.xbyzs.cf"><span> EnKey</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/12/ConvNeXt/" title="ConvNeXt">ConvNeXt</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-12T01:02:56.000Z" title="Created 2023-11-12 09:02:56">2023-11-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:02.892Z" title="Updated 2023-11-29 18:05:02">2023-11-29</time></span></div><div class="content">

1、简介论文原文： ConvNeXt)
自从Vit（Vision Transformer)在CV领域大放异彩， 越来越多的研究人员开始涌入Transformer中， 2021年在CV领域的文章绝大多数都是基于Transformer的， 比如2021年的ICCV的best paper （Swin transformer）， 而卷积神经网络已经开始慢慢淡出舞台中央。 卷积网络要被Transformer取代 了吗， 也许会在不久的将来。2022年一份月， Facebook AI Research和UC Berkeley一起发表了一篇文章A ConvNet for the 2020s, 在文章中提出了ConVNeXt纯卷积神经网络， 它对标的就是2021年非常火的Swin Transformer， 通过一系列实验对比， 在相同的FLOPs下， ConvNeXt相比Swin Transformer拥有更快的推理速度以及更高的准确率， 在ImageNet 22k上ConvNeXt-XL达到了87.8%的准确率。

仔细阅读这篇文章， 你会发现ConvNeXt使用的都是现有的结构和方法， 无任何 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/10/DeepLab-V3/" title="DeepLab_V3">DeepLab_V3</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-10T10:11:19.000Z" title="Created 2023-11-10 18:11:19">2023-11-10</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:16.917Z" title="Updated 2023-11-29 18:05:16">2023-11-29</time></span></div><div class="content">

1、简介论文原文： 论文原文)

DeepLab V3是2017年发表在CVPR上的文章， 与DeepLab V2相比， 个人感觉有如下的三种变化：1）引入Multi-grid， 2）改进ASPP结构， 3）把CRFs后处理给移除掉了。

DeepLabV3两种模型结构文章中, 穿插讲解两种模型的实验, 这两种模型分别是cascded model和ASPP model。在cascded model中没有使用ASPP模块， 在ASPP model中没有使用cascaded blocks模块。注意， 虽然文中提出了两种结构， 但作者说ASPP model比cascaded model略好点， 包括在Github上开源的一些代码， 大部分也是用的ASPP model。

Both our best cascaded model (in Tab. 4) and ASPP model in（Tab. 6) (in both cases without Dense CRF post-processing or MS-COCO pre-training) already outperform D ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/09/DeepLab-V2/" title="DeepLab_V2">DeepLab_V2</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-09T11:34:38.000Z" title="Created 2023-11-09 19:34:38">2023-11-09</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:13.172Z" title="Updated 2023-11-29 18:05:13">2023-11-29</time></span></div><div class="content">

1、简介论文原文：论文原文](https://arxiv.org/pdf/1606.00915.pdf))
这是一篇2016年发表在CVPR上的文章， 其实对比模型结构图， 发现DeepLab v1与Deep Lab v2相比，其实就是换了一个backbone（VGG-&gt;ResNet， 换个backbone大概能涨3个点）， 然后引入了一个新的模块ASPP（Atros Spatial Pyramid Pooling）， 其他的没有太大区别。

DCNNs应用在语义分割任务中的问题和上一篇文章一样， 在文章的引言部分作者提出了DCNNs应用在语义分割任务中遇到的问题。 
分辨率被降低（主要由于下采样stride&gt;1的层导致）
目标的多尺度问题
DCNNs的不变性（invariance)会降低定位精度

文中对应的解决方法针对分辨率被降低的问题， 一般就是将最后的几个Maxpooling层的stride设置成1（如果是通过卷积下采样的， 比如resnet， 同样将stride设置成1即可)， 然后再配合使用膨胀卷积。 

In order to overcome this  ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/09/DeepLab-V1/" title="DeepLab_V1">DeepLab_V1</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-09T11:31:54.000Z" title="Created 2023-11-09 19:31:54">2023-11-09</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:08.559Z" title="Updated 2023-11-29 18:05:08">2023-11-29</time></span></div><div class="content">

1、简介论文原文： 论文原文](https://arxiv.org/pdf/1412.7062.pdf))
这篇文章最早发表于2014年， 是Google和UCLA等共同的杰作， 也是一篇很经典的论文。 DeepLab系列的第一篇论文， 因为已经过了很久，所以只是做简单的记录。 


2、语义分割中存在的问题在论文的引言中首先抛出了两个问题（针对语义分割任务）：信号下采样导致分辨率降低和空间不敏感问题。 
对于第一个问题信号下采样， 作者说主要是采用Maxpooling导致的， 为了解决这个问题， 作者引入了’atrous’ (with holes) algorithm（空洞卷积/膨胀卷积/扩张卷积）
对于第二个问题空间不敏感, 作者说分类器自身的问题（分类器本身具备一定空间不变性）。为解决这个问题作者采用了fully-connected CRF(Conditional Random Field)方法， 这个方法只在DeepLabv1-v2中使用到了， 从V3之后即不去使用了， 而且这个方法很耗时。

DeepLabV1的优势相比于之前的一些网络， 本文提出的网络具有以下优势：

 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/08/SwinTransformer/" title="SwinTransformer">SwinTransformer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-08T10:07:06.000Z" title="Created 2023-11-08 18:07:06">2023-11-08</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:06:14.332Z" title="Updated 2023-11-29 18:06:14">2023-11-29</time></span></div><div class="content">

1、简介Swin Transformer是2012年微软研究院在ICCV上发表的一篇文章， 并荣获2021 ICCV最佳论文称号。 Swin Transformer网络是Transformer模型在视觉领域的又一次碰撞。 
论文名称：Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
论文地址： Swin Transformer

2、论文整体框架在正文开始之前， 先对比一下Swin transformer与Vision Transformers， 下图是swin Transformers文章中给出的图， 左边是Swin Transformers，右边是之前的Vision Transformers，通过对比可以发现有两点不同。

Swin Transformers使用了之前类似卷积神经网络的层次化构建方法（Hierachical feature maps）, 比如特征图尺寸有对图像下采样4倍的， 8倍的以及16倍的， 这样的backbone有助于在此基础上构建目标检测， 实例分割等任务。 而 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/07/TransposeConv/" title="TransposeConv">TransposeConv</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-07T02:02:47.000Z" title="Created 2023-11-07 10:02:47">2023-11-07</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:06:24.840Z" title="Updated 2023-11-29 18:06:24">2023-11-29</time></span></div><div class="content">

1、简介首先回顾一下普通卷积， 下图是以stride=1， padding=0， kernel_size=3为例， 假设输入特征图大小为4×4(假设输入输出都是单通道), 通过卷积后的特征图大小为2×2， 一般使用卷积的情况中， 要是特征图变小（stride&gt;1)， 要么保持不变(stride=1)， 当然也可以通过四周padding让特征图变大但是没有意义。


2、转置卷积转置卷积主要起到上采样的作用， 但转置卷积不是卷积的逆运算(一般卷积操作是不可逆的), 它只能恢复到原来的大小， 数值与原来的不一样。转置卷积的运算步骤可以归纳为以下几点：
在输入特征图元素间填充s-1行， 列0(其中s是转置卷积的步距)
在输入特征图四周填充p-1行， 列0（其中k表示转置卷积的kernel_size大小， p为转置卷积的padding， 注意这里的padding和卷积操作有些不同)
将卷积核参数上下、左右翻转
做正常卷积运算(填充0， 步距1)
下面假设输入的特征图为2×2（假设输入输出为单通道)， 通过转置卷积后的的得到4×4大小的特征图。 这里使用的转置卷积核大小k=3， str ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/05/VisionTransformer/" title="VisionTransformer">VisionTransformer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-05T03:13:51.000Z" title="Created 2023-11-05 11:13:51">2023-11-05</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-05T03:15:16.157Z" title="Updated 2023-11-05 11:15:16">2023-11-05</time></span></div><div class="content">

1、简介论文原文：Vit](https://arxiv.org/abs/2010.11929))
Transformer最初是针对NLP领域提出的， 并且在NLP领域大获成功。Vision-Transformer尝试将Transformer应用到CV领域。通过原论文中的实验发现，论文给出的模型在ImageNet1k上能够达到88.55%的准确率， 说明Transformer在CV领域确实能取得不错的效果。


2、模型详解原论文中，作者主要拿ResNet， Vit(纯Transformer模型)以及Hybrid（卷积和Transformer混合模型）三个模型进行比较。
下图是原论文中Vit的模型架构， 简单而言， 模型由三部分组成：
1、Linear Project of Flattened Patches(Embedding层)
2、Transformers Encoder(图右侧给出更加详细的结构)
3、MLP Head（用于分类的层结构）

Embedding层结构对于标准的Transformers模块， 要求输入的是token（向量）序列， 即二维矩阵[num_token, ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/04/Transofrmer/" title="Transofrmer">Transofrmer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-04T09:29:10.000Z" title="Created 2023-11-04 17:29:10">2023-11-04</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:06:18.454Z" title="Updated 2023-11-29 18:06:18">2023-11-29</time></span></div><div class="content">

1、简介论文原文：attention原文)
Transformer是2017年Google提出的， 当时主要是针对自然语言处理领域提出的。之前的RNN模型记忆长度有限且无法并行化，只有计算完t_i时刻后的数据才能计算t_{i+1}时刻的数据， 但Transformer都可以做到。在原论文中作者首先提出了self-attention的概念， 然后在此基础上提出Multi-Head Attention。本文针对self-attention以及multi-head attention的理论进行深入分析。

1、Self-Attention论文中的Transformer模型如下图所示：

下面针对Self-Attention展开说明。为了方便理解， 假设输入的序列长度为22， 输入就是两个节点x1, x2, 然后通过input Embedding也就是图中的f(x)， 将输入映射到a1， a2， 紧接着分别将a1， a2分别通过三个变换矩阵W_q, W_k, W_v（这三个参数是可训练的， 是共享的）， 得到对应的q^i, k^i, v^i, (在源码中，这部分直接使用全连接层实现的， 这 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/04/MobileNet/" title="MobileNet">MobileNet</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-04T01:32:47.000Z" title="Created 2023-11-04 09:32:47">2023-11-04</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:05:44.825Z" title="Updated 2023-11-29 18:05:44">2023-11-29</time></span></div><div class="content">

1、简介在之前的文章中讲的AlexNet， VGG， GoogleNet以及ResNet网络， 它们都是传统卷积神经网络（都是使用传统卷积层），缺点是内存需求大、运算量大而导致无法在移动设备以及嵌入式设备上运行， 这里要讲的MobileNet网络就是专门为移动端，嵌入式端而设计的。
1、MobileNetv1MobileNet模型是Google在2017年针对手机或嵌入式提出的轻量级模型， 专注于移动端或嵌入式设备中的轻量级CNN网络。相比于传统卷积神经网络， 在准确率小幅度降低的前提下大大减少模型参数与运算量。(相比VGG16准确率减少了0.9%,但模型参数只有VGG的1/32)
要说MobileNet网络的优点， 无疑是其中的Depthwise  Convolution结构（大大减少了运算量和参数数量)。下图展示了传统卷积与DW卷积的差异。在传统卷积中， 每个卷积核的channel与输入特征矩阵的channel相等(每个卷积核都会与输入特征矩阵的每一个维度进行卷积运算)。 
而在DW卷积中， 每个卷积核的channel都是等于1的(每个卷积核只负责输入特征矩阵的一个channe ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/04/ResNet/" title="ResNet">ResNet</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-04T01:27:40.000Z" title="Created 2023-11-04 09:27:40">2023-11-04</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-29T10:06:00.642Z" title="Updated 2023-11-29 18:06:00">2023-11-29</time></span></div><div class="content">

1、简介Resnet由微软实验室于2015年提出， 获得当年ImageNet竞赛分类任务第一名， 目标检测第一名。获得COCO数据集目标检测第一名， 图像分割第一名。
下图是ResNet34的简图。

网络的亮点

超深的网络结构(突破1000层)

提出residual模块(残差结构)

使用batch normalization加速训练(放弃使用dropout)


在ResNet网络提出之前， 传统的卷积神经网络都是通过一系列卷积层与下采样层进行堆叠得到的， 但是当网络堆叠到一定网络深度时， 就会出现如下两个问题：

梯度消失或梯度爆炸
退化问题(degradation problem)

在ResNet论文中说通过数据的预处理以及在网络中使用BN(batch normalization)层能够解决梯度消失或者梯度爆炸问题。但是对于退化问题（随着网络层数的加深， 效果还会变差）并无很好的解决方法。

所以ResNet论文提出了residual结构(残差结构)来减轻退化问题。下图是使用residual结构的卷积网络， 可以看到随着网络的不断加深， 效果并没有变差，反而变得更好了 ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/#content-inner">3</a><a class="extend next" rel="next" href="/page/3/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='https://gitee.com/guudman/blog_images/raw/master/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">AI4Future</div><div class="author-info__description">Not Only Look Once</div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/GuudMan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2663017379@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">微信公众号: AI4Future</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">30</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div></div></div></div></div></main><footer id="footer" style="background: #FFFFFF"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By AI4Future</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/chenxz21/hexo-theme-bcxm">Bcxm</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script type="text/javascript" src="https://fastly.jsdelivr.net/npm/leancloud-storage@4.10.0/dist/av-min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>