<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>彭能的博客</title>
  
  
  <link href="https://guudman.github.io/atom.xml" rel="self"/>
  
  <link href="https://guudman.github.io/"/>
  <updated>2023-11-04T01:34:40.080Z</updated>
  <id>https://guudman.github.io/</id>
  
  <author>
    <name>Neng Peng</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MobileNet</title>
    <link href="https://guudman.github.io/2023/11/04/MobileNet/"/>
    <id>https://guudman.github.io/2023/11/04/MobileNet/</id>
    <published>2023-11-04T01:32:47.000Z</published>
    <updated>2023-11-04T01:34:40.080Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>在之前的文章中讲的AlexNet， VGG， GoogleNet以及ResNet网络， 它们都是传统卷积神经网络（都是使用传统卷积层），缺点是<strong>内存需求大、运算量大而导致无法在移动设备以及嵌入式设备上运行</strong>， 这里要讲的MobileNet网络就是专门为移动端，嵌入式端而设计的。</p><h5 id="1、MobileNetv1"><a href="#1、MobileNetv1" class="headerlink" title="1、MobileNetv1"></a>1、<strong>MobileNetv1</strong></h5><p>MobileNet模型是Google在2017年针对手机或嵌入式提出的轻量级模型， 专注于移动端或嵌入式设备中的轻量级CNN网络。相比于传统卷积神经网络， 在准确率小幅度降低的前提下大大减少模型参数与运算量。(相比VGG16准确率减少了0.9%,但模型参数只有VGG的1&#x2F;32)</p><p>要说MobileNet网络的优点， 无疑是其中的Depthwise  Convolution结构（大大减少了运算量和参数数量)。下图展示了传统卷积与DW卷积的差异。在传统卷积中， 每个卷积核的channel与输入特征矩阵的channel相等(每个卷积核都会与输入特征矩阵的每一个维度进行卷积运算)。 </p><p>而在DW卷积中， 每个卷积核的channel都是等于1的(每个卷积核只负责输入特征矩阵的一个channel， 故卷积核的个数必须等于输入特征矩阵的channel， 从而使得输出特征矩阵的channel数也等于输入特征矩阵的channel数)</p><p><img src="/images/image-20231103154737433.png" alt="image-20231103154737433"></p><p>刚刚说了使用DW卷积后输出特征矩阵的channel是与输入特征矩阵的channel是相等的， 如果想改变&#x2F;自定义输出特征矩阵的channel， 那只需要在DW卷积后接一个PW卷积即可。如下图所示， 其实PW卷积就是普通的卷积而已(只不过卷积核大小为1)。通常DW卷积和PW卷积是放在一起使用的， 一起叫作Depthwise Separable Convolution（深度可分离卷积）</p><p><img src="/images/image-20231103155138897.png" alt="image-20231103155138897"></p><p>那Depthwise Separable Convolution（深度可分离卷积)与传统的卷积相比到底能节省多少计算量呢？</p><p>下面对比了这两个卷积方式的计算量， 其中Df是输入特征矩阵的宽高（这里假设宽和高相等）, Dk是卷积核的大小， M是输入特征矩阵的channel， N是输出特征矩阵的channel， 卷积计算量近似等于卷积核的高 × 卷积核的宽 × 卷积核的channel ×输入特征矩阵的高 × 输入特征矩阵的宽（假设stride&#x3D;1）。在mobilenet网络中，DW卷积都是使用3x3大小的卷积核， 所以理论上普通卷积计算量是DW+PW卷积的8到9倍。</p><p><img src="/images/image-20231103155948407.png" alt="image-20231103155948407"></p><p>下面分析一下mobilenet v1的网络结构， 左侧的表格是mobilenet v1的网络结构， 表中Conv表示普通卷积， Conv dw代表上面提到的DW卷积， s表示步距， 根据表格信息就能搭建出mobilenet v1网络。 在mobilenet v1原论文中， 还提出了两个超参数， 一个是$$\alpha$$， 一个是$$\beta$$,  $$\alpha$$参数是一个倍率因子， 用来调整卷积核的个数， $$\beta$$是控制输入网络的图像尺寸参数。下图右侧给出了使用不同$$\alpha$$和$$\beta$$网络的分类准确率、计算量以及模型参数。</p><p><img src="/images/image-20231103160611687.png" alt="image-20231103160611687"></p><h5 id="2、MobileNet-V2"><a href="#2、MobileNet-V2" class="headerlink" title="2、MobileNet V2"></a>2、<strong>MobileNet V2</strong></h5><p>在MobileNet v1的网络结构中能够发现， 网络的结构就像VGG一样是个直筒型的， 不像ResNet网络有shortcut连接方式， 而有人反映说MobileNet v1网络中DW卷积在训练时很容易废掉， 效果并没有那么理想。下一接下来看MobileNet v2网络。 </p><p>MobileNet v2网络是由google团队在2018年提出，相比MobileNet v1网络，准确率更高， 模型更小。 上面提到， MobileNet v1网络的亮点是DW卷积， 那么在MobileNet v2中的亮点是Inverted residual block（倒残差结构）, 如下图所示， 左侧是ResNet网络中的残差结构，右侧是Mobile Net v2中的倒残差结构。在残差结构中是1x1卷积降维-&gt;3x3卷积-&gt;1x1卷积升维， 在倒残差结构中正好相反， 是1x1卷积升维-&gt;3x3卷积-&gt;1x1卷积降维。 </p><p>为什么要这么做， 原文的解释是高维信息通过ReLU激活函数后丢失的信息更少（注意倒残差结构中基本使用的是ReLU6激活函数， 但是最后一个1x1的卷积层使用的是线性激活函数）</p><p><img src="/images/image-20231103170530054.png" alt="image-20231103170530054"></p><p>在使用倒残差结构时需要注意， 并不是所有的倒残差结构都有shortcut连接， 只有当stride&#x3D;1且输入特征矩阵与输出特征矩阵shape相同时才有shortcut连接（只有当shape相同时， 两个矩阵才有加法运算， 当stride&#x3D;1时并不能保证输入特征矩阵的channel与输出特征矩阵的channel相同）</p><p><img src="/images/image-20231103171132298.png" alt="image-20231103171132298"></p><p>下图是MobileNet v2网络的结构表， 其中t代表的是扩展因子（倒残差结构中第一个1x1卷积的扩展因子）， c代表输出特征矩阵的channel， n代表倒残差结构重复的次数， s代表步距(注意：这里的步距只是针对重复n次的第一层倒残差结构，后面的都默认为1)</p><p><img src="/images/image-20231104091906806.png" alt="image-20231104091906806"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;在之前的文章中讲的AlexNet， VGG， GoogleNet以及ResNet网络， 它们都是传统卷积神经网络（都是使用传</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>ResNet</title>
    <link href="https://guudman.github.io/2023/11/04/ResNet/"/>
    <id>https://guudman.github.io/2023/11/04/ResNet/</id>
    <published>2023-11-04T01:27:40.000Z</published>
    <updated>2023-11-04T02:54:14.464Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>Resnet由微软实验室于2015年提出， 获得当年ImageNet竞赛分类任务第一名， 目标检测第一名。获得COCO数据集目标检测第一名， 图像分割第一名。</p><p>下图是ResNet34的简图。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103142747004.png" alt="image"></p><p>网络的亮点</p><ul><li><p>超深的网络结构(突破1000层)</p></li><li><p>提出residual模块(残差结构)</p></li><li><p>使用batch normalization加速训练(放弃使用dropout)</p></li></ul><p>在ResNet网络提出之前， 传统的卷积神经网络都是通过一系列卷积层与下采样层进行堆叠得到的， 但是当网络堆叠到一定网络深度时， 就会出现如下两个问题：</p><ol><li>梯度消失或梯度爆炸</li><li>退化问题(degradation problem)</li></ol><p>在ResNet论文中说通过数据的预处理以及在网络中使用BN(batch normalization)层能够解决梯度消失或者梯度爆炸问题。但是对于退化问题（随着网络层数的加深， 效果还会变差）并无很好的解决方法。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103143453837.png" alt="image"></p><p>所以ResNet论文提出了residual结构(残差结构)来减轻退化问题。下图是使用residual结构的卷积网络， 可以看到随着网络的不断加深， 效果并没有变差，反而变得更好了。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103143719018.png" alt="image"></p><p>下面来分析一下论文中的残差结构(residual)。下图是论文中给出的两种残差结构，左边的残差结构是针对层数较少的网络， 例如ResNet18层和ResNet34层网络， 右边是针对网络层数较多的网络， 例如ResNet101, ResNet152等。</p><p>为什么深层网络要用右边的残差结构， 因为右边的残差结构能够减少网络参数与运算量。同样输入一个channel为256的特征矩阵， 如果使用左侧的残差结构大约需要1170648个参数， 但如果使用右侧的残差结构只需要69632个参数，因此在搭建深层网络时， 使用右侧的残差结构更合适。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103144556292.png" alt="image"></p><p>先对左边的残差结构(针对ResNet18&#x2F;34)进行分析， 如下图所示，该残差结构的主要分支是由两层3×3的卷积层组成， 而残差结构右侧的连接线是shortcut分支也叫做捷径分支(注意， 为了让主分支上的输出矩阵能够与捷径分支上的输出矩阵进行相加，必须保证这两个输出特征矩阵有相同的shape)。</p><p>仔细观察ResNet34网络结构，可以发现图中有一些虚线的残差结构， 在原论文中作者只是简单说这些虚线残差结构具有降维的作用。下图右侧给出了详细的虚线残差结构，注意<strong>每个卷积层的步距stride以及捷径分支上的卷积核的个数(与主分支上的卷积核个数相同)</strong></p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103144740973.png" alt="image"></p><p>接着再来分析针对ResNet50&#x2F;101&#x2F;152的残差结构， 如下图所示 ， 在该残差结构中，主分支使用了三个卷积层， 第一个是1x1的卷积层， 用来压缩channel维度， 第二个是3x3的卷积层， 第三个是1x1的卷积层用来还原channel维度（注意主分支上第一层卷积层和第二层卷积层所使用的卷积核个数是相同的，第三层是第一层的4倍)。该残差结构所对应的虚线残差结构如右侧图所示， 同样在捷径分支上有一个1x1的卷积层，它的卷积核个数与主分支上的第三层卷积核个数相同，注意每个卷积层的步距。**(注意： 原论文中， 在下图右侧虚线残差结构的主分支上， 第一个1×1卷积层的步距是2， 第二个3x3卷积层的步距是1。但是在pytorch官方实现过程中第一个1x1卷积层的步距是1， 第二个3x3卷积层步距是2， 这样做的好处是能够在top1上提升大概0.5%的准确率。可参考Resnet v1.5  <a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch">https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch</a>)** </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103145804618.png" alt="image"></p><p>pytorch官方说明</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103150713341.png" alt="image"></p><p>下表是原论文给出的不同深度的ResNet网络结构配置， 注意表中的残差结构给出了主分支上卷积核的大小与卷积核个数， 表中xN表示该残差结构重复N次</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103150928585.png" alt="image"></p><p>那到底哪些残差结构是虚线残差结构呢？</p><p>对于ResNet18&#x2F;34&#x2F;50&#x2F;101&#x2F;151, 表中conv3_x, conv4_x, conv5_x所对应的一系列残差结构的<strong>第一层残差结构都是虚线残差结构</strong>。引文这一系列残差结构的第一层都有调整输入特征矩阵shape的使命(将特征矩阵的高和宽缩减为原来的一半， 将深度channel调整成下一层残差结构所需的channel)。 </p><p>为了方便理解， 下面给出了ResNet34的网络结构图， 图中简单标注了一些信息。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103151759454.png" alt="image"></p><p>对于ResNet50&#x2F;101&#x2F;152，其实在conv2_x所对应的一系列残差结构的第一层也是虚线残差结构， 因为它需要调整输入特征矩阵的channel， 根据表格可知， 通过3x3的maxpool之后输出的特征矩阵的shape应该是[56, 56, 64],但是conv2_x所对应的一系列残差结构中实线残差结构的期望输入特征矩阵的shape是[56, 56, 256], (因为这样才能保证输入输出特征矩阵shape相同，才能将捷径分支的输出与主分支的输出进行相加)。所以第一层残差结构需要将shape从[56, 56, 64]调整为-&gt;[56, 56, 256]。注意，这里只调整channel维度， 高和宽不变(而conv3_x, conv4_x, conv5_x所对饮的一系列残差结构的第一层虚线残差结构不仅要调整channel，还要将高度和宽度缩减为原来的一半。）</p><h4 id="2、实现"><a href="#2、实现" class="headerlink" title="2、实现"></a>2、实现</h4><h5 id="1、pytorch实现"><a href="#1、pytorch实现" class="headerlink" title="1、pytorch实现"></a>1、pytorch实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_resnet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    注意：在原论文中，在虚线残差结构的主分支上， 第一个1x1卷积层的strid是2，第二个3x3卷积层的stride是1。</span></span><br><span class="line"><span class="string">    但是在pytorch官方实现过程中是第一个1x1卷积层的stride1， 第二个3x3卷积层的stride是2</span></span><br><span class="line"><span class="string">    这么做的好处是能够在top1上提升大约0.5%的准确率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                groups=<span class="number">1</span>, width_per_group=<span class="number">64</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line">        width = <span class="built_in">int</span>(out_channel * (width_per_group / <span class="number">64.</span>)) * groups</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,</span><br><span class="line">                               groups=groups, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>,</span><br><span class="line">                               bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(width)</span><br><span class="line">        <span class="comment"># -----------------------------------------------</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=width, out_channels=width,</span><br><span class="line">                               groups=groups, kernel_size=<span class="number">3</span>, stride=stride,</span><br><span class="line">                               bias=<span class="literal">False</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(width)</span><br><span class="line">        <span class="comment"># -----------------------------------------------</span></span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel * self.expansion,</span><br><span class="line">                               kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block,</span></span><br><span class="line"><span class="params">                 block_num,</span></span><br><span class="line"><span class="params">                 num_classes=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 include_top=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 width_per_group=<span class="number">64</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">        self.include_top = include_top</span><br><span class="line">        self.in_channel = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">        self.groups = groups</span><br><span class="line">        self.width_per_group = width_per_group</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=self.in_channel,</span><br><span class="line">                               kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(self.in_channel)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.layer1 = self._make_layer(block, <span class="number">64</span>, block_num[<span class="number">0</span>])</span><br><span class="line">        self.layer2 = self._make_layer(block, <span class="number">128</span>, block_num[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer(block, <span class="number">256</span>, block_num[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer(block, <span class="number">512</span>, block_num[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> self.include_top:</span><br><span class="line">            self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># output size = (1, 1)</span></span><br><span class="line">            self.fc = nn.Linear(<span class="number">512</span> * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, channel, block_num, stride=<span class="number">1</span></span>):</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.in_channel != channel * block.expansion:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.in_channel, channel * block.expansion,</span><br><span class="line">                          kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(channel * block.expansion))</span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.in_channel,</span><br><span class="line">                            channel,</span><br><span class="line">                            downsample=downsample,</span><br><span class="line">                            stride=stride,</span><br><span class="line">                            groups=self.groups,</span><br><span class="line">                            width_per_group=self.width_per_group))</span><br><span class="line">        self.in_channel = channel * block.expansion</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num):</span><br><span class="line">            layers.append(block(self.in_channel,</span><br><span class="line">                                channel,</span><br><span class="line">                                groups=self.groups,</span><br><span class="line">                                width_per_group=self.width_per_group))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line">        <span class="keyword">if</span> self.include_top:</span><br><span class="line">            x = self.avgpool(x)</span><br><span class="line">            x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">            x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet34</span>(<span class="params">num_classe=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet34-333f7ec4.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(BasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classe,</span><br><span class="line">                  include_top=include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet50-19c8e357.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet101</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet101-5d3b4d8f.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50_32x4d</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth</span></span><br><span class="line">    groups = <span class="number">32</span></span><br><span class="line">    width_per_group = <span class="number">4</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top,</span><br><span class="line">                  groups=groups,</span><br><span class="line">                  width_per_group=width_per_group)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50_32x8d</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth</span></span><br><span class="line">    groups = <span class="number">32</span></span><br><span class="line">    width_per_group = <span class="number">8</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top,</span><br><span class="line">                  groups=groups,</span><br><span class="line">                  width_per_group=width_per_group)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.rand((<span class="number">4</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">resnet = resnet101(<span class="number">5</span>)</span><br><span class="line">out = resnet(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2、TensorFlow实现"><a href="#2、TensorFlow实现" class="headerlink" title="2、TensorFlow实现"></a>2、TensorFlow实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_resnet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：0399</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, Model, Sequential</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(layers.Layer):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_channel, strides=<span class="number">1</span>, downsample=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = layers.Conv2D(out_channel, kernel_size=<span class="number">3</span>, strides=strides,</span><br><span class="line">                                   padding=<span class="string">&quot;SAME&quot;</span>, use_bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>)</span><br><span class="line">        <span class="comment"># ----------------------------------------</span></span><br><span class="line">        self.conv2 = layers.Conv2D(out_channel, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                                   padding=<span class="string">&quot;SAME&quot;</span>, use_bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>)</span><br><span class="line">        <span class="comment"># ----------------------------------------</span></span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.relu = layers.ReLU()</span><br><span class="line">        self.add = layers.Add()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">False</span></span>):</span><br><span class="line">        identity = inputs</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(inputs)</span><br><span class="line"></span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        x = self.bn1(x, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x, training=training)</span><br><span class="line"></span><br><span class="line">        x = self.add([x, identity])</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottlenect</span>(layers.Layer):</span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_channel, strides=<span class="number">1</span>, downsample=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottlenect, self).__init__()</span><br><span class="line">        self.conv1 = layers.Conv2D(out_channel, kernel_size=<span class="number">1</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&quot;conv1&quot;</span>)</span><br><span class="line">        self.bn1 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&quot;conv1/BatchNorm&quot;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------</span></span><br><span class="line">        self.conv2 = layers.Conv2D(out_channel, kernel_size=<span class="number">3</span>, use_bias=<span class="literal">False</span>,</span><br><span class="line">                                   strides=strides, padding=<span class="string">&quot;SAME&quot;</span>, name=<span class="string">&quot;conv2&quot;</span>)</span><br><span class="line">        self.bn2 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&quot;conv2/BatchNorm&quot;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------</span></span><br><span class="line">        self.conv3 = layers.Conv2D(out_channel * self.expansion, kernel_size=<span class="number">1</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&quot;conv3&quot;</span>)</span><br><span class="line">        self.bn3 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&quot;conv3/BatchNorm&quot;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------</span></span><br><span class="line">        self.relu = layers.ReLU()</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.add = layers.Add()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">False</span></span>):</span><br><span class="line">        identity = inputs</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(inputs)</span><br><span class="line"></span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        x = self.bn1(x, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.bn3(x, training=training)</span><br><span class="line">        x = self.add([identity, x])</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">block, in_channel, channel, block_num, name, strides=<span class="number">1</span></span>):</span><br><span class="line">    downsample = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> strides != <span class="number">1</span> <span class="keyword">or</span> in_channel != channel * block.expansion:</span><br><span class="line">        downsample = Sequential([</span><br><span class="line">            layers.Conv2D(channel * block.expansion, kernel_size=<span class="number">1</span>, strides=strides,</span><br><span class="line">                          use_bias=<span class="literal">False</span>, name=<span class="string">&quot;conv1&quot;</span>),</span><br><span class="line">            layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1.001e-5</span>, name=<span class="string">&quot;BatchNorm&quot;</span>)</span><br><span class="line">        ], name=<span class="string">&quot;shortcut&quot;</span>)</span><br><span class="line">    layers_list = []</span><br><span class="line">    layers_list.append(block(channel, downsample=downsample, strides=strides, name=<span class="string">&quot;unit_1&quot;</span>))</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num):</span><br><span class="line">        layers_list.append(block(channel, name=<span class="string">&quot;unit_&quot;</span> + <span class="built_in">str</span>(index + <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">return</span> Sequential(layers_list, name=name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_resnet</span>(<span class="params">block, block_num, im_width, im_height, num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># TensorFlow中tensor的通道顺序 NHWC</span></span><br><span class="line">    input_image = layers.Input(shape=(im_height, im_width, <span class="number">3</span>), dtype=<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;SAME&quot;</span>,</span><br><span class="line">                      use_bias=<span class="literal">False</span>, name=<span class="string">&quot;conv1&quot;</span>)(input_image)</span><br><span class="line">    x = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&quot;conv1/BatchNorm&quot;</span>)(x)</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;SAME&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = _make_layer(block, x.shape[-<span class="number">1</span>], <span class="number">64</span>, block_num[<span class="number">0</span>], name=<span class="string">&quot;block1&quot;</span>)(x)</span><br><span class="line">    x = _make_layer(block, x.shape[-<span class="number">1</span>], <span class="number">128</span>, block_num[<span class="number">1</span>], strides=<span class="number">2</span>, name=<span class="string">&quot;block2&quot;</span>)(x)</span><br><span class="line">    x = _make_layer(block, x.shape[-<span class="number">1</span>], <span class="number">256</span>, block_num[<span class="number">2</span>], strides=<span class="number">2</span>, name=<span class="string">&quot;block3&quot;</span>)(x)</span><br><span class="line">    x = _make_layer(block, x.shape[-<span class="number">1</span>], <span class="number">512</span>, block_num[<span class="number">3</span>], strides=<span class="number">2</span>, name=<span class="string">&quot;block4&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> include_top:</span><br><span class="line">        x = layers.GlobalAvgPool2D()(x)</span><br><span class="line">        x = layers.Dense(num_classes, name=<span class="string">&quot;logits&quot;</span>)(x)</span><br><span class="line">        predict = layers.Softmax()(x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        predict = x</span><br><span class="line">    model = Model(inputs=input_image, outputs=predict)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet34</span>(<span class="params">im_width=<span class="number">224</span>, im_height=<span class="number">224</span>, num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">return</span> _resnet(BasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], im_height, im_width, num_classes, include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50</span>(<span class="params">im_width=<span class="number">224</span>, im_height=<span class="number">224</span>, num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">return</span> _resnet(Bottlenect, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], im_height, im_width, num_classes, include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet101</span>(<span class="params">im_width=<span class="number">224</span>, im_height=<span class="number">224</span>, num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">return</span> _resnet(Bottlenect, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], im_height, im_width, num_classes, include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="built_in">input</span> = tf.random.uniform((<span class="number">8</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line">model = resnet34()</span><br><span class="line"><span class="built_in">print</span>(model(<span class="built_in">input</span>))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;Resnet由微软实验室于2015年提出， 获得当年ImageNet竞赛分类任务第一名， 目标检测第一名。获得COCO数据集</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>BatchNormalization</title>
    <link href="https://guudman.github.io/2023/11/04/BatchNormalization/"/>
    <id>https://guudman.github.io/2023/11/04/BatchNormalization/</id>
    <published>2023-11-04T01:22:37.000Z</published>
    <updated>2023-11-04T01:24:16.763Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>BatchNormalization原文： <a href="%5Barxiv.org/pdf/1502.03167.pdf%5D(https://arxiv.org/pdf/1502.03167.pdf)">BN原论文</a></p><p>Batch Normalization是google团队在2015年提出的， 该方法能够加速网络的收敛并提高准确率。</p><p><img src="/images/image-20231102153151497.png" alt="image-20231102153151497"></p><p>本文分为以下几个部分：</p><ol><li>BN的原理</li><li>使用pytorch验证本文观点</li><li>BN使用注意事项</li></ol><h4 id="2、Batch-Normalization原理"><a href="#2、Batch-Normalization原理" class="headerlink" title="2、Batch Normalization原理"></a>2、Batch Normalization原理</h4><p>在图像预处理中通常会对图像进行标准化处理， 这样能够加速网络的收敛， 对于Conv1来说， 输入就是满足某一分布的特征矩阵， 但是对Conv2而言的feature map就不一定满足某一分布规律了(<strong>注意这里所说的满足某一分布规律并不是指某一个feature map的数据要满足分布规律， 理论上指整个训练样本集所对应的feature map的数据要满足分布规律</strong>)。而我们的Batch Normalization的目的就是使我们的feature map满足均值为0，方差为1的分布规律。</p><p><img src="/images/image-20231102161021114.png" alt="image-20231102161021114"></p><p>下面是从原论文中截取的原话</p><p><img src="/images/image-20231102163727872.png" alt="image-20231102163727872"></p><p>对于一个拥有d维的输入x， 我们将对它的每一个维度进行标准化处理， 假设我们输入的x是RGB三通道的彩色图像，这里的d就是输入图像的channels即d&#x3D;3， $$x&#x3D;(x^{(1)}, x^{(2)}, x^{(3)})$$, 其中$$x^{(1)}$$代表的就是R通道对应的特征矩阵， 以此类推。标准化处理也是<strong>分别</strong>对我们的R通道， G通道， B通道进行处理。</p><p>原文中提供了更加相似的计算公式。</p><p><img src="/images/image-20231102164825714.png" alt="image-20231102164825714"></p><p>上面提到让<strong>feature map满足某一分布规律， 理论上指整个训练样本集所对应的feature map的数据要满足分布规律</strong>，也就是说要算出整个训练集的feature map然后再进行标准化处理。对于一个大型的数据集明显是不可能的，所以论文中说的是batch normalization， 也就是我们计算一个batch数据的feature map然后再进行标准化(batch越大越接近整个数据集的分布， 效果越好) .</p><p>根据上面的公式可以知道$$\mu_\beta$$代表计算的feature map每个维度（channel)的均值，<strong>注意$$\mu_\beta$$是一个向量，而不是一个值</strong>， $$\mu_\beta$$向量的每个元素代表一个维度(channel)的均值。$$\sigma^2_\beta$$代表计算的feature map每个维度(channels)的方差， <strong>$$\sigma^2_\beta$$是一个向量， 而不是一个值</strong>， $$\sigma^2_\beta$$向量的每一个元素代表一个维度(channel)的方差， 然后根据$$\mu_\beta$$和$$\sigma^2_\beta$$计算标准化处理得到的值。下图给出了一个计算$$\mu_\beta$$和$$\sigma^2_\beta$$的示例。</p><p><img src="/images/image-20231102171333525.png" alt="image-20231102171333525"></p><p>$$x^{(1)}&#x3D;{1, 1, 1, 2, 0, -1, 2, 2}$$  序列(1, 1, 1, 2, 0, -1, 2, 2)的均值为1， 方差为1</p><p>$$x^{(2)}&#x3D;{-1, 1, 0, 1, 0, -1, 3, 2}$$ 序列(-1, 1, 0, 1, 0, -1, 3, 2)的均值为0.5， 方差为1.5</p><p>$$\mu_1&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}x^{(1)}<em>i&#x3D;1$$$$\sigma^2_1&#x3D;\frac{1}{m}\sum</em>{i&#x3D;1}^{m}(x^{(1)}_i - \mu_1)^2&#x3D;1$$</p><p>$$\mu_2&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}x^{(2)}<em>i&#x3D;0.5$$$$\sigma^2_2&#x3D;\frac{1}{m}\sum</em>{i&#x3D;1}^{m}(x^{(2)}_i - \mu_2)^2&#x3D;1.5$$</p><p>所以可以得出<br>$$<br>\mu&#x3D;\left[<br>\begin{matrix}<br>  1\<br>  0.5<br> \end{matrix}<br> \right]<br>$$</p><p>$$<br>\sigma^2&#x3D;\left[<br>\begin{matrix}<br>  1\<br>  1.5<br> \end{matrix}<br> \right]<br>$$</p><p>上面的示例展示了一个batch为2(两张图片)的Batch Normalization的计算过程， 假设feature1， feature2分别是由image1、image2经过一系列卷积池化后的得到的特征矩阵， feature的channel为2， 那么$$x^{(1)}$$代表该batch的所有feature的channel1的数据， 同理$$x^{(2)}$$代表该batch的所有feature的channel2的数据。然后分别计算$$x^{(1)}$$和$$x^{(2)}$$的均值和方差， 得到$$\mu$$和$$\sigma^2$$两个向量。</p><p>然后再根据标准差计算公式<strong>分别</strong>计算每个channel的值(公式中的$$\varepsilon$$是一个很小的常量， 防止分母为零的情况)。 </p><p>batch normalization之后，每个元素的计算公式为：<br>$$<br>x’_i&#x3D;\frac{x_i - \mu}{\sqrt{\sigma^2 + \varepsilon}}<br>$$<br>网络训练过程中， 通过一个batch一个batch的数据进行训练， 但是在预测过程中通常是输入一张图片进行预测，因此预测是batch size&#x3D;1， 如果再通过上述方法计算均值和方差就没有意义了。</p><p>所以在训练过程中要去不断地计算每个batch的均值和方差， 并使用移动平均(moving average)的方法记录统计的均值和方差。在训练完成后我们可以近似认为所统计的均值和方法就等于整个训练集的均值和方差。然后在验证以及预测过程中， 就使用统计得到的均值和方差进行标准化处理。</p><p>其实还可以发现论文中还有$$\gamma$$和$$\beta$$两个参数， $$\gamma$$用来调整数值分布的方差大小， $$\beta$$用来调整数据均值的位置。这两个参数是在反向传播过程中学习得到的， $$\gamma$$默认为1， $$\beta$$默认为0。</p><h4 id="2、使用pytorch进行试验"><a href="#2、使用pytorch进行试验" class="headerlink" title="2、使用pytorch进行试验"></a>2、使用pytorch进行试验</h4><p>上面提到，在训练过程中， 均值$$\mu_{now}$$和方差$$\sigma^2_{now}$$是通过计算当前批次数据得到的,而在<strong>验证和预测</strong>过程中使用的均值和方差是一个统计量，记为$$\mu_{statistic}$$和$$\sigma^2_{statistic}$$。</p><p>$$\mu_{statistic}$$和$$\sigma^2_{statistic}$$的更新策略如下， 其中momentum默认取0.1<br>$$<br>\mu_{statistic + 1} &#x3D; (1 - momentum) * \mu_{statistic} + momentum * \mu_{now} \<br>\sigma^2_{statistic + 1} &#x3D; (1 - momentum)* \sigma^2_{statistic} + momentum<em>\sigma^2_{statistic}<br>$$<br>这里要注意， 在pytorch中对当前批次feature进行bn处理时使用的$$\sigma^2_{now}$$是<strong>总体标准差</strong>， 计算公式如下：<br>$$<br>\sigma^2_{now} &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{m}(x_i - \mu_{now})^2<br>$$<br>在更新统计量$$\sigma^2_{statistic}$$时采用的$$\sigma^2_{now}$$是*<em>样本标准差</em></em>, 计算公式如下：<br>$$<br>\sigma^2_{now} &#x3D; \frac{1}{m-1}\sum_{i&#x3D;1}^{m}(x_i - \mu_{now})^2<br>$$<br>下面是使用pytorch做的测试， </p><ul><li>bn_process函数是自定义的bn处理方法， 验证是否和使用官方bn处理方法结果一致。在bn_process中计算输入batch数据的每个维度(这里的维度是channel维度)的均值和标准差(标准差等于方差开平方)。然后通过计算得到的均值和<strong>总体标准差</strong>对feature每个维度进行标准化， 然后使用均值和<strong>样本标准差</strong>更新计算均值和标准差。</li><li>初始化统计均值是一个元素为0的向量， 元素个数等于channel的深度， 初始化统计方差是一个元素为1的向量， 元素个数等于channel的深度， 初始化为$$\gamma&#x3D;1, \beta&#x3D;0$$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : pytorch_batch_normalization.py</span></span><br><span class="line"><span class="string"># @Time       ：2023/11/3 11:13</span></span><br><span class="line"><span class="string"># @Author     ：0399</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bn_process</span>(<span class="params">feature, mean, var</span>):</span><br><span class="line">    feature_shape = feature.shape</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(feature_shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="comment"># [batch, channel, height, width]</span></span><br><span class="line">        feature_t = feature[:, i, :, :]</span><br><span class="line">        mean_t = feature_t.mean()</span><br><span class="line">        <span class="comment"># 总体标准差</span></span><br><span class="line">        std_t1 = feature_t.std()</span><br><span class="line">        <span class="comment"># 样本标准差  当ddof=1时，计算的是样本的标准差</span></span><br><span class="line">        std_t2 = feature_t.std(ddof=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># bn process</span></span><br><span class="line">        <span class="comment"># 这里记得加上eps和pytorch保持一致</span></span><br><span class="line">        feature[:, i, :, :] = (feature[:, i, :, :] - mean_t) / np.sqrt(std_t1 ** <span class="number">2</span> + <span class="number">1e-5</span>)</span><br><span class="line">        <span class="comment"># update calculating mean and var</span></span><br><span class="line">        mean[i] = mean[i] * <span class="number">0.9</span> + mean_t * <span class="number">0.1</span></span><br><span class="line">        var[i] = var[i] * <span class="number">0.9</span> + (std_t2 ** <span class="number">2</span>) * <span class="number">0.1</span></span><br><span class="line">    <span class="built_in">print</span>(feature)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成一个batch为2， channel为2， height, width均为2的特征向量</span></span><br><span class="line">feature1 = torch.randn(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 初始化均值和方差</span></span><br><span class="line">calculate_mean = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">calculate_var = [<span class="number">1.0</span>, <span class="number">1.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意使用copy()深拷贝</span></span><br><span class="line">bn_process(feature1.numpy().copy(), calculate_mean, calculate_var)</span><br><span class="line"></span><br><span class="line">bn = nn.BatchNorm2d(<span class="number">2</span>, eps=<span class="number">1e-5</span>)</span><br><span class="line">output = bn(feature1)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># feature</span></span><br><span class="line"><span class="string">[[[[ 0.648684   -0.85507095]</span></span><br><span class="line"><span class="string">   [ 0.58417344  1.8898206 ]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[ 0.97886676  1.3034139 ]</span></span><br><span class="line"><span class="string">   [-1.1044223  -0.48206437]]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[[-0.20273271  0.27278942]</span></span><br><span class="line"><span class="string">   [-1.3778524  -0.95981157]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[ 1.0157013  -1.6016374 ]</span></span><br><span class="line"><span class="string">   [-0.43428668  0.324429  ]]]]</span></span><br><span class="line"><span class="string"># output</span></span><br><span class="line"><span class="string">tensor([[[[ 0.6487, -0.8551],</span></span><br><span class="line"><span class="string">          [ 0.5842,  1.8898]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 0.9789,  1.3034],</span></span><br><span class="line"><span class="string">          [-1.1044, -0.4821]]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[[-0.2027,  0.2728],</span></span><br><span class="line"><span class="string">          [-1.3779, -0.9598]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 1.0157, -1.6016],</span></span><br><span class="line"><span class="string">          [-0.4343,  0.3244]]]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>结果明显一样，只是精度不同。</p><h4 id="4、使用BN时需要注意的问题"><a href="#4、使用BN时需要注意的问题" class="headerlink" title="4、使用BN时需要注意的问题"></a>4、使用BN时需要注意的问题</h4><ul><li><p>训练时training参数设置为true， 验证时设置为false。在pytorch中可通过创建模型的model.train()和model.eval()方法控制。</p></li><li><p>batch size尽可能设置大一点， 设置很小后表现很糟糕， 设置越大，求解出来的均值和方差越接近整个训练集的均值和方差。</p></li><li><p>建议将bn层凡在conv和激活层之间，且卷积层不要使用偏置bias， 因为设置了偏置，最后的结果也一样。</p></li></ul><p>$$<br>y_i &#x3D; \frac{x_i - \mu(x)}{\sqrt{\sigma^2(x)}} \<br>y_i^b &#x3D; \frac{x_i^b - \mu(x^b)}{\sqrt{\sigma^2(x^b)}} \<br>x_i^b &#x3D; x_i + b \<br> \<br>\mu(x^b) &#x3D; \mu(x) + b \<br>\sigma^2(x^b) &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{m}[x_i^b-\mu(x^b)]^2 \<br>              &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{m}[x_i + b - \mu(x) - b]^2 \<br>              &#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}[x_i - \mu(x)]^2  \<br>              &#x3D;\sigma^2(x) \<br>y_i^b &#x3D; \frac{x_i^b - \mu(x^b)}{\sqrt{\sigma^2(x^b)}}&#x3D;\frac{x_i + b - \mu(x) - b}{\sqrt{\sigma^2(x^b)}} &#x3D; y_i<br>$$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;BatchNormalization原文： &lt;a href=&quot;%5Barxiv.org/pdf/1502.03167.pdf</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的卷积</title>
    <link href="https://guudman.github.io/2023/11/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF/"/>
    <id>https://guudman.github.io/2023/11/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF/</id>
    <published>2023-11-02T12:07:13.000Z</published>
    <updated>2023-11-02T12:08:57.969Z</updated>
    
    <content type="html"><![CDATA[<h4 id="深度学习中什么是卷积，-它是如何工作的"><a href="#深度学习中什么是卷积，-它是如何工作的" class="headerlink" title="深度学习中什么是卷积， 它是如何工作的"></a>深度学习中什么是卷积， 它是如何工作的</h4><h5 id="1、数学物理层面"><a href="#1、数学物理层面" class="headerlink" title="1、数学物理层面"></a>1、数学物理层面</h5><p>数学上，连续卷积的定义如下：</p><p>(f * g)(t) &#x3D; ∫[f(τ) * g(t - τ)] dτ</p><p>其中，* 表示卷积操作，f(t) 和 g(t) 是两个函数，(f * g)(t) 是它们的卷积结果。</p><p>它的物理含义可以理解为：系统某一时刻的输出是由多个输入共同作用（叠加）的结果。</p><p>具体解释如下：</p><p>假设有两个物理量（例如信号，场或系统响应等），可用公式中的函数f(t) 和 g(t)表示。在物理上，卷积大概可以理解为：系统某一时刻的输出是由多个输入共同作用(叠加)的结果。</p><p>g(t) 是系统的响应函数（或滤波器）。它描述了系统对输入信号 f(t) 的响应方式。g(t) 中的每一个值表示了在时间 t 时，系统对输入信号的加权响应。</p><p>在卷积过程中，我们考虑了 f(t) 中的每个时间点 τ，并将其与 g(t - τ) 相乘。这相当于在时间轴上对 g(t - τ) 进行了平移，然后与 f(t) 相乘。这个过程捕捉了在不同时间点上，系统响应函数 g(t) 对输入信号 f(t) 的影响。</p><p>最后，通过对所有时间点的乘积进行积分，我们将所有加权的响应值进行累加。这就得到了在每个时间点 t 上的卷积结果 (f * g)(t)。该结果描述了输入信号 f(t) 通过系统响应函数 g(t) 后的响应情况。</p><p>因此，卷积公式提供了一种描述系统响应、信号传播和叠加效应的数学工具。它在物理学中被广泛应用，例如信号处理、系统响应分析、场的传播和滤波器设计等领域。</p><h5 id="2、图像层面"><a href="#2、图像层面" class="headerlink" title="2、图像层面"></a>2、图像层面</h5><p>在深度学习中，卷积层是神经网络中一种常用的操作，它使用卷积运算对输入数据进行特征提取。卷积层的数学公式可以通过以下方式解释：</p><p>假设输入数据是一个三维张量，具有形状为 [H, W, C]，其中 H 表示高度，W 表示宽度，C 表示通道数。卷积层使用一组称为卷积核（或过滤器）的权重来对输入进行卷积操作。</p><p>假设卷积核的形状为 [FH, FW, C, K]，其中 FH 和 FW 表示卷积核的高度和宽度，C 表示输入的通道数，K 表示卷积核的数量（也称为输出通道数）。一般而言，在图像领域，卷积核的高度和宽度相等，常见的设置有3x3、5x5、7x7等。</p><p>卷积操作的数学公式可以表示为：</p><p>输出特征图的某个位置的数值 &#x3D; sum(输入特征图的对应位置 * 卷积核的权重) + 偏置</p><p>具体而言，对于输出特征图的每个位置 (i, j, k)，其中 i 表示高度索引，j 表示宽度索引，k 表示通道索引，卷积操作可以表示为：</p><p>输出特征图[i, j, k] &#x3D; sum(sum(sum(输入特征图[m, n, c] * 卷积核[m, n, c, k]))) + 偏置[k]</p><p>其中，m 和 n 表示卷积核的高度和宽度索引，c 表示输入特征图的通道索引。</p><p>这个公式说明了卷积操作的过程。在每个位置 (i, j, k)，输入特征图与卷积核的对应位置进行逐元素相乘，然后将所有乘积相加。这相当于对输入特征图的局部区域与卷积核进行加权叠加，得到输出特征图的对应位置。</p><p>最后，可以通过添加偏置项来调整输出特征图的整体偏移量。偏置项在公式中表示为偏置[k]，其中 k 表示输出通道索引。</p><p>下面是一个大小为3x3的卷积核，步长为1的二维卷积示意图：</p><p><img src="/images/1686225397566-562949ca-4f3c-406e-886d-2f002a45b604.gif" alt="img"></p><p>可以看到3x3的卷积核以滑动窗口的形式在输入通道上滑动，并通过某种计算得到上面的输出值，每一个卷积核在一个位置会得到一个输出值，按照卷积核的移动顺序形成的输出矩阵。这是最直观的卷积形式。</p><p>具体计算的过程如下所示：</p><p><img src="/images/1686225862279-5bd2416b-b451-42dd-b5ea-98c5a8fbc9d8.png" alt="img"></p><p>卷积核在原始输入矩阵所覆盖区域对应元素相乘然后相加。</p><p>对于一个图片而言，一般的图片是三通道图片，即包含RGB三个通道，对一个图片的卷积计算，它是如何完成的呢？</p><p><img src="/images/1686226136566-f82570d7-9a1e-4c36-aad3-f256de218fe0.gif" alt="img"></p><p>具体计算过程解释如下：</p><p>首先将三通道图片切分成三个单通道，这里标记为B、G、R三个通道。B、G、R三个通道分别与对应的卷积核进行卷积计算，然后再通过偏置叠加在一起。这个就得到了三通道图片的卷积。</p><h5 id="3、卷积中的参数"><a href="#3、卷积中的参数" class="headerlink" title="3、卷积中的参数"></a>3、卷积中的参数</h5><p>以一个3x3卷积核为例，该卷积核中有9个参数，那么这9个参数是怎么来的呢？卷积核的个数又是什么呢？</p><p>其实卷积核中有9个参数也是作为超参数通过模型训练得到的，一个训练好的卷积核可以更好的提取图像的特征。</p><p>常见的初始化方法可以为恺明初始化。</p><p>如下是pytorch中搭建模型时常用的初始化方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">            nn.init.kaiming_normal_(m.weight, mode=<span class="string">&quot;fan_out&quot;</span>, nonlinearity=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">            nn.init.normal(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>对单通道的图片卷积过程，卷积核的个数，其实就是卷积之后输出通道的个数。而对于三通道的卷积过程，卷积核的个数等于输出通道数*输入通道数。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;深度学习中什么是卷积，-它是如何工作的&quot;&gt;&lt;a href=&quot;#深度学习中什么是卷积，-它是如何工作的&quot; class=&quot;headerlink&quot; title=&quot;深度学习中什么是卷积， 它是如何工作的&quot;&gt;&lt;/a&gt;深度学习中什么是卷积， 它是如何工作的&lt;/h4&gt;&lt;h5 i</summary>
      
    
    
    
    
    <category term="卷积" scheme="https://guudman.github.io/tags/%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>pytorch全连接层模拟回归</title>
    <link href="https://guudman.github.io/2023/11/02/pytorch%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E6%A8%A1%E6%8B%9F%E5%9B%9E%E5%BD%92/"/>
    <id>https://guudman.github.io/2023/11/02/pytorch%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E6%A8%A1%E6%8B%9F%E5%9B%9E%E5%BD%92/</id>
    <published>2023-11-02T11:58:58.000Z</published>
    <updated>2023-11-04T02:55:03.766Z</updated>
    
    <content type="html"><![CDATA[<h4 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h4><p>搭建两层全连接网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>预测的值画成曲线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : 301_regression.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class="string">&quot;TRUE&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x data (tensor), shape=(100, 1)</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># noisy y data (tensor), shappe(100, 1)</span></span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.2</span> * torch.rand(x.size())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch can only train on Variable, so convert them to Variable</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">1</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 针对回归的均方误差</span></span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line">plt.ion()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">    prediction = net(x)</span><br><span class="line">    loss = loss_func(prediction, y)</span><br><span class="line">    <span class="comment"># clear gradients for next train</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># backpropagation, comupte gradients</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># apply gradients</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        plt.cla()</span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        <span class="comment"># 绘制预测的值</span></span><br><span class="line">        plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">&#x27;r-&#x27;</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">&#x27;Loss=%.4f&#x27;</span> % loss.data.numpy(), fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>: <span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        <span class="comment"># plt.pause(0.1)</span></span><br><span class="line">        <span class="comment"># plt.ioff()</span></span><br><span class="line">        <span class="comment"># 保存为jpg图像</span></span><br><span class="line">        plt.savefig(<span class="string">f&#x27;./img/regression_<span class="subst">&#123;t&#125;</span>.jpg&#x27;</span>)</span><br><span class="line">        <span class="comment"># plt.show()</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>jgp图像转gif动图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : jpg_2_gif.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    root_path = <span class="string">&#x27;../img&#x27;</span></span><br><span class="line">    image_list = os.listdir(root_path)</span><br><span class="line">    gif_name = <span class="string">&#x27;./regression.gif&#x27;</span></span><br><span class="line">    <span class="comment"># duration between images</span></span><br><span class="line">    duration = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#### read images and write in gif</span></span><br><span class="line">    images = []</span><br><span class="line">    <span class="keyword">for</span> image_name <span class="keyword">in</span> image_list:</span><br><span class="line">        image_name = os.path.join(root_path, image_name)</span><br><span class="line">        images.append(imageio.imread(image_name))</span><br><span class="line">    imageio.mimwrite(gif_name, images, <span class="string">&#x27;GIF&#x27;</span>, duration=duration)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;success&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="结果可视化"><a href="#结果可视化" class="headerlink" title="结果可视化"></a>结果可视化</h4><p><img src="https://gitee.com/guudman/blog_images/raw/master/regression.gif" alt="image"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;实现过程&quot;&gt;&lt;a href=&quot;#实现过程&quot; class=&quot;headerlink&quot; title=&quot;实现过程&quot;&gt;&lt;/a&gt;实现过程&lt;/h4&gt;&lt;p&gt;搭建两层全连接网络&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td </summary>
      
    
    
    
    
    <category term="pytorch" scheme="https://guudman.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>GoogleNet解析</title>
    <link href="https://guudman.github.io/2023/11/02/GoogleNet%E8%A7%A3%E6%9E%90/"/>
    <id>https://guudman.github.io/2023/11/02/GoogleNet%E8%A7%A3%E6%9E%90/</id>
    <published>2023-11-02T11:37:32.000Z</published>
    <updated>2023-11-02T11:40:28.429Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>GoogleNet在2014年由Google团队提出，斩获当年ImageNet竞赛中Classification Task分类任务第一名。</p><p>论文原文： <a href="%5Barxiv.org/pdf/1409.4842.pdf%5D(https://arxiv.org/pdf/1409.4842.pdf)">GoogleNet原文</a></p><p><img src="/images/googlenet1.jpg" alt="googlenet1"></p><p>首先介绍一下该网络的亮点：</p><ul><li><p>引入Inception结构(融合不同尺度的特征信息)</p></li><li><p>使用1×1的卷积核进行降维以及映射处理</p></li><li><p>添加两个辅助分类器帮助训练</p></li><li><p>丢弃全连接层，使用平均池化层(大大减少模型参数, 除去两个辅助分类器， 网络大小只有vgg的1&#x2F;20)</p></li></ul><p>接着分析一下Inception结构：</p><p><img src="/images/image-20231102141742833.png" alt="image-20231102141742833"></p><p>​左图是论文中提出的inception原始结构， 右图是inception加上降维功能的结构。</p><p>先看<strong>左图</strong>， inception一共有4个分支， 也就是说输入的特征矩阵并行通过这个4个分支得到四个输出， 然后在将这个四个输出在深度维度(channel维度)进行拼接得到最终的输出(<strong>注意：为了让四个分支的输出能够在深度方向进行拼接， 必须保证四个分支输出的特征矩阵高度和宽度都相同</strong>)</p><p>分支1是卷积核大小为1×1的卷积层， stride&#x3D;1</p><p>分支2是卷积核大小为3×3的卷积层， stride&#x3D;1， padding&#x3D;1(保证输出特征矩阵的高和宽和输入特征矩阵相等)</p><p>分支3是卷积核大小为5×5的卷积层， stride&#x3D;1， padding&#x3D;2(保证输出特征矩阵的高和宽和输入特征矩阵相等)</p><p>分支4是池化核大小为3×3的最大池化下采样层， stride&#x3D;1， padding&#x3D;1(保证输出特征矩阵的高和宽和输入特征矩阵相等)</p><p>在看<strong>右图</strong>， 对比左图， 就是在分支2,3,4上加入了卷积核为1×1的卷积层， 目的是为了降维， 减少模型训练参数， 减少计算量。</p><p>下面看一下1×1卷积核如何减少模型参数的， 同样是对一个深度为512的特征矩阵使用64个大小为5×5的卷积核进行卷积， 不使用1×1卷积核进行降维一共需要819200个参数， 如果使用1×1卷积核进行降维， 一共需要50688个参数，明显减少了很多。</p><p><img src="/images/image-20231102143154611.png" alt="image-20231102143154611"></p><p>每个卷积核的参数如何确定呢， 下面是原论文中给出的参数列表， 对于我们搭建的inception模块， 所需要使用的参数有**#1x1, #3x3reduce, #3x3, #5x5reduce, #5x5, poolproj<strong>这6个参数， 分别对应着所需要的</strong>卷积核的个数**。</p><p>下面将inception模块所用到的参数信息标注在每个分支上， #1x1对应着分支上1x1的卷积核个数， #3x3reduce对应着分支2上1x1的卷积核个数， #3x3对应着分支2上3x3的卷积核个数， #5x5reduce对应着分支3上1x1的卷积核个数， #5x5对应着分支3上5x5的卷积核个数，poolproj对应着分支4上1x1的卷积核个数。</p><p><img src="/images/image-20231102144232732.png" alt="image-20231102144232732"></p><p>接下来看辅助分类器结构，网络中的两个辅助分类器结构一模一样的， 如下图所示：</p><p><img src="/images/image-20231102145636207.png" alt="image-20231102145636207"></p><p>这两个辅助分类器的输入分别来自Inception(4a)和inception(4d)。</p><p>辅助分类器的第一层是一个平均池化下采样层， 池化核大小为5x5， stride&#x3D;3， </p><p>第二层是卷积层， 卷积核大小为1x1, stride&#x3D;1, 卷积核个数是128</p><p>第三层是全连接层， 节点个数为1024</p><p>第四层是全连接层， 节点个数为1000(对应分类任务中分类类别数)</p><p>下面给出了GoogleNet网络结构图</p><p><img src="/images/Screen_Shot_googlenet.png" alt="Screen_Shot_googlenet"></p><h4 id="2、代码实现"><a href="#2、代码实现" class="headerlink" title="2、代码实现"></a>2、代码实现</h4><h5 id="1、pytorch实现"><a href="#1、pytorch实现" class="headerlink" title="1、pytorch实现"></a>1、pytorch实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_googlenet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># pytorch官方参考代码：https://github.com/pytorch/vision/blob/main/torchvision/models/googlenet.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GoogleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, aux_logits=<span class="literal">True</span>, init_weights=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GoogleNet, self).__init__()</span><br><span class="line">        self.aux_logits = aux_logits</span><br><span class="line">        <span class="comment"># (224 - 7 + 2 * 3)/2 + 1 = 112 (3, 224, 224) -&gt; (64, 112, 112)</span></span><br><span class="line">        self.conv1 = BaseConv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">7</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># (64, 112, 112) -&gt; (64, 56, 56) 看一下这里的参数是如何计算的，</span></span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># (56 - 3 + 2 * 0)/1 + 1 = 56 (64, 56, 56) -&gt; (64, 56, 56)</span></span><br><span class="line">        self.conv2 = BaseConv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (56 - 3 + 2 * 0)/1 + 1 = 56 (64, 56, 56) -&gt; (192, 56, 56)</span></span><br><span class="line">        self.conv3 = BaseConv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">192</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (192, 56, 56) -&gt; (192. 28. 28)</span></span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Inception3a 具体参数可查看表</span></span><br><span class="line">        self.inception3a = Inception(<span class="number">192</span>, <span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">        self.inception3b = Inception(<span class="number">256</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">192</span>, <span class="number">32</span>, <span class="number">96</span>, <span class="number">64</span>)</span><br><span class="line">        self.maxpool3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.inception4a = Inception(<span class="number">480</span>, <span class="number">192</span>, <span class="number">96</span>, <span class="number">208</span>, <span class="number">16</span>, <span class="number">48</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4b = Inception(<span class="number">512</span>, <span class="number">160</span>, <span class="number">112</span>, <span class="number">224</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4c = Inception(<span class="number">512</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4d = Inception(<span class="number">512</span>, <span class="number">112</span>, <span class="number">144</span>, <span class="number">288</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4e = Inception(<span class="number">528</span>, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">        self.maxpool4 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.inception5a = Inception(<span class="number">832</span>, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">        self.inception5b = Inception(<span class="number">832</span>, <span class="number">384</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">48</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.aux_logits:</span><br><span class="line">            <span class="comment"># Inception4b  512</span></span><br><span class="line">            self.aux1 = InceptionAux(<span class="number">512</span>, num_classes)</span><br><span class="line">            <span class="comment"># Inception4e 528</span></span><br><span class="line">            self.aux2 = InceptionAux(<span class="number">528</span>, num_classes)</span><br><span class="line">        <span class="comment"># 指定输出固定尺寸</span></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.4</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">1024</span>, num_classes)</span><br><span class="line">        <span class="keyword">if</span> init_weights:</span><br><span class="line">            self._initialize_weights()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># [N, 3, 224, 224] -&gt; [N, 64, 112, 112]</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="comment"># [N, 64, 112, 112] -&gt; [N, 64, 56, 56]</span></span><br><span class="line">        x = self.maxpool1(x)</span><br><span class="line">        <span class="comment"># [N, 64, 56, 56] -&gt; [N, 56, 56, 64]</span></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="comment"># [N, 64, 56, 56] -&gt; [N, 56, 56, 192]</span></span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        <span class="comment"># [N, 56, 56, 192] -&gt; [N, 28, 28, 192]</span></span><br><span class="line">        x = self.maxpool2(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [N, 28, 28, 192] -&gt; [N, 28, 28, 256]</span></span><br><span class="line">        x = self.inception3a(x)</span><br><span class="line">        <span class="comment"># [N, 28, 28, 256] -&gt; [N, 28, 28, 480]</span></span><br><span class="line">        x = self.inception3b(x)</span><br><span class="line">        <span class="comment"># [N, 28, 28, 480] - &gt; [N, 14, 14, 480]</span></span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#  [N, 14, 14, 480] -&gt; [N, 14, 14, 512]</span></span><br><span class="line">        x = self.inception4a(x)</span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> self.aux_logits:</span><br><span class="line">            aux1 = self.aux1(x)</span><br><span class="line">        <span class="comment">#   [N, 14, 14, 512] -&gt; [N, 14, 14, 512]</span></span><br><span class="line">        x = self.inception4b(x)</span><br><span class="line">        <span class="comment">#   [N, 14, 14, 512] -&gt; [N, 14, 14, 512]</span></span><br><span class="line">        x = self.inception4c(x)</span><br><span class="line">        <span class="comment">#   [N, 14, 14, 512] -&gt; [N, 14, 14, 528]</span></span><br><span class="line">        x = self.inception4d(x)</span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> self.aux_logits:</span><br><span class="line">            aux2 = self.aux2(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#   [N, 14, 14, 528] -&gt; [N, 14, 14, 832]</span></span><br><span class="line">        x = self.inception4e(x)</span><br><span class="line">        <span class="comment"># [N, 14, 14, 832] - &gt; [N, 7, 7, 832]</span></span><br><span class="line">        x = self.maxpool4(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#  [N, 7, 7, 832] -&gt; [N, 7, 7, 832]</span></span><br><span class="line">        x = self.inception5a(x)</span><br><span class="line">        <span class="comment">#  [N, 7, 7, 832] -&gt; [N, 7, 7, 1024]</span></span><br><span class="line">        x = self.inception5b(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#  [N, 7, 7, 1024] -&gt; [N, 1, 1, 1024]</span></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        <span class="comment">#  [N, 1, 1, 1024] -&gt; [N, 1024]</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="comment"># [N, 1024] -&gt; [N, num_classes]</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> self.aux_logits:</span><br><span class="line">            <span class="keyword">return</span> x, aux2, aux1</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                    nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__()</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        ch1x1: </span></span><br><span class="line"><span class="string">        ch3x3red: ch3x3reduce</span></span><br><span class="line"><span class="string">        ch3x3:</span></span><br><span class="line"><span class="string">        ch5x5red: ch5x5reduce</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.branch1 = BaseConv2d(in_channels, ch1x1, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.branch2 = nn.Sequential(</span><br><span class="line">            BaseConv2d(in_channels=in_channels, out_channels=ch3x3red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            <span class="comment"># 保证输出大小等于输入大小</span></span><br><span class="line">            BaseConv2d(in_channels=ch3x3red, out_channels=ch3x3, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        self.branch3 = nn.Sequential(</span><br><span class="line">            BaseConv2d(in_channels=in_channels, out_channels=ch5x5red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            <span class="comment"># 保证输出大小等于输入大小</span></span><br><span class="line">            BaseConv2d(in_channels=ch5x5red, out_channels=ch5x5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.branch4 = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            BaseConv2d(in_channels=in_channels, out_channels=pool_proj, kernel_size=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1 = self.branch1(x)</span><br><span class="line">        branch2 = self.branch2(x)</span><br><span class="line">        branch3 = self.branch3(x)</span><br><span class="line">        branch4 = self.branch4(x)</span><br><span class="line"></span><br><span class="line">        outputs = [branch1, branch2, branch3, branch4]</span><br><span class="line">        <span class="comment"># [batch, channel, h, w] torch.cat(outputs, 1)表示在channel维度上拼接</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionAux</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionAux, self).__init__()</span><br><span class="line">        self.averagepool = nn.AvgPool2d(kernel_size=<span class="number">5</span>, stride=<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># output [batch, 128, 4, 4]</span></span><br><span class="line">        self.conv = BaseConv2d(in_channels, <span class="number">128</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">1024</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># aux1: [N, 512, 14, 14] aux2: [N, 528, 14, 14]</span></span><br><span class="line">        x = self.averagepool(x)</span><br><span class="line">        <span class="comment"># aux1: [N, 512, 4, 4], aux2: [N, 528, 4, 4]</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="comment"># [N, 128, 4, 4]</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.5</span>, training=self.training)</span><br><span class="line">        <span class="comment"># [N, 2048]</span></span><br><span class="line">        x = F.relu(self.fc1(x), inplace=<span class="literal">True</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.5</span>, training=self.training)</span><br><span class="line">        <span class="comment"># [N, 2014]</span></span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="comment"># [N, num_classes]</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaseConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BaseConv2d, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># input = torch.rand((16, 3, 224, 224))</span></span><br><span class="line"><span class="comment"># googlenet = GoogleNet(num_classes=5, aux_logits=False, init_weights=True)</span></span><br><span class="line"><span class="comment"># print(googlenet)</span></span><br><span class="line"><span class="comment"># output = googlenet(input)</span></span><br><span class="line"><span class="comment"># print(output)</span></span><br></pre></td></tr></table></figure><h5 id="2、TensorFlow实现"><a href="#2、TensorFlow实现" class="headerlink" title="2、TensorFlow实现"></a>2、TensorFlow实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_googlenet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：0399</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, Model, Sequential</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">GoogleNet</span>(<span class="params">im_height=<span class="number">224</span>, im_width=<span class="number">224</span>, class_num=<span class="number">1000</span>, aux_logits=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># tensorflow通道顺序 NHWC</span></span><br><span class="line">    <span class="comment"># [None, 224, 224, 3]</span></span><br><span class="line">    input_image = layers.Input(shape=(im_height, im_width, <span class="number">3</span>), dtype=<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">    <span class="comment"># [None, 224, 224, 3] -&gt; [None, 112, 112, 64]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;SAME&quot;</span>,</span><br><span class="line">                      activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&quot;conv2d_1&quot;</span>)(input_image)</span><br><span class="line">    <span class="comment"># [None, 112, 112, 64] -&gt; [None, 56, 56, 64]</span></span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;SAME&quot;</span>, name=<span class="string">&quot;maxpool_1&quot;</span>)(x)</span><br><span class="line">    <span class="comment">#  [None, 56, 56, 64] -&gt;  [None, 56, 56, 64]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">1</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&quot;conv2d_2&quot;</span>)(x)</span><br><span class="line">    <span class="comment">#  [None, 56, 56, 64] -&gt;  [None, 56, 56, 192]  (56 - 3 + 2 * 1)/1 + 1 = 56</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&quot;conv2d_3&quot;</span>)(x)</span><br><span class="line">    <span class="comment">#  [None, 56, 56, 192] -&gt;  [None, 28, 28, 192]</span></span><br><span class="line"></span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)</span><br><span class="line">    <span class="comment">#  [None, 28, 28, 192] -&gt;  [None, 28, 28, 256]</span></span><br><span class="line">    x = Inception(<span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">32</span>, name=<span class="string">&quot;inception3a&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 28, 28, 256] -&gt; [None, 28, 28, 480]</span></span><br><span class="line">    x = Inception(<span class="number">128</span>, <span class="number">128</span>, <span class="number">192</span>, <span class="number">32</span>, <span class="number">96</span>, <span class="number">64</span>, name=<span class="string">&quot;inception3b&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 28, 28, 480] -&gt; [None, 14, 14, 480]</span></span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;SAME&quot;</span>, name=<span class="string">&quot;maxpool_2&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [None, 14, 14, 480] -&gt; [None, 14, 14, 512]</span></span><br><span class="line">    x = Inception(<span class="number">192</span>, <span class="number">96</span>, <span class="number">208</span>, <span class="number">16</span>, <span class="number">48</span>, <span class="number">64</span>, name=<span class="string">&quot;inception4a&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> aux_logits:</span><br><span class="line">        aux1 = InceptionAux(class_num, name=<span class="string">&quot;aux1&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 14, 14, 512] -&gt; [None, 14, 14, 512]</span></span><br><span class="line">    x = Inception(<span class="number">160</span>, <span class="number">112</span>, <span class="number">224</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>, name=<span class="string">&quot;inception4b&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 14, 14, 512] -&gt; [None, 14, 14, 512]</span></span><br><span class="line">    x = Inception(<span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>, name=<span class="string">&quot;inception4c&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 14, 14, 512] -&gt; [None, 14, 14, 528]</span></span><br><span class="line">    x = Inception(<span class="number">112</span>, <span class="number">144</span>, <span class="number">288</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>, name=<span class="string">&quot;inception4d&quot;</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> aux_logits:</span><br><span class="line">        aux2 = InceptionAux(class_num, name=<span class="string">&quot;aux2&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [None, 14, 14, 528] -&gt; [None, 14, 14, 832]</span></span><br><span class="line">    x = Inception(<span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>, name=<span class="string">&quot;inception4e&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 14, 14, 832] -&gt; [None, 7, 7, 832]</span></span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;SAME&quot;</span>, name=<span class="string">&quot;maxpool_3&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 7, 7, 832] -&gt; [None, 7, 7, 832]</span></span><br><span class="line">    x = Inception(<span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>, name=<span class="string">&quot;inception5a&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 7, 7, 832] -&gt; [None, 7, 7, 1024]</span></span><br><span class="line">    x = Inception(<span class="number">384</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">48</span>, <span class="number">128</span>, <span class="number">128</span>, name=<span class="string">&quot;inception5b&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 7, 7, 1024] -&gt; [None, 1, 1, 1024]</span></span><br><span class="line">    x = layers.AvgPool2D(pool_size=<span class="number">7</span>, strides=<span class="number">1</span>, name=<span class="string">&quot;avgpool_1&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 1, 1, 1024] -&gt; [None, 1024*1*1]</span></span><br><span class="line">    x = layers.Flatten(name=<span class="string">&quot;output_flatten&quot;</span>)(x)</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    x = layers.Dropout(rate=<span class="number">0.4</span>, name=<span class="string">&quot;output_dropout&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, class_num]</span></span><br><span class="line">    x = layers.Dense(class_num, name=<span class="string">&quot;output_dense&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">    aux3 = layers.Softmax(name=<span class="string">&quot;aux_3&quot;</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> aux_logits:</span><br><span class="line">        model = models.Model(inputs=input_image, outputs=[aux1, aux2, aux3])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model = models.Model(inputs=input_image, outputs=aux3)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__()</span><br><span class="line">        self.branch1 = layers.Conv2D(filters=ch1x1, kernel_size=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.branch2 = Sequential([</span><br><span class="line">            layers.Conv2D(filters=ch3x3red, kernel_size=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            layers.Conv2D(filters=ch3x3, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)])</span><br><span class="line"></span><br><span class="line">        self.branch3 = Sequential([</span><br><span class="line">            layers.Conv2D(filters=ch5x5red, kernel_size=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            layers.Conv2D(filters=ch5x5, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)])</span><br><span class="line"></span><br><span class="line">        self.branch4 = Sequential([</span><br><span class="line">            <span class="comment"># caution: default stride=pool_size</span></span><br><span class="line">            layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">&quot;SAME&quot;</span>),</span><br><span class="line">            layers.Conv2D(filters=pool_proj, kernel_size=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, <span class="built_in">input</span>, **kwargs</span>):</span><br><span class="line">        branch1 = self.branch1(<span class="built_in">input</span>)</span><br><span class="line">        branch2 = self.branch2(<span class="built_in">input</span>)</span><br><span class="line">        branch3 = self.branch3(<span class="built_in">input</span>)</span><br><span class="line">        branch4 = self.branch4(<span class="built_in">input</span>)</span><br><span class="line">        outputs = layers.concatenate([branch1, branch2, branch3, branch4])</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionAux</span>(layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionAux, self).__init__()</span><br><span class="line">        self.avgpool = layers.AvgPool2D(pool_size=<span class="number">5</span>, strides=<span class="number">3</span>)</span><br><span class="line">        self.conv = layers.Conv2D(<span class="number">128</span>, kernel_size=<span class="number">1</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.fc1 = layers.Dense(units=<span class="number">1024</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(units=num_classes)</span><br><span class="line">        self.softmax = layers.Softmax()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, **kwargs</span>):</span><br><span class="line">        <span class="comment"># aux1 [None, 14, 14, 512] aux2 [None, 14, 14, 528]</span></span><br><span class="line">        <span class="comment"># aux1: [None, 14, 14, 512] -&gt; [None, 4, 4, 512]  (14 - 5)/3 + 1 = 4</span></span><br><span class="line">        <span class="comment"># axu2: [None, 14, 14, 528] -&gt; [None, 4, 4, 528]  (14 - 5)/3 + 1 = 4</span></span><br><span class="line">        x = self.avgpool(inputs)</span><br><span class="line">        <span class="comment"># aux1 [None, 4, 4, 512]-&gt; [4, 4, 512]  aux2 [None, 4, 4, 528]-&gt; [4, 4, 528]</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        x = layers.Flatten()(x)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = layers.Dropout(rate=<span class="number">0.5</span>)(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = layers.Dropout(rate=<span class="number">0.5</span>)(x)</span><br><span class="line">        x = self.softmax(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># input = tf.random.uniform((16, 224, 224, 3))</span></span><br><span class="line"><span class="comment"># googlenet = GoogleNet(class_num=5, aux_logits=False)</span></span><br><span class="line"><span class="comment"># output = googlenet(input)</span></span><br><span class="line"><span class="comment"># print(output)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;GoogleNet在2014年由Google团队提出，斩获当年ImageNet竞赛中Classification Task分</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>VggNet解析</title>
    <link href="https://guudman.github.io/2023/11/02/VggNet%E8%A7%A3%E6%9E%90/"/>
    <id>https://guudman.github.io/2023/11/02/VggNet%E8%A7%A3%E6%9E%90/</id>
    <published>2023-11-02T11:17:44.000Z</published>
    <updated>2023-11-04T03:03:54.549Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>论文原文： [VggNet原文](<a href="https://arxiv.org/pdf/1409.1556.pdf">1409.1556.pdf (arxiv.org)</a>)</p><p>VGG由牛津大学视觉几何小组(Visual Geometry Group, VGG)提出的一种深层卷积网， 该网络在2014年获得定位任务的第一名， 分类任务的第二名。VGG可以看成是加深版的AlexNet， 都是conv + FC layer组成。</p><p>下图是VGG16模型的结构简图</p><center><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102101734336.png"><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102101734336.png" alt="image"></p><p><strong>网络的亮点</strong></p><ul><li>通过堆叠多个3×3的卷积代替大尺度卷积核 (在保证相同感受野的前提下能够减少所需的参数量)</li></ul><p>论文中提到，通过堆叠两个3×3的卷积核代替5×5的卷积核， 堆叠三个3×3的卷积核代替7×7的卷积核。It is easy to see that a stack of <strong>two</strong> <strong>3</strong> <strong>× 3</strong> conv layers (without spatial pooling in between) has an effective receptive field of <strong>5 × 5</strong>; <strong>three</strong> such layers have a <strong>7 × 7</strong> effective receptive field.</p><p>下面给出一个实例</p><p>使用7×7卷积核所需参数， 假设输入输出通道数均为C。</p><p>7×7×C×C &#x3D; 49C²</p><p>堆叠3个3×3卷积核所需参数， 假设输入输出通道数均为C。</p><p>3×3×C×C + 3×3×C×C + 3×3×C×C &#x3D; 27C²</p><p>经过对比发现使用3层3×3的卷积层比使用7×7的卷积核参数更少。</p><p>下图是从原论文中截取的几种VGG模型的配置表， 表中作者呈现了几种不同深度的配置(11层, 13层， 16层， 19层)是否使用<strong>LRN</strong>以及1×1卷积层与3×3卷积层的差异。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102102435922.png" alt="image"></p><h4 id="2、代码实现"><a href="#2、代码实现" class="headerlink" title="2、代码实现"></a>2、代码实现</h4><h5 id="1、pytorch实现"><a href="#1、pytorch实现" class="headerlink" title="1、pytorch实现"></a>1、pytorch实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_vgg.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先搭建vgg19</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VggNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(VggNet, self).__init__()</span><br><span class="line">        <span class="comment"># (224 - 3 + 2*1)/1 + 1 = 224 -&gt; (224, 224, 64)</span></span><br><span class="line">        self.conv1_64_1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (223 - 3 + 2*1)1 + 1 = 224 -&gt; (224, 224, 64)</span></span><br><span class="line">        self.conv1_64_2 = nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (224, 224, 64) -&gt; (112, 112, 64)</span></span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># (112 - 3 + 2*1)/1 + 1 = 112  (112, 112, 64) -&gt; (112, 112, 128)</span></span><br><span class="line">        self.conv2_128_1 = nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2_128_2 = nn.Conv2d(in_channels=<span class="number">128</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (112, 128, 128) -&gt; (56, 56, 128)</span></span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># (56 - 3 + 2 * 1)/ 1 + 1 = 56 (56, 56, 128) -&gt; (56, 56, 256)</span></span><br><span class="line">        self.conv3_256_1 = nn.Conv2d(in_channels=<span class="number">128</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3_256_2 = nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3_256_3 = nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># maxpool  (56, 56, 256) -&gt; (28, 28, 256)</span></span><br><span class="line">        self.maxpool3 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv4_512_1 = nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv4_512_2 = nn.Conv2d(in_channels=<span class="number">512</span>, out_channels=<span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv4_512_3 = nn.Conv2d(in_channels=<span class="number">512</span>, out_channels=<span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># maxpool (28, 28, 512) -&gt; (14, 14, 512)</span></span><br><span class="line">        self.maxpool4 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># (14, 14, 512)</span></span><br><span class="line">        self.conv5_512_1 = nn.Conv2d(in_channels=<span class="number">512</span>, out_channels=<span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv5_512_2 = nn.Conv2d(in_channels=<span class="number">512</span>, out_channels=<span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv5_512_3 = nn.Conv2d(in_channels=<span class="number">512</span>, out_channels=<span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># maxpool (14, 14, 512) -&gt; (7, 7, 512)</span></span><br><span class="line">        self.maxpool5 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">7</span> * <span class="number">7</span> * <span class="number">512</span>, <span class="number">4096</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>)</span><br><span class="line">        self.fc3 = nn.Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1_64_1(x)</span><br><span class="line">        x = self.conv1_64_2(x)</span><br><span class="line">        x = self.maxpool1(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2_128_1(x)</span><br><span class="line">        x = self.conv2_128_2(x)</span><br><span class="line">        x = self.maxpool2(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv3_256_1(x)</span><br><span class="line">        x = self.conv3_256_2(x)</span><br><span class="line">        x = self.conv3_256_3(x)</span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv4_512_1(x)</span><br><span class="line">        x = self.conv4_512_2(x)</span><br><span class="line">        x = self.conv4_512_3(x)</span><br><span class="line">        x = self.maxpool4(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv5_512_1(x)</span><br><span class="line">        x = self.conv5_512_2(x)</span><br><span class="line">        x = self.conv5_512_3(x)</span><br><span class="line">        x = self.maxpool5(x)  <span class="comment"># [batch, c, h, w] -&gt; [-1, c*h*w]</span></span><br><span class="line"></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">512</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># input = torch.rand([8, 3, 224, 224])</span></span><br><span class="line"><span class="comment"># vggnet = VggNet()</span></span><br><span class="line"><span class="comment"># print(vggnet)</span></span><br><span class="line"><span class="comment"># output = vggnet(input)</span></span><br><span class="line"><span class="comment"># print(output)</span></span><br></pre></td></tr></table></figure><h5 id="2、TensorFlow实现"><a href="#2、TensorFlow实现" class="headerlink" title="2、TensorFlow实现"></a>2、TensorFlow实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_vggnet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：0399</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, Model, Sequential</span><br><span class="line"></span><br><span class="line">CONV_KERNEL_INITIALIZER = &#123;</span><br><span class="line">    <span class="string">&#x27;class_name&#x27;</span>: <span class="string">&#x27;VarianceScaling&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;config&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;scale&#x27;</span>: <span class="number">2.0</span>,</span><br><span class="line">        <span class="string">&#x27;mode&#x27;</span>: <span class="string">&#x27;fan_out&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;distribution&#x27;</span>: <span class="string">&#x27;truncated_normal&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DENSE_KERNEL_INITIALIZER = &#123;</span><br><span class="line">    <span class="string">&#x27;class_name&#x27;</span>: <span class="string">&#x27;VarianceScaling&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;config&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;scale&#x27;</span>: <span class="number">1.</span> / <span class="number">3.</span>,</span><br><span class="line">        <span class="string">&#x27;mode&#x27;</span>: <span class="string">&#x27;fan_out&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;distribution&#x27;</span>: <span class="string">&#x27;uniform&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">VGG</span>(<span class="params">feature, im_height=<span class="number">224</span>, im_width=<span class="number">224</span>, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># tensorflow中的tensor通道顺序是NHWC</span></span><br><span class="line">    input_image = layers.Input(shape=(im_height, im_width, <span class="number">3</span>), dtype=<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">    x = feature(input_image)</span><br><span class="line">    x = layers.Flatten()(x)</span><br><span class="line">    x = layers.Dropout(rate=<span class="number">0.5</span>)(x)</span><br><span class="line">    x = layers.Dense(<span class="number">2048</span>, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                     kernel_initializer=DENSE_KERNEL_INITIALIZER)(x)</span><br><span class="line">    x = layers.Dropout(rate=<span class="number">0.5</span>)(x)</span><br><span class="line">    x = layers.Dense(<span class="number">2048</span>, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                     kernel_initializer=DENSE_KERNEL_INITIALIZER)(x)</span><br><span class="line">    x = layers.Dropout(rate=<span class="number">0.5</span>)(x)</span><br><span class="line">    x = layers.Dense(num_classes, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                     kernel_initializer=DENSE_KERNEL_INITIALIZER)(x)</span><br><span class="line">    output = layers.Softmax()(x)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=input_image, outputs=output)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_feature</span>(<span class="params">cfg</span>):</span><br><span class="line">    feature_layers = []</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> cfg:</span><br><span class="line">        <span class="keyword">if</span> v == <span class="string">&#x27;M&#x27;</span>:</span><br><span class="line">            feature_layers.append(layers.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            feature_layers.append(layers.Conv2D(v, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>,</span><br><span class="line">                                                kernel_initializer=CONV_KERNEL_INITIALIZER))</span><br><span class="line">    <span class="keyword">return</span> Sequential(feature_layers, name=<span class="string">&quot;feature&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cfgs = &#123;</span><br><span class="line">    <span class="string">&#x27;vgg11&#x27;</span>: [<span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg13&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg16&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg19&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">model_name=<span class="string">&quot;vgg16&quot;</span>, im_height=<span class="number">224</span>, im_width=<span class="number">224</span>, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    cfg = cfgs[model_name]</span><br><span class="line">    model = VGG(make_feature(cfg), im_height=im_height, im_width=im_width, num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># input = tf.random.uniform((4, 224, 224, 3))</span></span><br><span class="line"><span class="comment"># vggnet = vgg(&quot;vgg16&quot;, num_classes=5)</span></span><br><span class="line"><span class="comment"># print(vggnet.summary())</span></span><br><span class="line"><span class="comment"># print(vggnet(input))</span></span><br></pre></td></tr></table></figure></center>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;论文原文： [VggNet原文](&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>AlexNet解析</title>
    <link href="https://guudman.github.io/2023/11/02/AlexNet%E8%A7%A3%E6%9E%90/"/>
    <id>https://guudman.github.io/2023/11/02/AlexNet%E8%A7%A3%E6%9E%90/</id>
    <published>2023-11-02T11:11:45.000Z</published>
    <updated>2023-11-02T11:14:15.476Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1、AlexNet简介"><a href="#1、AlexNet简介" class="headerlink" title="1、AlexNet简介"></a>1、AlexNet简介</h4><p>Alexnet是2012年ILSVRC 2012(ImageNet Large Scale Visual Recognition Challenge)竞赛的冠军网络， 分类准确率由传统的70%提升到80%(当时传统方法已进入瓶颈期，所以这么大的提升是非常厉害的)。它是由Hinton和他的学生Alex设计的。也是在那年之后，深度学习模型开始迅速发展。下面的图就是Alexnet原论文中截取的网络结构图。</p><p>Alexnet论文原文， <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a></p><p><img src="/images/image-20231102091945023.png" alt="image-20231102091945023"></p><p>图中有上下两部分是因为作者使用两块GPU进行并行训练， 所以上下两部分的结果是一模一样的。我们直接看下面部分就行了。</p><p>接着说说该网络的亮点：</p><p>(1)首次使用了GPU进行网络加速训练</p><p>(2)使用了ReLU激活函数， 而不是传统的Sigmoid激活函数以及Tanh激活函数</p><p>(3)使用了LRN局部相应归一化</p><p>(4)在全连接层的前两层使用了Dropout方法按照一定比例随机失活神经元，以减少过拟合</p><p>接着给出卷积或池化后的矩阵尺寸大小计算公式</p><p>N &#x3D; (W - F  + 2p)&#x2F;s + 1</p><p>其中w是输入图片大小， F是 卷积核或池化核大小， p是padding的像素个数， s是步距。</p><p>接下来对每一层进行详细分析</p><h4 id="2、模型结构参数剖析"><a href="#2、模型结构参数剖析" class="headerlink" title="2、模型结构参数剖析"></a>2、模型结构参数剖析</h4><p><strong>卷积层1</strong></p><p>由于使用了两块GPU， 所以卷积核的个数需要乘以2：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Conv1:</span><br><span class="line">    input<span class="built_in">_</span>size: [224, 224, 3] -&gt; output：(224 – 11 + (1 + 2))/4 + 1=55 -&gt;(55, 55, 96)</span><br><span class="line">    kernels: 48 * 2</span><br><span class="line">    kernel<span class="built_in">_</span>size: 11</span><br><span class="line">    stride: 4</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>**Conv1: kernels&#x3D;48 × 2 &#x3D; 96， kernel_size&#x3D;11, padding&#x3D;[1, 2], stride&#x3D;4 **</p><p>因此卷积核的个数为96， kernel_size代表卷积核的尺寸， padding代表特征矩阵上下左右补零的参数，stride代表步距。</p><p>输入图片的shape&#x3D;[224, 224, 3]， 输出矩阵的计算公式为： (224  - 11 + (1 + 2)) &#x2F; 4 + 1 &#x3D; 55</p><p>所以输出矩阵的shape为[55, 55, 96]</p><p><strong>Conv1</strong>的实现过程【两个GPU计算过程一模一样，所以kernels就按照一块GPU来搭建】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytroch</span></span><br><span class="line">self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, kernel_size=<span class="number">11</span>, out_channels=<span class="number">48</span>, padding=<span class="number">2</span>, stride=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># tensorflow</span></span><br><span class="line">x = layers.Conv2D(filters=<span class="number">48</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br></pre></td></tr></table></figure><p><strong>最大池化下采样层1</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">maxpooling1: </span><br><span class="line">input<span class="built_in">_</span>size:(55, 55, 96) kernel<span class="built_in">_</span>size:3</span><br><span class="line">padding:0</span><br><span class="line">stride:2</span><br><span class="line">outpu<span class="built_in">_</span>size(27, 27, 96)</span><br></pre></td></tr></table></figure><p><strong>Maxpool1: kernel_size&#x3D;3, padding&#x3D;0, stride&#x3D;2</strong></p><p>kernel_size表示池化核大小， padding表示矩阵上下左右补零的参数， stride代表步距。</p><p>输入特征矩阵的shape&#x3D;[55, 55, 96], 输出特征矩阵的shape&#x3D;[27, 27 , 96]</p><p>shape计算： (W -F + 2P)&#x2F;S+1 &#x3D; (55 - 3 + 2*0)&#x2F;2 + 1&#x3D;27</p><p><strong>Maxpool1</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch:</span></span><br><span class="line">self.maxpooling1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># tensorflow</span></span><br><span class="line">x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)  <span class="comment"># [None, 27, 27, 48]</span></span><br></pre></td></tr></table></figure><p><strong>卷积层2</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Conv2:</span><br><span class="line">input<span class="built_in">_</span>size:[27, 27, 96]</span><br><span class="line">kernel<span class="built_in">_</span>size:5</span><br><span class="line">kernels: 128 * 2</span><br><span class="line">padding:2</span><br><span class="line">stride:1</span><br><span class="line"></span><br><span class="line">output<span class="built_in">_</span>size:[27, 27, 256]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>**Conv2: kernel&#x3D;128×2, kernel_size&#x3D;5, padding&#x3D;2, stride&#x3D;1 **</p><p>输入特征矩阵的深度为[27, 27, 96], 输出特征矩阵尺寸计算公式为：(27 – 5 + 2 * 2)&#x2F;1+ 1&#x3D;27</p><p>所以输出特征矩阵的尺寸为[27, 27, 256]</p><p><strong>Conv2</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch</span></span><br><span class="line">self.conv2 = nn.Conv2d(in_channels=<span class="number">48</span>, kernel_size=<span class="number">5</span>, out_channels=<span class="number">128</span>, padding=<span class="number">2</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow</span></span><br><span class="line"><span class="comment"># 当stride=1且padding=same时， 表示输出尺寸与输入尺寸相同 -&gt;[None, 27, 27, 128]</span></span><br><span class="line">x = layers.Conv2D(filters=<span class="number">128</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br></pre></td></tr></table></figure><p><strong>最大池化下采样层2</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">maxpooling2:</span><br><span class="line">input<span class="built_in">_</span>size:[27, 27, 256] </span><br><span class="line">kernel<span class="built_in">_</span>size:3</span><br><span class="line">padding:0</span><br><span class="line">stride:2  </span><br><span class="line">output<span class="built_in">_</span>size:[13, 13, 256]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Maxpool2: kernel_size&#x3D;3, padding&#x3D;0, stride&#x3D;2</strong></p><p>kernel_size表示池化核大小， padding表示矩阵上下左右补零的参数， stride代表步距。</p><p>输入特征矩阵的shape&#x3D;[27, 27, 256], 输出特征矩阵的shape&#x3D;[13, 13 , 256]</p><p>shape计算： (W -F + 2P)&#x2F;S+1 &#x3D; (27- 3 + 2*0)&#x2F;2 + 1&#x3D;13</p><p><strong>Maxpool2</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch:</span></span><br><span class="line">self.maxpooling2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># tensorflow</span></span><br><span class="line"><span class="comment"># [None, 27, 27, 128] -&gt; [None, 13, 13, 128]</span></span><br><span class="line">x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)</span><br></pre></td></tr></table></figure><p><strong>卷积层3</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conv3:</span><br><span class="line">input<span class="built_in">_</span>size:[13, 13, 256]</span><br><span class="line">kernels: 192*2 = 384</span><br><span class="line">kernel<span class="built_in">_</span>size:3</span><br><span class="line">padding:1</span><br><span class="line">stride:1</span><br><span class="line">output<span class="built_in">_</span>size:[13, 13, 384]</span><br></pre></td></tr></table></figure><p>**Conv3: kernel&#x3D;192×2, kernel_size&#x3D;3, padding&#x3D;1, stride&#x3D;1 **</p><p>输入特征矩阵的深度为[27, 27, 96], 输出特征矩阵尺寸计算公式为：(13– 3 + 2 * 1)&#x2F;1+ 1&#x3D;13</p><p>所以输出特征矩阵的尺寸为[13, 13, 384]</p><p><strong>Conv3</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch</span></span><br><span class="line">self.conv3 = nn.Conv2d(in_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, out_channels=<span class="number">192</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow</span></span><br><span class="line"><span class="comment"># stride=1, padding=same, 输出不变 -&gt;[None, 13, 13, 192]</span></span><br><span class="line">x = layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>卷积层4</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conv4:</span><br><span class="line">input<span class="built_in">_</span>size:[13, 13, 384]</span><br><span class="line">kernels: 192*2 = 384</span><br><span class="line">kernel<span class="built_in">_</span>size:3</span><br><span class="line">padding:1</span><br><span class="line">stride:1</span><br><span class="line"></span><br><span class="line">output<span class="built_in">_</span>size:[13, 13, 384]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>**Conv4: kernel&#x3D;192×2, kernel_size&#x3D;3, padding&#x3D;1, stride&#x3D;1 **</p><p>输入特征矩阵的深度为[13, 13, 384], 输出特征矩阵尺寸计算公式为：(13– 3 + 2 * 1)&#x2F;1+ 1&#x3D;13</p><p>所以输出特征矩阵的尺寸为[13, 13, 384]</p><p><strong>Conv4</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch</span></span><br><span class="line">self.conv4 = nn.Conv2d(in_channels=<span class="number">192</span>, kernel_size=<span class="number">3</span>, out_channels=<span class="number">192</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow</span></span><br><span class="line"><span class="comment"># -&gt;[None, 13, 13, 192]</span></span><br><span class="line">x = layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>卷积层5</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conv5:</span><br><span class="line"> input<span class="built_in">_</span>size:[13, 13, 384]</span><br><span class="line">kernels: 128*2 = 256</span><br><span class="line">kernel<span class="built_in">_</span>size:3</span><br><span class="line">padding:1</span><br><span class="line">stride:1</span><br><span class="line"></span><br><span class="line">output<span class="built_in">_</span>size:[13, 13, 256]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输入特征矩阵的深度为[13, 13, 384], 输出特征矩阵尺寸计算公式为：(13– 3 + 2 * 1)&#x2F;1+ 1&#x3D;13</p><p>所以输出特征矩阵的尺寸为[13, 13, 256]</p><p><strong>Conv5</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch</span></span><br><span class="line">self.conv5 = nn.Conv2d(in_channels=<span class="number">192</span>, kernel_size=<span class="number">3</span>, out_channels=<span class="number">128</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># tensorflow</span></span><br><span class="line"><span class="comment"># -&gt;[None, 13, 13, 128]</span></span><br><span class="line">x = layers.Conv2D(filters=<span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br></pre></td></tr></table></figure><p><strong>最大池化下采样3</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">maxpool3:</span><br><span class="line">input<span class="built_in">_</span>size:[13, 13, 256] </span><br><span class="line">kernel<span class="built_in">_</span>size:3</span><br><span class="line">padding:0</span><br><span class="line">stride:2</span><br><span class="line"></span><br><span class="line">output<span class="built_in">_</span>size:[6, 6, 256]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输入特征矩阵的shape&#x3D;[13, 13, 256], 输出特征矩阵的shape&#x3D;[6, 6, 256]</p><p>shape计算： (W -F + 2P)&#x2F;S+1 &#x3D; (13- 3 + 2*0)&#x2F;2 + 1&#x3D;6</p><p><strong>Maxpool3</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch</span></span><br><span class="line">self.maxpooling3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># tensorflow</span></span><br><span class="line"><span class="comment"># -&gt;[None, 6, 6, 128]</span></span><br><span class="line">x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)</span><br></pre></td></tr></table></figure><p><strong>全连接层1</strong></p><p><strong>uni_size: 4096, unit_size为全连接层的节点个数， 两块GPU所以翻倍</strong></p><p><strong>全连接层2</strong></p><p><strong>uni_size: 4096, unit_size为全连接层的节点个数， 两块GPU所以翻倍</strong></p><p><strong>全连接层3</strong></p><p><strong>uni_size: 1000</strong>， 该层为输出层， 输出节点数对应分类任务中分类类别数。</p><h4 id="3、参数列表"><a href="#3、参数列表" class="headerlink" title="3、参数列表"></a>3、参数列表</h4><table><thead><tr><th><strong>名称</strong></th><th align="center"><strong>Input_size</strong></th><th><strong>Kernel_size</strong></th><th><strong>Kernel_num</strong></th><th><strong>padding</strong></th><th><strong>Stride</strong></th><th><strong>Output_size</strong></th><th><strong>尺寸计算</strong></th></tr></thead><tbody><tr><td>Conv1</td><td align="center">(224,  224,3)</td><td>11</td><td>48*2</td><td>[1,2]</td><td>4</td><td>(55, 55, 96)</td><td>(224-11+2*2)4+1&#x3D;55</td></tr><tr><td>Maxpooling1</td><td align="center">(55, 55, 96)</td><td>3</td><td></td><td>0</td><td>2</td><td>(27,  27, 96)</td><td>(55-3+2*0)&#x2F;2+1&#x3D;27</td></tr><tr><td>Conv2</td><td align="center">(27,  27, 96)</td><td>5</td><td>128*2</td><td>2</td><td>1</td><td>(27,27, 256)</td><td>(27-5+2*2)&#x2F;1+1&#x3D;27</td></tr><tr><td>Maxpooling2</td><td align="center">(27,27, 256)</td><td>3</td><td></td><td>0</td><td>2</td><td>(13, 13, 256)</td><td>(27-3+2*0)&#x2F;2+1&#x3D;13</td></tr><tr><td>Conv3</td><td align="center">(13, 13, 256)</td><td>3</td><td>192*2</td><td>1</td><td>1</td><td>(13, 13, 384)</td><td>(13-3+2*1)&#x2F;1+1&#x3D;13</td></tr><tr><td>Conv4</td><td align="center">(13, 13, 384)</td><td>3</td><td>192*2</td><td>1</td><td>1</td><td>(13, 13, 384)</td><td>(13-3+2*1)&#x2F;1+1&#x3D;13</td></tr><tr><td>Conv5</td><td align="center">(13,13, 384)</td><td>3</td><td>128*2</td><td>1</td><td>1</td><td>(13, 13, 256)</td><td>(13-3+2*1)&#x2F;1+1&#x3D;13</td></tr><tr><td>Maxpooling3</td><td align="center">(13,  13, 256)</td><td>3</td><td></td><td>0</td><td>2</td><td>(6,6,256)</td><td>(13-3+2*0)&#x2F;2+1&#x3D;6</td></tr><tr><td>FC1</td><td align="center"></td><td>2048</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>FC2</td><td align="center"></td><td>2048</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>FC3</td><td align="center"></td><td>1000</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><h4 id="4、代码实现"><a href="#4、代码实现" class="headerlink" title="4、代码实现"></a>4、代码实现</h4><h5 id="1、pytorch实现"><a href="#1、pytorch实现" class="headerlink" title="1、pytorch实现"></a>1、pytorch实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_alexnet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, kernel_size=<span class="number">11</span>, out_channels=<span class="number">48</span>, padding=<span class="number">2</span>, stride=<span class="number">4</span>)</span><br><span class="line">        self.maxpooling1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">48</span>, kernel_size=<span class="number">5</span>, out_channels=<span class="number">128</span>, padding=<span class="number">2</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.maxpooling2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, out_channels=<span class="number">192</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv4 = nn.Conv2d(in_channels=<span class="number">192</span>, kernel_size=<span class="number">3</span>, out_channels=<span class="number">192</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv5 = nn.Conv2d(in_channels=<span class="number">192</span>, kernel_size=<span class="number">3</span>, out_channels=<span class="number">128</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.maxpooling3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">128</span> * <span class="number">6</span> * <span class="number">6</span>, out_features=<span class="number">2048</span>)</span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">2048</span>)</span><br><span class="line">        self.fc3 = nn.Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.maxpooling1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.maxpooling2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.conv4(x)</span><br><span class="line">        x = self.conv5(x)</span><br><span class="line">        x = self.maxpooling3(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">128</span> * <span class="number">6</span> * <span class="number">6</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu((self.fc2(x)))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># input = torch.rand([32, 3, 224, 224])</span></span><br><span class="line"><span class="comment"># alexnet = AlexNet()</span></span><br><span class="line"><span class="comment"># print(alexnet)</span></span><br><span class="line"><span class="comment"># output = alexnet(input)</span></span><br><span class="line"><span class="comment"># print(output)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2、TensorFlow实现"><a href="#2、TensorFlow实现" class="headerlink" title="2、TensorFlow实现"></a>2、TensorFlow实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_alexnet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：0399</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, Model, Sequential</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">AlexNet_v1</span>(<span class="params">im_height=<span class="number">224</span>, im_width=<span class="number">224</span>, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># tensorflow中的通道顺序是NHWC</span></span><br><span class="line">    input_image = layers.Input(shape=(im_height, im_width, <span class="number">3</span>), dtype=<span class="string">&quot;float32&quot;</span>)  <span class="comment"># [None, 224, 224, 3]</span></span><br><span class="line">    <span class="comment"># x = layers.Conv2D()</span></span><br><span class="line">    x = layers.ZeroPadding2D(((<span class="number">1</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>)))(input_image)  <span class="comment"># [None, 227, 227, 3]</span></span><br><span class="line">    <span class="comment"># (227 - 11 + 2*0) / 4 + 1 = 55  -&gt; [None, 55, 55, 48]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">48</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)  <span class="comment"># [None, 27, 27, 48]</span></span><br><span class="line">    <span class="comment"># 当stride=1且padding=same时， 表示输出尺寸与输入尺寸相同 -&gt;[None, 27, 27, 128]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">128</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 27, 27, 128] -&gt; [None, 13, 13, 128]</span></span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)</span><br><span class="line">    <span class="comment"># stride=1, padding=same, 输出不变 -&gt;[None, 13, 13, 192]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># -&gt;[None, 13, 13, 192]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># -&gt;[None, 13, 13, 128]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># -&gt;[None, 6, 6, 128]</span></span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = layers.Flatten()(x)  <span class="comment"># [None, 128*6*6]</span></span><br><span class="line">    x = layers.Dropout(<span class="number">0.2</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 2048]</span></span><br><span class="line">    x = layers.Dense(<span class="number">2048</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    x = layers.Dropout(<span class="number">0.2</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 2048]</span></span><br><span class="line">    x = layers.Dense(<span class="number">2048</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    x = layers.Dense(num_classes)(x)</span><br><span class="line"></span><br><span class="line">    predict = layers.Softmax()(x)</span><br><span class="line">    <span class="built_in">print</span>(predict)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># model = models.Model(inputs=input_image, outputs=predict)</span></span><br><span class="line">    model = models.Model(inputs=input_image, outputs=predict)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet_v2</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet_v2, self).__init__()</span><br><span class="line">        self.features = Sequential([</span><br><span class="line">            <span class="comment"># [None, 224, 224, 3] -&gt; [None, 227, 227, 3]</span></span><br><span class="line">            layers.ZeroPadding2D(((<span class="number">1</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>))),</span><br><span class="line">            <span class="comment"># padding=&quot;valid&quot;表示向上取整 (227 - 11)/4 + 1=55 [None, 227, 227, 3]-&gt;[None, 55, 55, 48]</span></span><br><span class="line">            layers.Conv2D(filters=<span class="number">48</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            <span class="comment"># [55, 55, 48] -&gt; [None, 27, 27, 48]</span></span><br><span class="line">            layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">            <span class="comment"># stride=1, padding=same, 尺寸不变 [None, 27, 27, 48] -&gt;[None, 27, 27, 128]</span></span><br><span class="line">            layers.Conv2D(filters=<span class="number">128</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            <span class="comment"># [None, 27, 27, 128] -&gt;[None, 13, 13, 128]</span></span><br><span class="line">            layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">            <span class="comment"># [None, 13, 13, 128] -&gt; [None, 13, 13, 192]</span></span><br><span class="line">            layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            <span class="comment"># [None, 13, 13, 192] -&gt; [None, 13, 13, 192]</span></span><br><span class="line">            layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            <span class="comment"># [None, 13, 13, 192] -&gt; [None, 13, 13, 128]</span></span><br><span class="line">            layers.Conv2D(filters=<span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            <span class="comment"># [None, 13, 13, 128] -&gt; [None, 6, 6, 128]</span></span><br><span class="line">            layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [None, 128*6*6]</span></span><br><span class="line">        self.flatten = layers.Flatten()</span><br><span class="line">        self.classifier = Sequential([</span><br><span class="line">            layers.Dropout(<span class="number">0.2</span>),</span><br><span class="line">            layers.Dense(<span class="number">1024</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            layers.Dropout(<span class="number">0.2</span>),</span><br><span class="line">            layers.Dense(<span class="number">128</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            layers.Dense(num_classes),</span><br><span class="line">            layers.Softmax()</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># input = tf.random.uniform(shape=(16, 224, 32, 3))</span></span><br><span class="line"><span class="built_in">input</span> = tf.random.uniform(shape=(<span class="number">8</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># alexnet = AlexNet_v2(num_classes=5)</span></span><br><span class="line"><span class="comment"># print(alexnet)</span></span><br><span class="line"><span class="comment"># print(alexnet.call(input))</span></span><br><span class="line">AlexNet_v1(<span class="number">224</span>, <span class="number">224</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;1、AlexNet简介&quot;&gt;&lt;a href=&quot;#1、AlexNet简介&quot; class=&quot;headerlink&quot; title=&quot;1、AlexNet简介&quot;&gt;&lt;/a&gt;1、AlexNet简介&lt;/h4&gt;&lt;p&gt;Alexnet是2012年ILSVRC 2012(ImageNet </summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>scipy中常用的数据结构</title>
    <link href="https://guudman.github.io/2023/10/11/scipy%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    <id>https://guudman.github.io/2023/10/11/scipy%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</id>
    <published>2023-10-11T07:28:31.000Z</published>
    <updated>2023-10-11T07:31:37.397Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1、-scipy中常用的数据结构"><a href="#1、-scipy中常用的数据结构" class="headerlink" title="1、 scipy中常用的数据结构"></a>1、 scipy中常用的数据结构</h3><h4 id="1-1-scipy-sparse-coo-matrix"><a href="#1-1-scipy-sparse-coo-matrix" class="headerlink" title="1.1 scipy.sparse.coo_matrix"></a>1.1 scipy.sparse.coo_matrix</h4><p>coo_matrix全称是A sparse matrix in Coordinate format， 一种基于坐标格式的系数矩阵， 每个矩阵时一个三元组（行， 列， 值）</p><p>构造方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> coo_matrix</span><br><span class="line">coo = coo_matrix(np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">6</span>]).reshape(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(coo)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  (0, 0)1</span></span><br><span class="line"><span class="string">  (0, 1)2</span></span><br><span class="line"><span class="string">  (0, 2)3</span></span><br><span class="line"><span class="string">  (1, 0)4</span></span><br><span class="line"><span class="string">  (1, 1)5</span></span><br><span class="line"><span class="string">  (1, 2)6</span></span><br><span class="line"><span class="string">  coo_matrix中值表示非零元素， 如果该位置是0， 则没有改行的数值</span></span><br><span class="line"><span class="string">  如， 当输入的array是[1, 0, 3, 0, 5]</span></span><br><span class="line"><span class="string">  (0, 0)1</span></span><br><span class="line"><span class="string">  (0, 2)3</span></span><br><span class="line"><span class="string">  (1, 1)5</span></span><br><span class="line"><span class="string">  (1, 2)6</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 第二种方法</span></span><br><span class="line">row = np.array([<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">col = np.array([<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">data = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>])</span><br><span class="line">coo_matrix_1 = coo_matrix((data, (row, col)), shape=(<span class="number">4</span>, <span class="number">4</span>)).toarray()</span><br><span class="line"><span class="built_in">print</span>(coo_matrix_1)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">coo_matrix((data, (i, j)), [shape=(M, N)])</span></span><br><span class="line"><span class="string">data即矩阵存储的数据，i为行下标，j为列下标，</span></span><br><span class="line"><span class="string">data,i,j的关系为：A[i[k], j[k]] = data[k]</span></span><br><span class="line"><span class="string">0行0列是4， 3行3列是5， 以此类推</span></span><br><span class="line"><span class="string">[[4 0 9 0]</span></span><br><span class="line"><span class="string"> [0 7 0 0]</span></span><br><span class="line"><span class="string"> [0 0 0 0]</span></span><br><span class="line"><span class="string"> [0 0 0 5]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="1-2-scipy-sparse-csr-matrix"><a href="#1-2-scipy-sparse-csr-matrix" class="headerlink" title="1.2 scipy.sparse.csr_matrix"></a>1.2 scipy.sparse.csr_matrix</h4><p>csr是Compressed Sparse Row matrix的缩写， 即基于行存储的压缩稀疏矩阵，</p><p>有几种不同的构造方法， csr_matrix(D), D是一个稠密矩阵或2维的ndarray</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csr_matrix</span><br><span class="line">csr = csr_matrix(np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]).reshape(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">其实跟coo_matrix很像</span></span><br><span class="line"><span class="string">  (0, 0)1</span></span><br><span class="line"><span class="string">  (0, 1)2</span></span><br><span class="line"><span class="string">  (0, 2)3</span></span><br><span class="line"><span class="string">  (1, 0)4</span></span><br><span class="line"><span class="string">  (1, 1)5</span></span><br><span class="line"><span class="string">  (1, 2)6</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment">#  csr_matrix((M, N), [dtype])构造一个shape为(M, N)的dtype类型的空矩阵</span></span><br><span class="line">csr_matrix_1 = csr_matrix((<span class="number">3</span>, <span class="number">4</span>), dtype=np.int8).toarray()</span><br><span class="line"><span class="built_in">print</span>(csr_matrix_1)</span><br><span class="line"><span class="string">&quot;&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[[0 0 0 0]</span></span><br><span class="line"><span class="string"> [0 0 0 0]</span></span><br><span class="line"><span class="string"> [0 0 0 0]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 这种方法跟coo_matrix中的也是一样的</span></span><br><span class="line">row = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">col = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">data = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">csr_matrix_2 = csr_matrix((data, (row, col)), shape=(<span class="number">3</span>, <span class="number">3</span>)).toarray()</span><br><span class="line"><span class="built_in">print</span>(csr_matrix_2)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[[1 0 2]</span></span><br><span class="line"><span class="string"> [0 0 3]</span></span><br><span class="line"><span class="string"> [4 5 6]]</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string"> 按行存储，即先存储第0行，然后第1行，依次到最后一行，即先扫描row数组的数据，第一个数据是0即第0行，然后扫描col的第一个数据是0即第0列，那么第0行第0列存储的值就是data的第一个数据即1，然后继续扫描row的第二个数据还是0即还是第0行，col对应的第二个数据是2即第2列，data的第二个数据是2，即第0行第2列的数据是2，依次扫描row，找对应的col和data构造稀疏矩阵。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1、-scipy中常用的数据结构&quot;&gt;&lt;a href=&quot;#1、-scipy中常用的数据结构&quot; class=&quot;headerlink&quot; title=&quot;1、 scipy中常用的数据结构&quot;&gt;&lt;/a&gt;1、 scipy中常用的数据结构&lt;/h3&gt;&lt;h4 id=&quot;1-1-scipy</summary>
      
    
    
    
    
    <category term="图神经网络" scheme="https://guudman.github.io/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>neo4j基本使用</title>
    <link href="https://guudman.github.io/2023/09/11/neo4j%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
    <id>https://guudman.github.io/2023/09/11/neo4j%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</id>
    <published>2023-09-11T11:09:51.000Z</published>
    <updated>2023-11-04T02:58:15.218Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1-neo4j数据类型"><a href="#1-neo4j数据类型" class="headerlink" title="1. neo4j数据类型"></a>1. neo4j数据类型</h4><p>1.Node: 节点， 基本语法：Node(*label, **properties)</p><p>第一步：连接neo4j数据库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> py2neo <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;http://localhost:7474&quot;</span></span><br><span class="line">graph = Graph(url, username=<span class="string">&quot;neo4j&quot;</span>, password=<span class="string">&quot;szyh&quot;</span>)</span><br></pre></td></tr></table></figure><p>第二步：创建节点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立节点</span></span><br><span class="line">node_1 = Node(<span class="string">&quot;英雄&quot;</span>, name=<span class="string">&quot;张无忌&quot;</span>)</span><br><span class="line">node_2 = Node(<span class="string">&quot;英雄&quot;</span>, name=<span class="string">&quot;杨道&quot;</span>, 武力值=<span class="string">&#x27;100&#x27;</span>)</span><br><span class="line">node_3 = Node(<span class="string">&quot;派别&quot;</span>, name=<span class="string">&quot;明教&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 存入图数据库</span></span><br><span class="line">graph.create(node_1)</span><br><span class="line">graph.create(node_2)</span><br><span class="line">graph.create(node_3)</span><br><span class="line"><span class="built_in">print</span>(node_1)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/guudman/blog_images/raw/master/1692668067239-e7227df7-867e-41ad-9cf6-fb6d85176385.png" alt="image"></p><p>2 relationship关系基本语法： Relationship((start_node, type, end_node, **properties)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加关系</span></span><br><span class="line">node_1_to_node_2 = Relationship(node_1, <span class="string">&#x27;教主&#x27;</span>, node_2)</span><br><span class="line">node_3_to_node_1 = Relationship(node_3, <span class="string">&#x27;统领&#x27;</span>, node_1)</span><br><span class="line">node_2_to_node_3 = Relationship(node_2, <span class="string">&#x27;师出&#x27;</span>, node_3)</span><br><span class="line">graph.create(node_1_to_node_2)</span><br><span class="line">graph.create(node_3_to_node_1)</span><br><span class="line">graph.create(node_2_to_node_3)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/guudman/blog_images/raw/master/1692668510722-f9833f06-967c-46c2-b96a-691de7332604.png" alt="image"></p><ol><li>Path路径，基本语法：Path(*entities), 注意entities是实体(关系，节点都可以作为实体)</li></ol><p><img src="https://gitee.com/guudman/blog_images/raw/master/1692668994710-e1991773-6b57-4b22-8471-ff8e25e395e5.png" alt="image"></p><ol><li>Subgraph:子图是节点和关系的任意集合，它也是Node, Relationship和Path的基类， 基本语法：Subgraph(nodes, relationships)。 空子图表示为None, 使用bool()可以测试是否为空，参数要按数组输入，如下面代码：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建子图，通过子图的方式更新数据库</span></span><br><span class="line">node_7 = Node(<span class="string">&#x27;英雄&#x27;</span>, name=<span class="string">&#x27;张翠山&#x27;</span>)</span><br><span class="line">node_8 = Node(<span class="string">&#x27;英雄&#x27;</span>, name=<span class="string">&#x27;殷素素&#x27;</span>)</span><br><span class="line">node_9 = Node(<span class="string">&#x27;英雄&#x27;</span>, name=<span class="string">&#x27;狮王&#x27;</span>)</span><br><span class="line"></span><br><span class="line">relationship7 = Relationship(node_1, <span class="string">&#x27;生父&#x27;</span>, node_7)</span><br><span class="line">relationship8 = Relationship(node_1, <span class="string">&#x27;生母&#x27;</span>, node_8)</span><br><span class="line">relationship9 = Relationship(node_1, <span class="string">&#x27;义父&#x27;</span>, node_9)</span><br><span class="line">subgraph_1 = Subgraph(nodes=[node_7, node_8, node_9],</span><br><span class="line">                      relationships=[relationship7, relationship8, relationship9])</span><br><span class="line">graph.create(subgraph_1)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/guudman/blog_images/raw/master/1692669527475-c9d0a0dc-9d4d-4ffd-98ed-a9afeb17a4b5.png" alt="image"></p><p>工作流</p><ol><li>GraphService: 基于图服务的工作流</li><li>Graph: 基于图数据库的工作流</li><li>Transaction: 基于事务的工作流，一个Transaction分两个任务，增加一个新节点，将该节点与已有节点创建新关系。这两个任务有一个没完成，整个工作流就不会生效。通常，这种方式通过Graph.begin(readonly&#x3D;False)构造函数构造，参数readonly表示只读</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个新事务</span></span><br><span class="line">transcation_1 = graph.begin()</span><br><span class="line"><span class="comment"># 创建一个新Node</span></span><br><span class="line">node_10 = Node(<span class="string">&#x27;武当&#x27;</span>, name=<span class="string">&#x27;张三丰&#x27;</span>)</span><br><span class="line">transcation_1.create(node_10)</span><br><span class="line"><span class="comment"># 创建两个关系: 张无忌-&gt;(师公)-&gt;张三丰 张翠山-&gt;(妻子)-&gt;殷素素</span></span><br><span class="line">relationship10 = Relationship(node_1, <span class="string">&#x27;师公&#x27;</span>, node_10)</span><br><span class="line">relationship11 = Relationship(node_7, <span class="string">&#x27;妻子&#x27;</span>, node_8)</span><br><span class="line">transcation_1.create(relationship10)</span><br><span class="line">transcation_1.create(relationship11)</span><br><span class="line">transcation_1.commit()</span><br></pre></td></tr></table></figure><p>一个transaction增加的一个节点和两个关系</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/1692670406583-378901a5-73a1-4897-b269-2913bec75ad8.png" alt="image"></p><p>删</p><ol><li>删除数据库中所有的节点和关系: graph.delete_all()</li><li>其他删除方法如下(删除的基础是查询，但凡查询条件没错，就不会删错)</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除所有，</span></span><br><span class="line"><span class="comment"># graph.delete_all()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照id删除，要删除节点前，首先需删除关系，否则会报错:ClientError</span></span><br><span class="line"><span class="comment"># graph.run(&#x27;math (r) where id(r) = 3 delete r&#x27;)</span></span><br><span class="line"><span class="comment"># 按照name属性删除，先增加一个独立的节点</span></span><br><span class="line">node_x = Node(<span class="string">&#x27;英雄&#x27;</span>, name=<span class="string">&#x27;韦一笑&#x27;</span>)</span><br><span class="line"><span class="comment"># graph.create(node_x)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># graph.run(&#x27;match (n:英雄&#123;name:\&#x27;韦一笑\&#x27;&#125;) delete n&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除一个节点及与之相连的关系</span></span><br><span class="line"><span class="comment"># graph.run(&#x27;match (n:英雄&#123;name:\&#x27;韦一笑\&#x27;&#125;) detach delete n&#x27;)</span></span><br><span class="line"><span class="comment"># 删除某一类型的关系</span></span><br><span class="line">graph.run(<span class="string">&#x27;match ()-[r:喜欢]-&gt;() delete r;&#x27;</span>)</span><br><span class="line"><span class="comment"># 删除子图</span></span><br><span class="line"><span class="comment"># delete(subgraph_1)</span></span><br></pre></td></tr></table></figure><p>改</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 改</span><br><span class="line"># 将狮王的武力值改为100</span><br><span class="line">node_9[&#x27;武力值&#x27;]=100</span><br><span class="line"># 本地修改完，要push到服务器上哦</span><br><span class="line">test_graph.push(node_9)</span><br></pre></td></tr></table></figure><p>查询</p><p>查询，按照路径、节点、关系等查询</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了方便查询更多内容，新增一些关系和节点</span></span><br><span class="line">transcation_2 = graph.begin()</span><br><span class="line">node_100 = Node(<span class="string">&#x27;巾帼&#x27;</span>, name=<span class="string">&#x27;赵敏&#x27;</span>)</span><br><span class="line">re_100 = Relationship(node_1, <span class="string">&#x27;Love&#x27;</span>, node_100)</span><br><span class="line"></span><br><span class="line">node_101 = Node(<span class="string">&#x27;巾帼&#x27;</span>, name=<span class="string">&#x27;周芷若&#x27;</span>)</span><br><span class="line">re_101 = Relationship(node_1, <span class="string">&#x27;knows&#x27;</span>, node_101)</span><br><span class="line">re_101_ = Relationship(node_101, <span class="string">&#x27;hate&#x27;</span>, node_100)</span><br><span class="line"></span><br><span class="line">node_102 = Node(<span class="string">&#x27;巾帼&#x27;</span>, name=<span class="string">&#x27;小昭&#x27;</span>)</span><br><span class="line">re_102 = Relationship(node_1, <span class="string">&#x27;knows&#x27;</span>, node_102)</span><br><span class="line"></span><br><span class="line">node_103 = Node(<span class="string">&#x27;巾帼&#x27;</span>, name=<span class="string">&#x27;蛛儿&#x27;</span>)</span><br><span class="line">re_103 = Relationship(node_103, <span class="string">&#x27;Love&#x27;</span>, node_1)</span><br><span class="line">transcation_2.create(node_100)</span><br><span class="line">transcation_2.create(re_100)</span><br><span class="line"></span><br><span class="line">transcation_2.create(node_101)</span><br><span class="line">transcation_2.create(re_101)</span><br><span class="line">transcation_2.create(re_101_)</span><br><span class="line"></span><br><span class="line">transcation_2.create(node_102)</span><br><span class="line">transcation_2.create(re_102)</span><br><span class="line"></span><br><span class="line">transcation_2.create(node_103)</span><br><span class="line">transcation_2.create(re_103)</span><br><span class="line">transcation_2.commit()</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/guudman/blog_images/raw/master/1692672097202-eba07f27-1ef1-4c02-87ee-4c00d87b949d.png" alt="image"></p><p>NodeMatcher: 定位满足特定条件的节点，基本语法：NodeMatcher(graph).match(*labels, **properties)， </p><table><thead><tr><th>方法名</th><th>功能</th></tr></thead><tbody><tr><td>first(）</td><td>返回查询结果第一个Node, 没有则返回空</td></tr><tr><td>where(conditino,properties)</td><td>过滤查询结果</td></tr><tr><td>order_by</td><td>排序</td></tr></tbody></table><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 定义查询</span><br><span class="line">node_match = NodeMatcher(graph)</span><br><span class="line"># # 单个节点， 按照label和name查询</span><br><span class="line">node_single = node_match.match(&#x27;英雄&#x27;, name=&#x27;杨逍&#x27;).first()</span><br><span class="line">print(&quot;单节点查询:&quot;, node_single)</span><br><span class="line"></span><br><span class="line"># # 按label查询</span><br><span class="line"># node_hero = list(node_match.match(&#x27;英雄&#x27;).all())</span><br><span class="line"># print(&quot;查询结果类型&quot;, type(node_hero))</span><br><span class="line">node_hero = node_match.match(&#x27;英雄&#x27;).__iter__()</span><br><span class="line"></span><br><span class="line"># 循环取值</span><br><span class="line">i = 0</span><br><span class="line">for node in node_hero:</span><br><span class="line">    print(&quot;label查询第&#123;&#125;个为&#123;&#125;&quot;.format(i, node))</span><br><span class="line">    i += 1</span><br><span class="line"></span><br><span class="line"># 按照name查询</span><br><span class="line">node_name = node_match.match(name=&#x27;张无忌&#x27;)</span><br><span class="line">print(&quot;name查询结果&quot;, node_name)</span><br><span class="line"></span><br><span class="line"># 按照id查询</span><br><span class="line">node_id = node_match.get(1)</span><br><span class="line">print(&quot;id查询结果&quot;, node_id)</span><br></pre></td></tr></table></figure><ol><li>NodeMatch， 基本用法， NodeMatch(graph, labels&#x3D;frozenset({}), predicates&#x3D;(), order_by&#x3D;(), skip&#x3D;None, limit&#x3D;None)可以看出，NodeMatcher与NodeMatch参数完全不同，后面可以加很多条件</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">node_match1 = NodeMatch(graph, labels=<span class="built_in">frozenset</span>(&#123;<span class="string">&#x27;英雄&#x27;</span>&#125;))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span>*<span class="number">15</span>,<span class="string">&#x27;遍历所有节点&#x27;</span>, <span class="string">&#x27;=&#x27;</span>*<span class="number">15</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> <span class="built_in">iter</span>(node_match1):</span><br><span class="line">    <span class="built_in">print</span>(node)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span>*<span class="number">15</span>,<span class="string">&#x27;查询结果计数&#x27;</span>, <span class="string">&#x27;=&#x27;</span>*<span class="number">15</span>)</span><br><span class="line"><span class="built_in">print</span>(node_match1.__len__())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照武力值排序查询结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span>*<span class="number">15</span>,<span class="string">&#x27;按照武力值排序查询结果&#x27;</span>, <span class="string">&#x27;=&#x27;</span>*<span class="number">15</span>)</span><br><span class="line">wu = node_match1.order_by(<span class="string">&#x27;_.武力值&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> wu:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line">=============== 遍历所有节点 ===============</span><br><span class="line">(_3104:英雄 &#123;name: <span class="string">&#x27;\u6768\u9053&#x27;</span>, 武力值: <span class="string">&#x27;100&#x27;</span>&#125;)</span><br><span class="line">(_3257:英雄 &#123;name: <span class="string">&#x27;\u5f20\u65e0\u5fcc&#x27;</span>&#125;)</span><br><span class="line">(_4209:英雄 &#123;name: <span class="string">&#x27;\u5f20\u7fe0\u5c71&#x27;</span>&#125;)</span><br><span class="line">(_4210:英雄 &#123;name: <span class="string">&#x27;\u72ee\u738b&#x27;</span>, 武力值: <span class="number">10</span>&#125;)</span><br><span class="line">(_4211:英雄 &#123;name: <span class="string">&#x27;\u6bb7\u7d20\u7d20&#x27;</span>&#125;)</span><br><span class="line">=============== 查询结果计数 ===============</span><br><span class="line"><span class="number">5</span></span><br><span class="line">=============== 按照武力值排序查询结果 ===============</span><br><span class="line">(_3104:英雄 &#123;name: <span class="string">&#x27;\u6768\u9053&#x27;</span>, 武力值: <span class="string">&#x27;100&#x27;</span>&#125;)</span><br><span class="line">(_4210:英雄 &#123;name: <span class="string">&#x27;\u72ee\u738b&#x27;</span>, 武力值: <span class="number">10</span>&#125;)</span><br><span class="line">(_3257:英雄 &#123;name: <span class="string">&#x27;\u5f20\u65e0\u5fcc&#x27;</span>&#125;)</span><br><span class="line">(_4209:英雄 &#123;name: <span class="string">&#x27;\u5f20\u7fe0\u5c71&#x27;</span>&#125;)</span><br><span class="line">(_4211:英雄 &#123;name: <span class="string">&#x27;\u6bb7\u7d20\u7d20&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure><ol><li>RelationshipMatcher:用于选择满足一组特定标准的关系的匹配器，基础语法，relation&#x3D;RelationshipMatcher(graph)</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">relation = RelationshipMatcher(graph)</span><br><span class="line"><span class="comment"># Node表示any node</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span>*<span class="number">15</span>,<span class="string">&#x27;hate关系查询&#x27;</span>, <span class="string">&#x27;=&#x27;</span>*<span class="number">15</span>)</span><br><span class="line">x = relation.<span class="keyword">match</span>(nodes=<span class="literal">None</span>, r_type=<span class="string">&#x27;hate&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> x_ <span class="keyword">in</span> x:</span><br><span class="line">    <span class="built_in">print</span>(x_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加两关系</span></span><br><span class="line">rel_1 = Relationship(node_101, <span class="string">&#x27;情敌&#x27;</span>, node_102)</span><br><span class="line">rel_2 = Relationship(node_102, <span class="string">&#x27;情敌&#x27;</span>, node_103)</span><br><span class="line">graph.create(rel_1)</span><br><span class="line">graph.create(rel_2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 情敌查询结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span>*<span class="number">15</span>,<span class="string">&#x27;hate关系查询结果&#x27;</span>, <span class="string">&#x27;=&#x27;</span>*<span class="number">15</span>)</span><br><span class="line">x = relation.<span class="keyword">match</span>(nodes=<span class="literal">None</span>, r_type=<span class="string">&#x27;情敌&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> x_ <span class="keyword">in</span> x:</span><br><span class="line">    <span class="built_in">print</span>(x_)</span><br><span class="line">=============== hate关系查询 ===============</span><br><span class="line">(周芷若)-[:hate &#123;&#125;]-&gt;(赵敏)</span><br><span class="line">=============== hate关系查询结果 ===============</span><br><span class="line">(小昭)-[:情敌 &#123;&#125;]-&gt;(蛛儿)</span><br><span class="line">(周芷若)-[:情敌 &#123;&#125;]-&gt;(小昭)</span><br><span class="line"></span><br><span class="line">Process finished <span class="keyword">with</span> exit code <span class="number">0</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;1-neo4j数据类型&quot;&gt;&lt;a href=&quot;#1-neo4j数据类型&quot; class=&quot;headerlink&quot; title=&quot;1. neo4j数据类型&quot;&gt;&lt;/a&gt;1. neo4j数据类型&lt;/h4&gt;&lt;p&gt;1.Node: 节点， 基本语法：Node(*label, **</summary>
      
    
    
    
    
    <category term="neo4j" scheme="https://guudman.github.io/tags/neo4j/"/>
    
  </entry>
  
  <entry>
    <title>关于我</title>
    <link href="https://guudman.github.io/2023/09/09/%E5%85%B3%E4%BA%8E%E6%88%91/"/>
    <id>https://guudman.github.io/2023/09/09/%E5%85%B3%E4%BA%8E%E6%88%91/</id>
    <published>2023-09-09T07:55:49.000Z</published>
    <updated>2023-09-12T17:44:28.195Z</updated>
    
    <content type="html"><![CDATA[<h2 id="自我介绍"><a href="#自我介绍" class="headerlink" title="自我介绍"></a>自我介绍</h2><p>。。。待更新</p><h2 id="后续计划"><a href="#后续计划" class="headerlink" title="后续计划"></a>后续计划</h2><p>* </p><p>* </p><h2 id="希望你的加入"><a href="#希望你的加入" class="headerlink" title="希望你的加入"></a>希望你的加入</h2><p>* </p><p>* </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;自我介绍&quot;&gt;&lt;a href=&quot;#自我介绍&quot; class=&quot;headerlink&quot; title=&quot;自我介绍&quot;&gt;&lt;/a&gt;自我介绍&lt;/h2&gt;&lt;p&gt;。。。待更新&lt;/p&gt;
&lt;h2 id=&quot;后续计划&quot;&gt;&lt;a href=&quot;#后续计划&quot; class=&quot;headerlink&quot; ti</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>欢迎来到我的世界</title>
    <link href="https://guudman.github.io/2023/09/08/%E4%B8%96%E7%95%8C%EF%BC%8C%E4%BD%A0%E5%A5%BD/"/>
    <id>https://guudman.github.io/2023/09/08/%E4%B8%96%E7%95%8C%EF%BC%8C%E4%BD%A0%E5%A5%BD/</id>
    <published>2023-09-08T08:39:39.830Z</published>
    <updated>2023-09-12T17:44:04.117Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Quick-Start&quot;&gt;&lt;a href=&quot;#Quick-Start&quot; class=&quot;headerlink&quot; title=&quot;Quick Start&quot;&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;&lt;h3 id=&quot;Create-a-new-post&quot;&gt;&lt;a href=&quot;#</summary>
      
    
    
    
    
  </entry>
  
</feed>
