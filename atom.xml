<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ai4Future</title>
  
  
  <link href="https://guudman.github.io/atom.xml" rel="self"/>
  
  <link href="https://guudman.github.io/"/>
  <updated>2023-11-17T09:54:19.237Z</updated>
  <id>https://guudman.github.io/</id>
  
  <author>
    <name>AI4Future</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Mask_R-CNN</title>
    <link href="https://guudman.github.io/2023/11/17/Mask-R-CNN/"/>
    <id>https://guudman.github.io/2023/11/17/Mask-R-CNN/</id>
    <published>2023-11-17T09:53:10.000Z</published>
    <updated>2023-11-17T09:54:19.237Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>Mask R-CNN是2017年发表的文章， 一作是何恺明， 该论文也获得了ICCV 2017年最佳论文奖。并且网络提出后，又霸榜了MS COCO的各项任务，包括目标检测、实例分割以及人体关键点检测任务。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117034359378.png" alt="image-20231117034359378"></p><p>Mask R-CNN是在Faster R-CNN的基础上增加了一个用于预测目标分割Mask的分支（即可预测目标的Bounding Boxes信息、类别信息以及分割Mask信息）</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117034835252.png" alt="image-20231117034835252"></p><p>Mask R-CNN不仅能够同时进行目标检测与分割， 还能很容易扩展到其他任务， 比如预测人体关键点信息。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117035116897.png" alt="image-20231117035116897"></p><p>Mask R-CNN的结构也很简单， 就是通过RoIAlign（在原Fast R-CNN中是RoIPool）得到RoI基础上并行添加一个Mask分支（小型的FCN), 见下图，之前Faster R-CNN是在RoI基础上接上一个Fast R-CNN检测头，即图中class, box分支， 现在又并行了一个Mask分支。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117034359378.png" alt="image-20231117034359378"></p><p>注意带和不带FPN结构的Mask R-CNN在Mask分支上略有不同， 对于带有FPN结构的Mask R-CNN它的class， box分支和Mask分支并不是共用一个RoIAlign。在训练过程中， 对于class， box分支RoIAlign将RPN（Region Proposal Network）得到的Proposals池化到7x7大小， 对于Mask分支RoIAlign将Proposals池化到14×14大小。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117061321377.png" alt="image-20231117061321377"></p><h4 id="RoI-Align"><a href="#RoI-Align" class="headerlink" title="RoI Align"></a>RoI Align</h4><p>在之前的Faster RCNN中， 会使用RoIPool将RPN得到的Proposal池化到相同大小， 这个过程会涉及到quantization或者说取整操作， 这会导致定位不是那么准确（文中称为misalignment问题）</p><p>下面的示意图就是RoIPool的执行过程， 其中会经历两次quantization， 假设通过RPN得到了一个Proposal, 它在原图上的左上角坐标是(10, 10), 右下角的坐标是（124， 124）， 对于要映射的特征层相对原图的步距为32， 通过 RoIPool期望输出为2×2大小。</p><ul><li>将Proposal映射到特征层上， 对于左上角坐标10/32四舍五入后等于0， 对于右下角坐标124/32四舍五入后等于4， 即映射在特征层上的左上角坐标为（0， 0）右下角坐标为（4， 4）。对应下图特征层上从第0行到第4行， 从第0列到第4列的区域（黑色矩形框）, 这是第一次quantization。</li><li>对于期望输出的2×2大小，所以需要经映射在特征层上的Proposal划分成2x2大小区域， 但现在映射在特征层上的Proposal是5×5大小， 无法均分， 所以强行划分后有的区域大有的区域小， 如下图所以，这是第二次quantization。</li><li>对划分后的每个子区域进行maxpool即可得到RoIPool的输出， 即下图中蓝色对应的四个数字。</li></ul><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117065102092.png" alt="image-20231117065102092"></p><p>为解决这个问题， 作者提出了RoIAlign方法代替RoIPool, 以获得更加精细的空间定位信息。</p><p>作者提到将RoI替换成RoIAlign后，分割的Mask准确率相对提升了10%到50%, 并且将预测Mask和class进行了解耦， 解耦后也带来了很大的提升。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117071100480.png" alt="image-20231117071100480"></p><p>下图就是RoIAlign的执行过程，同样假设通过RPN得到了一个Proposal， 它在原图上的左上角坐标是（10， 10）， 右下角的坐标是（124， 124）， 对应要映射的特征层相对原图的步距是32， 通过RoIAlign期望的输出为2×2大小。</p><p>将Proposal映射到特征层上， 左上角作为（0.3125， 0.3125）(不进行四舍五入)， 右下角坐标为（3.875， 3.875）（不进行四舍五入）。为了方便理解， 将特征层上的每个元素都用一个点表示， 就能得到图中下方的gri网格， 图中蓝色的框就是Proposal。</p><p>由于期望输出大小为2x2， 故将Porposal划分成2×2四个子区域，接着根据sampling_ratio在每个子区域中设置采样点， 原论文中默认设置的sampling_ratio为4， 这里为了方便讲解， 将sampling_ratio设置为1.</p><p>然后计算每个子区域中每个采样点的值（利用双线性插值计算）， 最后对每个区域内的所有采样点取均值即为该子区域的输出。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117073133360.png" alt="image-20231117073133360"></p><p>以第一个子区域为例， 这里将sample_ratio设置为1， 所以每个子区域只需设置一个采样点， 第一个子区域的采样点为图中黄色的点（即该子区域的中心点）， 坐标为（1.203， 1.203），然后找到离该采样点最近的四个点（即图中用红色箭头标出的四个黑点），然后利用双线性插值即可计算得到采样点对应的输出-0.8546, 又由于该子区域只有一个采样点， 故该子区域的输出就为-0.8546。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117075332516.png" alt="image-20231117075332516"></p><h3 id="Mask-Branch-FCN"><a href="#Mask-Branch-FCN" class="headerlink" title="Mask Branch(FCN)"></a>Mask Branch(FCN)</h3><p>对于带有FPN和不带有FPN的Mask R-CNN, 它们的Mask分支不太一样， 下图是左边不带有FPN结构的Mask分支， 右侧是带有FPN结构的Mask分支（灰色部分为原Faster R-CNN预测的box， class信息的分支，白色部分为Mask分支）</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117061321377.png" alt="image-20231117061321377"></p><p>下图为带有FPN的Mask分支结构示意图。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/Mask_r-cnn_fpn.jpg" alt="Mask_r-cnn_fpn"></p><p>之前的FCN中提到过， FCN是对每一个像素对每个类别都会预测一个分数， 然后通过softmax得到每个类别的概率， 哪个概率高就将该像素分配给哪个类别， 但在Mask R-CNN中， 作者将预测Mask和class进行了解耦， 即对输入的RoI针对每个类别单独预测一个Mask， 最终根据box， cls分支预测的classes信息来选择对应类别的Mask。解耦后得到很大的提升， 下表是原论文中给出的消融实验结果，其中softmax代表原FCN方式， sigmoid代表Mask R-CNN采取的方式（Mask和class进行了解耦）。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117081349378.png" alt="image-20231117081349378"></p><p>在训练网络的时候输入Mask分支的目标是由RPN提供的， 即Proposals， 但在预测的时候输入Mask分支的目标是由Fast R-CNN提供的（即预测的最终目标）。并且训练时采用的是Proposals全部是Fast R-CNN阶段匹配到的正样本。在训练时Mask利用RPN提供的目标信息能够扩充训练样本的多样性（因为RPN提供的目标边界框并不是很准确， 一个目标可以呈现出不同的情景，类似于围着目标做随机裁剪。从另一个方面来看， 通过Fast R-CNN得到的输出一般都比较准确了， 再通过NMS后剩下的目标就更少了）。在预测时为了获得更加准确的目标分割信息以及减少计算量（通过Fast R-CNN后的目标数会更少）， 此时利用的是Fast R-CNN提供的目标信息。</p><h4 id="Mask-R-CNN损失"><a href="#Mask-R-CNN损失" class="headerlink" title="Mask R-CNN损失"></a>Mask R-CNN损失</h4><p>Mask R-CNN损失就是在Faster R-CNN的基础上加了Mask分支上的损失， 即：</p><script type="math/tex; mode=display">Loss = L_{rpn} + L_{fast_rcnn} + L_{mask}</script><p>关于mask分支上的损失就是二值交叉熵损失（Binary Cross Entropy）</p><h4 id="Maskf分支损失"><a href="#Maskf分支损失" class="headerlink" title="Maskf分支损失"></a>Maskf分支损失</h4><p>在理解Mask分支损失计算之前， 要弄清楚logist（网络预测的输出）是什么， targets（对应的GT）是什么。前面有提到训练时输入Mask分支的目标是RPN提供的Proposals， 所以网络预测的logits是针对每个Proposal对应每个类别的Mask信息（注意预测的mask大小都是28×28）。并且这里输入的Proposals都是正样本（在Fast R-CNN阶段采样得到的）， 对应的GT信息（box， cls）也是知道的。</p><p>如下图所示， 假设通过RPN得到了一个Proposal（图中黑色的矩形框）， 通过RoIAlign后得到对应的特征信息（shape为14×14×c）。接着通过Mask  Branch预测每个类别的Mask信息得到图中的logits（logits通过sigmoid激活函数后， 所有的值都被映射到0和1之间）。通过Fast R-CNN分支正负样本匹配过程我们能够知道该Proposal的GT类别为猫（cat）， 所以将logits中对应类别猫的预测mask（shape为28×28）提取出来。然后根据Proposal在原图对应的GT上裁剪并缩放到28×28大小，得到图中的GT mask（对应目标区域为1， 背景区域为0），最后计算logits中预测类别为猫的mask与GT amsk的BCELoss（BinaryCrossEntropyLoss）即可。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/Mask_r-cnn_1.jpg" alt="Mask_r-cnn_1"></p><h4 id="Mask-Branch预测使用"><a href="#Mask-Branch预测使用" class="headerlink" title="Mask Branch预测使用"></a>Mask Branch预测使用</h4><p>在真正预测推理的时候， 输入Mask分支的目标是由Fast R-CNN分支提供的。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/Mask_r-cnn_1_1.jpg" alt="Mask_r-cnn_1_1"></p><p>如上图所示， 通过Fast R-CNN分支， 我们能够得到最终的预测目标框架框信息以及类别信息。接着将目标边界框信息提供给Mask分支就能预测得到该目标的logits信息。再根据Fast R-CNN分支提供的类别信息将logits对应类别的Mask信息提取出来，即针对该目标预测的Mask信息（shape为28x28， 由于通过sigmoid激活函数， 数值在0都1之间）， 然后利用双线性插值将Mask缩放到预测目标框大小， 并放到原图对应区域。接着通过设置的阈值(默认为0.5)将mask转换成一张二值图，比如预测值大于0.5的区域设置为前景剩下区域设置为背景。现在对预测的每个目标就可以在原图中绘制出边界框信息， 类别信息以及目标Mask信息。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117174543776.png" alt="image-20231117174543776"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h4&gt;&lt;p&gt;Mask R-CNN是2017年发表的文章， 一</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>YoloV5</title>
    <link href="https://guudman.github.io/2023/11/17/YoloV5/"/>
    <id>https://guudman.github.io/2023/11/17/YoloV5/</id>
    <published>2023-11-17T09:46:42.000Z</published>
    <updated>2023-11-17T10:05:55.027Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>YOLOv5项目的作者是<code>Glenn Jocher</code>并不是原<code>Darknet</code>项目的作者<code>Joseph Redmon</code>。并且这个项目至今都没有发表过正式的论文。<a href="https://github.com/ultralytics/yolov5">YOLOV5仓库</a>早在2020年5月就已创建， 如今已迭代多个版本。本文是针对V6.1版本展开的， 下表是V6.1版本中贴出的关于不同大小模型以及输入尺度对应的mAP、推理速度、参数数量以及理论计算的FLOPs。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Model</th><th>size</th><th style="text-align:center">mAPval</th><th>mAPval</th><th>Speed</th><th>Speed</th><th>Speed</th><th>params</th><th>FLOPs</th></tr></thead><tbody><tr><td style="text-align:center"></td><td>pixels</td><td style="text-align:center">0.5:0.95</td><td>0.5</td><td>CPU b1(ms)</td><td>V100 b1(ms)</td><td>V100 b32(ms)</td><td>(M)</td><td>@640 (B)</td></tr><tr><td style="text-align:center">YOLOv5n</td><td>640</td><td style="text-align:center">28</td><td>45.7</td><td>45</td><td>6.3</td><td>0.6</td><td>1.9</td><td>4.5</td></tr><tr><td style="text-align:center">YOLOv5s</td><td>640</td><td style="text-align:center">37.4</td><td>56.8</td><td>98</td><td>6.4</td><td>0.9</td><td>7.2</td><td>16.5</td></tr><tr><td style="text-align:center">YOLOv5m</td><td>640</td><td style="text-align:center">45.4</td><td>64.1</td><td>224</td><td>8.2</td><td>1.7</td><td>21.2</td><td>49</td></tr><tr><td style="text-align:center">YOLOv5l</td><td>640</td><td style="text-align:center">49</td><td>67.3</td><td>430</td><td>10.1</td><td>2.7</td><td>46.5</td><td>109.1</td></tr><tr><td style="text-align:center">YOLOv5x</td><td>640</td><td style="text-align:center">50.7</td><td>68.9</td><td>766</td><td>12.1</td><td>4.8</td><td>86.7</td><td>205.7</td></tr><tr><td style="text-align:center">YOLOv5n6</td><td>1280</td><td style="text-align:center">36</td><td>54.4</td><td>153</td><td>8.1</td><td>2.1</td><td>3.2</td><td>4.6</td></tr><tr><td style="text-align:center">YOLOv5s6</td><td>1280</td><td style="text-align:center">44.8</td><td>63.7</td><td>385</td><td>8.2</td><td>3.6</td><td>12.6</td><td>16.8</td></tr><tr><td style="text-align:center">YOLOv5m6</td><td>1280</td><td style="text-align:center">51.3</td><td>69.3</td><td>887</td><td>11.1</td><td>6.8</td><td>35.7</td><td>50</td></tr><tr><td style="text-align:center">YOLOv5l6</td><td>1280</td><td style="text-align:center">53.7</td><td>71.3</td><td>1784</td><td>15.8</td><td>10.5</td><td>76.8</td><td>111.4</td></tr><tr><td style="text-align:center">YOLOv5x6</td><td>1280</td><td style="text-align:center">55</td><td>72.7</td><td>3136</td><td>26.2</td><td>19.4</td><td>140.7</td><td>209.8</td></tr><tr><td style="text-align:center">+TTA</td><td>1536</td><td style="text-align:center">55.8</td><td>72.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></div><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>网络结构由以下几个部分组成</p><p>Backbone: New CSP-Darknet53</p><p>Neck: SPPF, New CSP-PAN</p><p>Head； YOLOv3 Head</p><p>下面是根据yolov5l.yaml绘制的网络整体结构， YOLOv5针对不同大小（n, s, m, l, x）的网络整体架构都一样， 只不过在每个子模块中采用不同的深度和宽度， 分别对应yaml文件中的depth_multiple和width_multiple参数。另外， 官方出了n, s, m, l, x版本外还有n6, s6, l6, x6，区别在于后者是针对更大分辨率的图片比如1280x1280， 当然结构上也有些差异， 后者会下采样64倍， 采用4个预测特征层， 而前者只会下采样到32倍且采用3个预测特征层。本文只讨论前者，下图为yolov5l。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov5.jpg" alt="yolov5"></p><p>对于YOLOV4， 其实YOLOV5在<strong>Backbone</strong>部分无太大变化。但YOLOV5在v6.0版本相比之前版本有一个很小的改动， 把网络的第一层（原来是Focus模块）换成了一个6×6大小的卷积层。<strong>二者在理论上是等价的</strong>，但对于现有的一些GPU设备（以及相应的优化算法）使用6×6大小的卷积才能够比使用Focus模块更加高效。</p><p>下图是原来的Focus模块（和之前的Swin Transformer中的Patch Merging类似）， 将每个2×2的相邻像素划分为一个patch， 然后将每个patch中相同位置（同一颜色）像素拼接在一起就得到了4个feature map， 然后再接上一个3×3大小的卷积层。这和直接使用一个6×6大小的卷积层等效。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov5_focus.jpg" alt="yolov5_focus"></p><p>在Neck部分的变化还是相比较大， 首先是将SPP换成了SPPF（Glenn Jocher自己设计的）， 二者的作用一样， 但是后者的效率更高。SPP结构如下图所示， 是将输入并行通过多个大小不同的Maxpool， 然后做进一步融合， 能够在一定程度上解决目标多尺度问题。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov5_spp.jpg" alt="yolov5_spp"></p><p>而SPPF结构是将输入串行通过多个5×5大小的MaxPool层， 这里需要注意的是串行两个5x5大小的MaxPool层是和一个9x9大小的MaxPoll层计算结果是一样的， 串行三个5x5大小的MaxPool层是和一个13x13大小的MaxPoll层计算结果是一样的</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/SPPF.jpg" alt="SPPF"></p><h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><p>YOLOv5中使用了多种数据增强</p><p><strong>Mosaic</strong>，将四张图片拼成一张图片</p><p><strong>Copy paste</strong>，将部分目标随机的粘贴到图片中，前提是数据要有<code>segments</code>数据才行，即每个目标的实例分割信息。</p><p><strong>Random affine(Rotation, Scale, Translation and Shear)</strong>，随机进行仿射变换，但根据配置文件里的超参数发现只使用了<code>Scale</code>和<code>Translation</code>即缩放和平移。</p><p><strong>MixUp</strong>，就是将两张图片按照一定的透明度融合在一起，具体有没有用不太清楚，毕竟没有论文，也没有消融实验。代码中只有较大的模型才使用到了<code>MixUp</code>，而且每次只有10%的概率会使用到。</p><p><strong>Albumentations</strong>，主要是做些滤波、直方图均衡化以及改变图片质量等等，我看代码里写的只有安装了<code>albumentations</code>包才会启用，但在项目的<code>requirements.txt</code>文件中<code>albumentations</code>包是被注释掉了的，所以默认不启用。</p><p><strong>Augment HSV(Hue, Saturation, Value)</strong>，随机调整色度，饱和度以及明度。</p><h4 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h4><p>yolov5中使用了多种训练策略， 这里简单总结几点</p><p><strong>Multi-scale training(0.5~1.5x)，</strong>多尺度训练，假设设置输入图片的大小为640 × 640 ，训练时采用尺寸是在0.5 × 640 ∼ 1.5 × 640 之间随机取值，注意取值时取得都是32的整数倍（因为网络会最大下采样32倍）。</p><p><strong>AutoAnchor(For training custom data)</strong>，训练自己数据集时可以根据自己数据集里的目标进行重新聚类生成Anchors模板。</p><p><strong>Warmup and Cosine LR scheduler</strong>，训练前先进行<code>Warmup</code>热身，然后在采用<code>Cosine</code>学习率下降策略</p><p><strong>EMA(Exponential Moving Average)</strong>，可以理解为给训练的参数加了一个动量，让它更新过程更加平滑</p><p><strong>Mixed precision</strong>，混合精度训练，能够减少显存的占用并且加快训练速度，前提是GPU硬件支持。</p><p><strong>Evolve hyper-parameters</strong>，超参数优化，没有炼丹经验的人勿碰，保持默认就好。</p><h4 id="损失计算"><a href="#损失计算" class="headerlink" title="损失计算"></a>损失计算</h4><p>YOLOv5的损失主要是三部分组成。</p><p>Classes loss, 分类损失， 采用的是BCE loss， 注意只计算正样本的分类损失</p><p>Objectness loss, obj损失， 采用的是BCE loss， 这里的obj表示网络预测的目标边界框与GT Box的CIoU， 这里计算的是所有样本的obj损失</p><p>Location loss: 定位损失， 采用的是CIoU loss, 注意只计算正样本的定位损失</p><script type="math/tex; mode=display">Loss = \lambda_1L_{cls} + \lambda_2L_{obj} + \lambda_3L_{loc}</script><p>其中lambda为平衡系数</p><h4 id="平衡不同尺度的损失"><a href="#平衡不同尺度的损失" class="headerlink" title="平衡不同尺度的损失"></a>平衡不同尺度的损失</h4><p>这里是指三个预测特征层（P3,P4,P5)上的obj损失采用不同的权重， 针对预测小目标的预测特征层（P3)采用的权重是4.0， 针对中等目标的预测特征层（P4）采用的权重是1.0, 针对预测大目标的预测特征层（P5）采用的权重是0.4， 这里说的是针对COCO数据集设置的超参数。</p><script type="math/tex; mode=display">L_{obj} = 4.0 \cdot L_{obj}^{medium} + 0.4 \cdot L^{large}_{obj}</script><h4 id><a href="#" class="headerlink" title=" "></a> </h4><h4 id="消除Grid敏感度"><a href="#消除Grid敏感度" class="headerlink" title="消除Grid敏感度"></a>消除Grid敏感度</h4><p>主要是调整目标中心点相对Grid网络的左上角偏移量，下图是YOLOv2， v3的计算公式。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117020720005.png" alt="image-20231117020720005"></p><p>其中</p><p>t_x是网络预测的目标中心x坐标偏移量（相对于网格的左上角）</p><p>t_y是网络预测的目标中心y坐标偏移量（相对于网格的左上角）</p><p>c_x是对应网格左上角的x坐标</p><p>c_y是对应网络左上角的y坐标</p><script type="math/tex; mode=display">\sigma</script><p>是Sigmoid激活函数， 将预测的偏移量限制到0和1之间， 即预测的中心点不会超出对应的Grid Cell区域。关于预测目标中心点相对Grid网格左上角（c_x, c_y）偏移量为σ(t_x)， σ(t_y)， yolov4作者认为这样不是很合理， 比如当真实目标中心带你非常靠近网格的左上角点【意味着σ(t_x)， σ(t_y)趋近于0】或者右下角点【意味着σ(t_x)， σ(t_y)趋近于1】时， 网络预测值需要负无穷或者正无穷时才能取到，而这种极端的值网络一般无法达到。为解决这个问题， 作者对偏移量进行了缩放，从原来的（0， 1）缩放到（-0.5， 1.5）, 这样网络预测的偏移量就能方便达到0或者1， 因此最终预测的目标中心点b_x, b_y的计算公式为：</p><script type="math/tex; mode=display">b_x = (2 \cdot \sigma(t_x) - 0.5) + c_x</script><script type="math/tex; mode=display">b_y = (2 \cdot \sigma(t_y) - 0.5) + c_y</script><p>在YOLOv5中除了调整预测Anchor相对Grid网格左上角（c_x, c_y)偏移量外， 还调整了预测目标高宽的计算公式， 之前是：<script type="math/tex">b_w = p_w \cdot e^{tw}</script></p><script type="math/tex; mode=display">b_h = p_h \cdot e^{th}</script><p>在YOLOv5调整为：</p><script type="math/tex; mode=display">b_w = p_w \cdot (2 \cdot \sigma(t_w)) ^ 2</script><script type="math/tex; mode=display">b_h = p_h \cdot (2 \cdot \sigma(t_h)) ^ 2</script><p>原来的计算公式并没有对预测目标宽高做限制，这样可能出现梯度爆炸，训练不稳定等问题。</p><h4 id="匹配正样本（Buidl-Targets"><a href="#匹配正样本（Buidl-Targets" class="headerlink" title="匹配正样本（Buidl Targets)"></a>匹配正样本（Buidl Targets)</h4><p>yolov5与yolov4类似， 主要区别在于GT Box与Anchor Templates模板的匹配方式，在yolov4中直接将每个GT Box与对应的Anchor Template模板计算IoU， 只要IoU大于设定的阈值就算匹配成功， 但在YOLOv5中， 作者先去计算每个GT Box与对应的Anchor Template目标的高宽比例， 即</p><script type="math/tex; mode=display">r_w = w_{gt} / w_{at}</script><script type="math/tex; mode=display">r_h = h_{gt} / h_{at}</script><p>然后统计这些比例和它们倒数之间的最大值， 这里可以理解成计算GT Box和Anchor Templates分别在宽高一级高度方向的最大差异（当相等的时候比例为1， 差异最小）</p><script type="math/tex; mode=display">r^{max}_{w} = max(r_w, 1/r_w)</script><script type="math/tex; mode=display">r^{max}_{h} = max(r_h, 1/r_h)</script><p>接着统计$$r^{max}_w, r^{max}_h之间的最大值， 即宽度和高度方向差异最大的值：</p><script type="math/tex; mode=display">r^{max} = max(r^{max}_w, r^{max}_h)</script><p>如果GT Box和对应的Anchor Template的r^{max}小于阈值anchor_t（在源码中默认设置为4.0）， 即GT Box和对应的Anchor Template的高，宽比例相差不算太大，则将GT Box分配给该Anchor Template模板。为方便理解，做图如下。假设对某个GT Box而言， 其实只要GT Box满足在某个Anchor Template高和宽的x0.25和x4.0倍之间就算匹配成功。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117024711216.png" alt="image-20231117024711216"></p><p>剩下的步骤和YOLOv4中的一致</p><p>将GT投影到对应预测特征层上， 根据GT的中心点定位到对应的cell， 注意图中有三个对应的cell， 因为网络预测中心点的偏移范围已经调整到了（-0.5， 1.5），所以按理说只要Grid Cell左上角点距离GT中心点在（-0.5， 1.5）范围内它们对应的Anchor都能回归到GT的位置处，这样就能让正样本的数量得到大量的扩充。 </p><p>则这个三个Cell对应的AT2和AT3都为正样本。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117030514848.png" alt="image-20231117030514848"></p><p>还需要注意的是， yolov5源码中扩展Cell时只会往上、下、左、右四个方向扩展， 不会往左上、右下、左下、右下方向扩展。下面又给出了一些根据<script type="math/tex">GT_X^{center}, GT_y^{center}</script>的位置扩展的一些cell案例，其中%1表示会取余并保留一位小数。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117030859200.png" alt="image-20231117030859200"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h4&gt;&lt;p&gt;YOLOv5项目的作者是&lt;code&gt;Glenn </summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Yolov4</title>
    <link href="https://guudman.github.io/2023/11/14/Yolov4/"/>
    <id>https://guudman.github.io/2023/11/14/Yolov4/</id>
    <published>2023-11-14T10:54:43.000Z</published>
    <updated>2023-11-14T10:59:09.105Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>论文原文：</p><p><a href="[arxiv.org/pdf/1506.02640.pdf](https://arxiv.org/pdf/1506.02640.pdf">yolov1原文</a>)</p><p><a href="[arxiv.org/pdf/1612.08242.pdf](https://arxiv.org/pdf/1612.08242.pdf">yolov2原文</a>)</p><p><a href="[arxiv.org/pdf/1804.02767.pdf](https://arxiv.org/pdf/1804.02767.pdf">yolov3原文</a>)</p><p><a href="[arxiv.org/pdf/2004.10934.pdf](https://arxiv.org/pdf/2004.10934.pdf">yolov4原文</a>)</p><p>YOLOV4是2020年AlexEY Bochkovskiy等人发表在CVPR上的一篇文章， 并不是Darknet的原始作者Joseph Redmon发表的， 但这个工作已经被Jose Redmon大佬认可了。如果将YOLOV4和原始的YOLOV3相比较确实有很大的提升， 但和Utralytics版的YOLOV3 SPP相比提升确实不大， 但毕竟Ultralytics的YOLOv3 SPP以及YOLOv5都没有发表过正式的论文， 所以这里先聊聊Alexey Bochkovskit的YOLOV4。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113093117660.png" alt="image-20231113093117660"></p><hr><p>YOLOv4的亮点</p><p>阅读原论文，会发信啊作者就是将当年所有的常用技术罗列了一遍，然后做了一堆消融实验。网络结构</p><p>在论文3.4章节中介绍了YOLOV4网络的具体结构</p><p>Backbone: CSPDarknet53</p><p>Neck: SPP, PAN</p><p>Head: YOLOv3</p><p>相比之前的yolov3, 改进了以下Backbone, 在Darknet53中引入了CSP模块（来自CSPNet）。在Neck部分， 采用了SPP模块（Utralytics版的YOLOv3 SPP就使用到了）以及PAN模块（来自PANet）， head部分使用的还是原来的检测头。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113094007031.png" alt="image-20231113094007031"></p><p>关于SSP（Spatial Pyramid Pooling）模块， SPP就是将特征层分别通过一个池化核大小为5×5， 9×9， 13×13的最大池化层， 最后在通道方向进行concat拼接在做进一步融合， 这样能够在一定程度上解决目标多尺度问题，如下图所示。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov4.jpg" alt="yolov4"></p><p>PAN（Path Aggregation Network）结构其实就是在FPN（从顶到底信息融合）的基础上加上了从底到顶的信息融合， 如下图所示。 </p><p><a href="[arxiv.org/pdf/1803.01534.pdf](https://arxiv.org/pdf/1803.01534.pdf">PAN路径聚合网络论文原文</a>)</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113100223861.png" alt="image-20231113100223861"></p><p>但YOLOv4的PAN结构和原始论文的融合方式又略有差异， 如下图， 图(a)是原始论文的融合方式， 即特征层之间融合时是直接通过相加的方式进行融合的， 但在YOLOV4中是通过在通道方向concat拼接的方式进行融合的。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113100523933.png" alt="image-20231113100523933"></p><hr><h4 id="优化策略"><a href="#优化策略" class="headerlink" title="优化策略"></a>优化策略</h4><p>这里直接说一下训练检测器时作者采用的一些方法。</p><h5 id="Eliminate-grid-sensitivity"><a href="#Eliminate-grid-sensitivity" class="headerlink" title="Eliminate grid sensitivity"></a>Eliminate grid sensitivity</h5><p>在原来的YOLOv3中， 关于计算预测目标中心坐标计算公式是：</p><script type="math/tex; mode=display">b_x = \sigma(t_x) +  c_x</script><script type="math/tex; mode=display">b_y = \sigma(t_y) +  c_y</script><p>其中， tx， ty分别是网络预测的目标中心x、y坐标偏移量（相对于网络的左上角）</p><p>cx, cy分别是对应网络左上角的x、y坐标</p><script type="math/tex; mode=display">\sigma是sigmoid激活函数， 将预测的偏移量控制在0到1之间， 即预测的中心点不会超出赌赢的Grid Cell区域。 ![image-20231113101648651](https://gitee.com/guudman/blog_images/raw/master/image-20231113101648651.png)YOLOv4中作者认为这样不太合理， 比如当真实目标中心点非常靠近网络的左上角或者右下角时， 即预测的偏移趋近于0或趋近于1时， 网络的预测值需要负无穷或正无穷时才能取到， 而这种极端的值网络一般无法达到， 为解决这个问题，作者引入了一个大于1 的缩放系数</script><p>b_x = (\sigma(t_x) \cdot scale_{xy} - \frac{scale_{xy} - 1}{2}) + c_x</p><script type="math/tex; mode=display"></script><p>b_y = (\sigma(t_y) \cdot scale_{xy} - \frac{scale_{xy} - 1}{2}) + c_y</p><p>$$</p><p>通过引入这个系数， 网络的预测值能够达到0或者1。</p><h5 id="Mosaic-data-augmentation"><a href="#Mosaic-data-augmentation" class="headerlink" title="Mosaic data augmentation"></a>Mosaic data augmentation</h5><p>在数据预处理时阿静四张图片拼成一张图片， 增加学习样本的多样性。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113102716181.png" alt="image-20231113102716181"></p><h5 id="IOU-threshold（正样本匹配）"><a href="#IOU-threshold（正样本匹配）" class="headerlink" title="IOU threshold（正样本匹配）"></a>IOU threshold（正样本匹配）</h5><p>在yolov3中针对每一个GT都只分配了一个Anchor, 但在YOLOv4包括之前的YOLOv3 SPP以及YOLOv5中一个GT可以同时分配多个Anchor， 它们是直接使用Anchor模块与GT Boxes进行粗略匹配， 然后在定位到对应cell的对应Anchor。</p><p>针对某个预测特征层采用如下的三种Anchor模块AT 1, AT 2, AT 3</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov4_1.jpg" alt="yolov4_1"></p><p>将每个GT Boxes与每个Anchor模板进行匹配（这里直接将GT和Anchor模板左上角对齐， 然后计算IoU</p><p>如果GT与某个Anchor模块的IoU大于给定的阈值， 则将GT分配给该Anchor模块， 如图中的AT 2</p><p>将GT投影到对应预测特征层上， 根据GT的中心点定位到对应的cell</p><p>则该cell对应的AT2为正样本</p><p>yolov4以及yolov5中关于匹配正样本的方法有些不同， 主要原因是引入了缩放因子scale_xy, 通过缩放后网络预测中心的偏移范围从原来的（0， 1）调整到了（-0.5， -1.5）, 所以对于同一个GT Boxes可以分配更多的Anchor， 即正样本的数量多了。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov4_1_2.jpg" alt="yolov4_1-第 2 页"></p><p>将每个GT Boxes与每个Anchor模板进行匹配（这里直接将GT和Anchor模板左上角对齐， 然后计算IoU, 在yolov4中IoU的阈值设置的是0.213</p><p>如果GT与某个Anchor模板的IoU大于给定的阈值， 则将GT分配给改Anchor模板， 如图中的AT 2</p><p>将GT投影到对应预测特征层上， 根据GT的中心点定位到对应的cell（注意图中有三个对应的cell）</p><p>则这三个cell对应的AT2都为正样本</p><p>为什么图中的GT会定位到3个cell。刚刚说了网络预测中心点的偏移范围已经调整到了（-0.5， 1.5）, 所以按理说只要Grid Cell左上角距离GT中心点在（-0.5， 1.5）范围内它们对应的Anchor都能回归到GT的位置上。在回过头看看上面的例子， GTx^center, GTy^center距离落入的Grid Cell左上角距离都小于0.5，  所以该Grid Cell上方的Cell以及左侧的Cell都满足条件， 即Cell左上角距离GT中心在（-0.5， 1.5）范围内。这样会让正样本的数量得到大量的扩充。但需要注意的是， yolov5源码中扩展cell时只会往上，下，左边，右边四个方向扩展，不会往左上， 右下， 左下， 右下方向扩展。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113114629932.png" alt="image-20231113114629932"></p><h5 id="Optimizer-Anchors"><a href="#Optimizer-Anchors" class="headerlink" title="Optimizer Anchors"></a>Optimizer Anchors</h5><p>在yolov3中使用anchor模板</p><div class="table-container"><table><thead><tr><th>目标类型</th><th>Anchors模板</th></tr></thead><tbody><tr><td>小尺度</td><td>（10×13), (16×30), (33×23)</td></tr><tr><td>中尺度</td><td>（30×61), (62×45), (59×119)</td></tr><tr><td>大尺度</td><td>（116×90), (156×198), (373×326)</td></tr></tbody></table></div><p>在yolov4中针对512×512尺度采用的anchor模板</p><div class="table-container"><table><thead><tr><th>目标类型</th><th>Anchors模板</th></tr></thead><tbody><tr><td>小尺度</td><td>（12×16), (19×36), (40×23)</td></tr><tr><td>中尺度</td><td>（36×75), (76×55), (72×146)</td></tr><tr><td>大尺度</td><td>（142×110), (192×243), (459×401)</td></tr></tbody></table></div><h5 id="CIoU-定位损失"><a href="#CIoU-定位损失" class="headerlink" title="CIoU(定位损失)"></a>CIoU(定位损失)</h5><p>在yolov3中定位损失采用的是MSE损失， 但在yolov4中采用的是CIoU损失。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113150754303.png" alt="image-20231113150754303"></p><h4 id="CSPDarjnet53网络结构"><a href="#CSPDarjnet53网络结构" class="headerlink" title="CSPDarjnet53网络结构"></a>CSPDarjnet53网络结构</h4><p>CSPDarknet53就是将CSP结构融入了Darknet53中。CSP结构是在CSPNet（Cross Stage Partial Network）论文中提出的， CSP作者说在目标检测任务中使用了CSP结构有如下好处：</p><blockquote><ol><li>Strengthening learning ability of a CNN</li><li>Removing computational bottlenecks</li><li>Reducing memroy costs</li></ol></blockquote><p>即减少网络的计算量以及对显存的占用， 同时保证网络的能力不变或者略微提升。 CSP结构的思想参考原论文绘制的CSPDenseNet， 进入每个stage（一般在下采样后）先将数据划分成两部分， 如下图所示的Part1和Part2。但具体怎么划分呢， 在CSPNet中直接按照通道均分， 但在YOLOv4网络中通过两个1x1的卷积层来实现的。在Part2后跟一堆Blocks然后再通过1×1的卷积层（图中的Transition）。接着将两个分支的信息在通道方向进行Concat拼接， 最后通过1×1的卷积层进一步融合（图中的Transition）。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/cspnd.jpg" alt="cspnd"></p><p>接下来详细分析CSPDarknet53网络的结构， 图中</p><p>k表示卷积核的大小， s代表步距， c代表通过该模块输出的特征层channels， 注意CSPDarknet53 Backbone中所有的激活函数都是Mish激活函数。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov4_2.jpg" alt="yolov4_2"></p><h4 id="yolov4网络结构"><a href="#yolov4网络结构" class="headerlink" title="yolov4网络结构"></a>yolov4网络结构</h4><p>如下图为yolov4网络结构</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov4_3-16999321127172.jpg" alt="yolov4_3"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h4&gt;&lt;p&gt;论文原文：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;[a</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Grad-CAM</title>
    <link href="https://guudman.github.io/2023/11/12/Grad-CAM/"/>
    <id>https://guudman.github.io/2023/11/12/Grad-CAM/</id>
    <published>2023-11-12T07:23:48.000Z</published>
    <updated>2023-11-13T02:21:38.960Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>论文原文： <a href="https://arxiv.org/pdf/1610.02391.pdf">Grad_CAM</a></p><p>对于常用的深度学习网络（例如CNN， 普遍认为是个黑盒， 可解释性并不强）， 它为什么会这么预测， 它的关注点在哪里， 我们并不知道， 很多科研人员想方设法的探索其内在的联系， 也有很多相关的论文。 这篇Grad-CAM并不是最新的论文， 但是很有参考意义。通过Grad-CAM我们能够绘制出如下的热力图（对于给定类别， 网络到底关注哪些区域）。Grad-CAM（Gradient-weighted Classes Activation Mapping）是CAM（Class Activation Mapping）的升级版， Grad-CAM比CAM更具一般性。但CAM比较致命的问题是需要修改网络结构并重新训练， 而Grad-CAM完美避开了这些问题， 本文不对CAM进行讲解。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231112105641758.png" alt="image-20231112105641758"></p><p>Grad-CAM能够帮助我们分析网络对于某个类别的关注区域， 那么我们通过网络关注的区域能够反过来分析网络是否正确学习到正确的特征或者信息。例如， 作者训练了一个二分类网络， Nurse和Doctor， 如下图， 第一列是预测时输入的原图， 第二列是Biased model（具有偏见的模型）通过Grad-CAM绘制的热力图。 第三列是Unbiased model（不具偏见的模型）通过Grad-CAM绘制的热力图。通过对比发现， Biased model对于Nurse（护士）这个类别关注的是人的性别，可能模型认为Nurse都是女性，很明显这是带有偏见的。比如第二行第二列这个图， 明明是个女doctor， 但bias modek却认为她是Nurse（可能因为模型关注到它是女性）。而Unbiased model关注的是Nurse和Doctor使用的工作器具以及服装， 明显这个更合理。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231112110347160.png" alt="image-20231112110347160"></p><hr><h4 id="Grad-CAM介绍以及实验"><a href="#Grad-CAM介绍以及实验" class="headerlink" title="Grad-CAM介绍以及实验"></a>Grad-CAM介绍以及实验</h4><h5 id="理论介绍"><a href="#理论介绍" class="headerlink" title="理论介绍"></a>理论介绍</h5><p>作者的想法还是比较简单， 参见下图， 我们这里简单看一下Image Classification任务， 首先网络进行正向传播， 得到特征层A（一般指的是最后一个卷积层的输出）和网络预测值y（注意， 这里指的是softmax激活之前的数值）。假设我们想看一下网络针对Tiger Cat这个类别的感兴趣区域， 假设网络针对Tiger Cat类别预测的值为<script type="math/tex">y^c</script>。紧接着对<script type="math/tex">y^c</script>进行反向传播， 能够得到反向传播特征层A的梯度信息<script type="math/tex">A'</script>。通过计算得到针对特征层A每个通道额重要程度， 然后进行加权求和通过ReLU就行了， 最终得到的结果即是Grad-CAM。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231112111240261.png" alt="image-20231112111240261"></p><p>至于为什么这么做， 首先得到的特征层A是网络对原图进行特征提取得到的结果， 越往后的特征层抽象程度越高， 语义信息越丰富，而且利用CNN抽取得到的特能图是能够保留空间信息的（Transformer同样）。所以Grad-CAM在CNN中的A一般都指的是最后一个卷积层的输出（参考下图， 越往后的特征层效果越好）。当然特征层A包含了所有我们感兴趣的语义信息， 但具体哪些语义信息对应哪个我们并不清楚。紧接着通过对类别c的预测值<script type="math/tex">y^c</script>进行反向传播， 得到反传回特征层A的梯度信息A’, 那么A’就是<script type="math/tex">y^c</script>对A求得的偏导， 换句话说， A’代表A中每个元素对<script type="math/tex">y^c</script>的贡献， 贡献越大网络就认为越重要。然后对A’在w， h上求均值就能得到针对A每个通道的重要程度（这里针对类别c而言的）。最后进行简单的加权求和再通过ReLU就能得到文中说的Grad-CAM。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231112112738557.png" alt="image-20231112112738557"></p><p>关于Grad-CAM总结下来就是下面这个公式：</p><script type="math/tex; mode=display">L^c{Grad-CAM}=ReLU(\sum_{k}\alpha^c_kA^k)</script><p>A代表某个特征层， 论文中一般指的是最后一个卷积层输出的特征层</p><p>k代表特征层A中第k个通道（channel）</p><p>c代表类别c</p><script type="math/tex; mode=display">A^k$$代表特征层A中通道k的数据$$\alpha^c_k$$代表针对$$A^k$$的权重。 关于$$\alpha ^c_k</script><script type="math/tex; mode=display">\alpha ^c_k = \frac{1}{Z}\sum_i\sum_{j}\frac{\alpha y^c}{\alpha A^k_{ij}}</script><script type="math/tex; mode=display">y^c$$代表网络针对类别c预测的分数， 注意这里没有经过softmax激活$$A^k_{ij}$$代表特征层A在通道k中坐标ij位置处的数据Z代表特征层的高度×宽度通过上面的公式可知$$\alpha^c$$ 就是通过预测类别c的预测分数$$y^c$$进行反向传播， 然后利用反传到特征层A上的梯度信息计算特征层A每个通道k的重要程度， 然后通过$$\alpha$$对特征A每个通道的数据进行加权求和， 最后通过ReLU激活函数得到Grad-CAM（论文中说ReLU是为了过滤掉Negative pixls， 而Negative pixles很可能归属于其他类别的pixels）。当然一般还要通过一些后处理，插值等方法与原图叠加得到最终的可视化结果。 如下图所示， CNN Extractor代表CNN 特征提取器， GAP代码Global Average Pooling, FC代表全连接层。 ![Grad-cam1](https://gitee.com/guudman/blog_images/raw/master/Grad-cam1.jpg)假设网络正向传播得到的特征层A如图所示（这里为了方便只画了两个channel， 数据都是随机写的）， 针对类别cat的预测值进行反向传播得到针对特征层A的梯度信息A'，接着利用上面的公式（2）计算针对特征层A每个通道的权重， 就是求A'每个通道的均值。</script><p>\alpha ^c_k = \frac{1}{Z}\sum_i\sum_{j}\frac{\alpha y^c}{\alpha A^k_{ij}}</p><script type="math/tex; mode=display">那么有：</script><p>\alpha^{Cat} =\left(<br>\begin{matrix}<br>  \alpha_1^{Cat} \\<br>   \alpha_2^{Cat}<br> \end{matrix}<br> \right) = \left(<br>\begin{matrix}<br>  \frac{1}{3} \\<br>   -\frac{2}{3}<br> \end{matrix}<br> \right)</p><script type="math/tex; mode=display">然后我们再带人公式</script><p>L^c_{Grad-CAM} = ReLU(\sum_k \alpha^c_{k}A^k)</p><script type="math/tex; mode=display">得到对应类别cat的Grad-CAM</script><p>L^{Cat}_{Grad-CAM} = ReLU(\frac{1}{3}\cdot \left(<br>\begin{matrix}<br> 1 &amp; 0&amp;2\\<br> 3 &amp; 5&amp;0\\<br> 1 &amp; 1&amp;1<br> \end{matrix}<br> \right)) +</p><script type="math/tex; mode=display"></script><p> (-\frac{2}{3})\cdot \left(<br>\begin{matrix}<br> 0 &amp; 1&amp;0\\<br> 3 &amp; 1&amp;0\\<br> 1 &amp; 0&amp;1<br> \end{matrix}<br> \right)) \\ </p><script type="math/tex; mode=display"></script><p> =ReLU(\left(<br>\begin{matrix}<br> \frac{1}{3} &amp; -\frac{2}{3}&amp; \frac{2}{3}\\<br> -1 &amp; 1&amp;0\\<br>  -\frac{1}{3} &amp; \frac{1}{3}&amp; -\frac{1}{3}<br> \end{matrix}<br> \right)))</p><script type="math/tex; mode=display">##### 梯度计算实例上面在计算Grad-CAM时， 其实主要是计算正向传播过程中得到的特征层A和反向传播得到的A', 得到特征层A很简单， 大家也经常会提取某个特征层进行分析或特征融合等。但获取A'会相对麻烦点， 计算倒不是难点因为常用的深度学习框架会自动帮我们计算， 只是很少会用到反传的信息。 那么A’究竟是怎么去计算的。下面构建一个非常简单的神经网络， 重要结构是一个卷积层+一个全连接层， 通过这个例子来演示如何计算反向传播过程中某个特征层的梯度。 ![Grad-cam2](https://gitee.com/guudman/blog_images/raw/master/Grad-cam2.jpg)根据上图， 可得output第一个元素的计算公式如下：</script><p>y_1 = f_{fc}(f_{conv2d}(X,W_1),W^1_2)</p><script type="math/tex; mode=display">其中X代表输入(input), $$f_{conv2d}$$表示卷积的计算过程， $$f_{fc}$$表示全连接的计算， $$W_1$$代表卷积层对应的权重（为了方便， 不考虑偏置）， $$W^1_2$$代表全连接层中第一个节点对应的权重。这里先令$$f_{conv2d}$$即卷积层输出的结果为$$O=(O_{11}, O_{12}, O_{21}, O_{22})$$（为方便后续计算， 这里直接展平写成向量形式）分别对应图中的$$(4, 7, 5, 6)^T$$,这里的O是向量， 那么y1的计算公式为：</script><p>y_1 = f_{fc}(O, W_2^1) = O_{11}\cdot W^{11}_2 +  O_{12}\cdot W^{12}_2 +  O_{21}\cdot W^{13}_2 +  O_{22}\cdot W^{14}_2</p><script type="math/tex; mode=display">接着对O进行偏导：</script><p>\frac{\partial y_1}{\partial O} = \frac{\partial y_1}{\partial(O_{11}, O_{12}, O_{21}, O_{22})^T}</p><script type="math/tex; mode=display"></script><p> =(0, 1, 0, 1)^T</p><script type="math/tex; mode=display">接着将上面的结果reshape得到</script><p>\left(<br>\begin{matrix}<br> 0 &amp; 1 \\<br> 0 &amp; 1<br> \end{matrix}<br> \right)</p><p>$$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;论文原文： &lt;a href=&quot;h</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>ConvNeXt</title>
    <link href="https://guudman.github.io/2023/11/12/ConvNeXt/"/>
    <id>https://guudman.github.io/2023/11/12/ConvNeXt/</id>
    <published>2023-11-12T01:02:56.000Z</published>
    <updated>2023-11-12T02:17:59.546Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>论文原文： <a href="[arxiv.org/pdf/2201.03545.pdf](https://arxiv.org/pdf/2201.03545.pdf">ConvNeXt</a>)</p><p>自从Vit（Vision Transformer)在CV领域大放异彩， 越来越多的研究人员开始涌入Transformer中， 2021年在CV领域的文章绝大多数都是基于Transformer的， 比如2021年的ICCV的best paper （Swin transformer）， 而卷积神经网络已经开始慢慢淡出舞台中央。 卷积网络要被Transformer取代 了吗， 也许会在不久的将来。2022年一份月， Facebook AI Research和UC Berkeley一起发表了一篇文章A ConvNet for the 2020s, 在文章中提出了ConVNeXt纯卷积神经网络， 它对标的就是2021年非常火的Swin Transformer， 通过一系列实验对比， 在相同的FLOPs下， ConvNeXt相比Swin Transformer拥有更快的推理速度以及更高的准确率， 在ImageNet 22k上ConvNeXt-XL达到了87.8%的准确率。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110171124691.png" alt="image-20231110171124691"></p><p>仔细阅读这篇文章， 你会发现ConvNeXt使用的都是现有的结构和方法， 无任何结构或者方法上的创新，而且源码也非常精简， 100多行代码就能搭建完成。 相比Swin Transformer简直不要太简单。 之前看Swin transformer时，滑动窗口， 相对位置索引， 不光原理理解起来很吃力，源码也让人绝望（但不可否认Swin Transformer的成功以及巧妙的设计思想）。为什么基于Transformers结构的模型比卷积神经网络要好呢， 论文中的作者认为可鞥就是随着技术的不断发展， 各种新的架构以及优化策略促使Transformer模型的效果更好， 那么使用相同的策略去训练卷积神经网络也能达到相同的效果吗？</p><blockquote><p>In this work, we investigate the architectural distinctions between ConvNets and Transformers and try to identify the confounding variables when comparing the network performance. Our research is intended to bridge the gap between the pre-ViT and post-ViT eras for ConvNets, as well as to test the limits of what a pure ConvNet can achieve.</p></blockquote><h4 id="设计方案"><a href="#设计方案" class="headerlink" title="设计方案"></a>设计方案</h4><p>作者首先利用训练网络Vision Transformers的策略去训练原始的ResNet50模型， 发现比原始效果要好很多， 并将此结果作为后续实验额的基准baseline， 接下来的实验包含如下部分。</p><p>macro design</p><p>ResnetXt</p><p>inverted bottleneck</p><p>large kernel size</p><p>variout layer-wise micro designs</p><blockquote><p>Our starting point is a ResNet-50 model. We first train it with similar training techniques used to train vision Transformers and obtain much improved results compared to the original ResNet-50. This will be our baseline. We then study a series of design decisions which we summarized as 1) macro design, 2) ResNeXt, 3) inverted bottleneck, 4) large kernel size, and 5) various layer-wise micro designs.</p></blockquote><p>下图展现了每个方案对最终结果的影响。 很显然最后得到的ConNetX在相同FLOPs下准确率已经超过了Swin Transformer。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110172201944.png" alt="image-20231110172201944"></p><hr><h4 id="Macro-design"><a href="#Macro-design" class="headerlink" title="Macro design"></a>Macro design</h4><p>在这个部分主要研究两个方面</p><p>Changing stage compute ratio. 在原ResNet网络中， 一般conv4_x（即stage3）堆叠的block的次数是最多的， 如下图中的ResNet50中stage1到stage4堆叠block的次数是（3， 4， 6， 3）比例大概是1:1:2:1，但在Swin Transformer中， 比如Swin-T的比例是1:1:3:1， Swin-L的比例是1:1:9:1。很明显， 在Swin Transformer中， stage3堆叠block的占比更高。所以作者将ResNet50中的堆叠次数由(3, 4, 6, 3)调整成（3， 3， 9，3）,和Swin-T拥有相似的FLOPs。进行调整后， 准确率78.8%提升到了79.4%。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110174426233.png" alt="image-20231110174426233"></p><p>Changing stem to “Patchify”， 在之前的卷积神经网络中， 一般最初始的下采样模块stem一般都是通过一个卷积核大小为7×7步距为2的卷积层以及一个步距为2的最大池化下采样层共同组成。 高和宽都是下采样的4倍。 但在Transformer模型中一般都是通过一个卷积核非常大而且相邻窗口之间没有重叠的（即stride 等于kernel_size)卷积层进行下采样。比如在Swin transformer中采用的是一个卷积核大小为4×4步距为4的卷积层构成patchify, 同样是下采样4倍。所以作者将ResNet中的stem也换成了和Swin Transformer一样的patchify， 替换后准确率从79.4%提升到79.5%， 并且FLOPs也降低了一点。 </p><hr><h4 id="ResNeXt-ify"><a href="#ResNeXt-ify" class="headerlink" title="ResNeXt-ify"></a>ResNeXt-ify</h4><p>接下来作者借鉴了ResNeXt中的组卷积grouped convolution, 因为ResNeXt相比普通的ResNet而言在FLOPs以及accuracy之间做到了更好的平衡。 而作者采用的是更激进的depthwise convolution. 即group数和通道数channel相同。 之前在讲MobileNet时讲解过这个。这样做的另外一个原因是作者认为depthwise convolution和self-attention中的加权求和操作很相似。 </p><blockquote><p>We note that depthwise convolution is similar to the weighted sum operation in self-attention</p></blockquote><p>接着作者将最初的通道数由64调整成96和Swin Transformer保持一致， 最终准确率达到了80.5%。</p><hr><h4 id="Inverted-Bottleneck"><a href="#Inverted-Bottleneck" class="headerlink" title="Inverted Bottleneck"></a>Inverted Bottleneck</h4><p>作者认为Transformer block中的MLP模块非常像MobileNetV2中的Inverted Bottleneck模块， 即两头细中间粗。下图a是ResNet中采用的Bottleneck模块， b是MobileNetV2采用的Inverted Bottleneck模块。c是ConvNeXt采用的是Inverted Bottleneck模块。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110175852997.png" alt="image-20231110175852997"></p><p>作者采用Inverted Bottleneck模块后， 在较小的模型上准确率由80.5%提升到了80.6%， 在较大的模型上准确率由81.9%提升到了82.6%。</p><hr><h4 id="Larget-Kernel-Sizes"><a href="#Larget-Kernel-Sizes" class="headerlink" title="Larget Kernel Sizes"></a>Larget Kernel Sizes</h4><p>在Transformer中一般都是对全局做self-attention， 比如Vision Transformer， 即使是Swin Transformer也有7×7大小的窗口。但现在主流的卷积神经网络都是采用3×3大小的窗口， 因为之前VGG论文中说通过堆叠多个3×3的窗口可以替代一个更大的窗口。而且现在的GPU设备针对3×3大小的卷积核做了很多的优化， 所以会更高效， 接着作者做了如下两个改动：</p><p>Moving up depthwise conv layer, 即将depthwise conv模块上移， 原来是1×1 conv-&gt;depthwise conv-&gt;1x1 conv， 现在变成了depthwise conv-&gt;1×1 conv-&gt;1x1 conv，这么做是因为在Transformer中， MAS模块是放在MLP模块之前的， 所以这里进行效仿， 将depthwise conv上移， 这里改动后， 准确率下降到了79.9%, 同时FLOPs也减少了。</p><p>Increasing the kernel size, 接着作者将depthwise conv的卷积核大小由3×3改成了7×7（和swin Transformers一样）， 当然作者也尝试了其他尺寸， 包括3, 5, 7, 9, 11发现取到7时准确率达到了饱和， 并且准确率从79.9%增长到80.6%(7×7)。</p><hr><p>Micro Design</p><p>接下来作者再聚集到更细小的差异， 比如激活函数以及Normalization.</p><p>Replacing ReLU with GELU, 在Transformer中激活函数基本用的都是GELU， 而在卷积神经网络中最常用的是ReLU， 于是作者又将激活函数替换成了GELU， 替换后发现准确率没发生变化。 </p><p>Fewer activation functions, 使用更少的激活函数， 在卷积神经网络中， 一般会在每个卷积层或全连接层后接上一个激活函数， 但在Transformer中并不是每个模块后都跟有激活函数， 比如MLP中只有第一个全连接层后跟了GELU激活函数， 接着作者再ConvNeXt中也减少了激活函数的使用， 如下图所示， 减少后发现准确率增长到了81.3%。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/SwinTransformerBlock.jpg" alt="SwinTransformerBlock"></p><p>Fewer normalization layers: 使用更少的Normalization. 同样在Transformer中， Normalization使用的也比较少， 作者也减少了ConvNeXt Block中的Normalization层， 只保留了depthwise conv后的Normalization层， 此时准确率达到了81.4%， 已经超过了Swin-T。</p><p>Substituting BN with LN, 将BN替换成LN。Batch Normalization（BN）在卷积神经网络中是非常常用的操作了，它可以加速网络的收敛并减少过拟合（但用的不好也是个大坑）。但在Transformer中基本都用的Layer Normalization（LN），因为最开始Transformer是应用在NLP领域的，BN又不适用于NLP相关任务。接着作者将BN全部替换成了LN，发现准确率还有小幅提升达到了81.5%。</p><p>Separate downsample layers. 单独的下采样层。在ResNet网络中stage2-stage4的下采样都是通过将主分支上3x3的卷积层步距设置成2，捷径分支上1x1的卷积层步距设置成2进行下采样的。但在Swin Transformer中是通过一个单独的Patch Merging实现的。接着作者就为ConvNext网络单独使用了一个下采样层，就是通过一个Laryer Normalization加上一个卷积核大小为2步距为2的卷积层构成。更改后准确率就提升到了82.0%。</p><hr><p>对于ConvNeXt网络， 作者提出了T/S/B/L四个版本， 计算复杂度刚好和Swin Transformer中的T/S/B/L相似。 </p><p>这四个版本的配置如下：</p><blockquote><p>ConvNeXt-T: C = (96, 192, 384, 768), B = (3, 3, 9, 3)<br>ConvNeXt-S: C = (96, 192, 384, 768), B = (3, 3, 27, 3)<br>ConvNeXt-B: C = (128, 256, 512, 1024), B = (3, 3, 27, 3)<br>ConvNeXt-L: C = (192, 384, 768, 1536), B = (3, 3, 27, 3)<br>ConvNeXt-XL: C = (256, 512, 1024, 2048), B = (3, 3, 27, 3)</p></blockquote><p>其中C代表4个stage中输入的通道数， B代表每个stage重复堆叠block的次数。</p><hr><h4 id="ConvNeXt-T结构图"><a href="#ConvNeXt-T结构图" class="headerlink" title="ConvNeXt-T结构图"></a>ConvNeXt-T结构图</h4><p>下图是根据源码绘制的网络结构图， 会发现ConvNeXt Block中还有一个Layer Scale操作， 其实它就是将输入的特征乘以上一个可训练的参数， 该参数就是一个向量， 元素个数与特征层channel相同， 即对每个channel的数据进行缩放。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/ConvNeXt.jpg" alt="ConvNeXt"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;论文原文： &lt;a href=&quot;[</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>DeepLab_V3</title>
    <link href="https://guudman.github.io/2023/11/10/DeepLab-V3/"/>
    <id>https://guudman.github.io/2023/11/10/DeepLab-V3/</id>
    <published>2023-11-10T10:11:19.000Z</published>
    <updated>2023-11-10T10:15:53.286Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>论文原文： <a href="[arxiv.org/pdf/1706.05587.pdf](https://arxiv.org/pdf/1706.05587.pdf">论文原文</a>)</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110092047747.png" alt="image-20231110092047747"></p><p>DeepLab V3是2017年发表在CVPR上的文章， 与DeepLab V2相比， 个人感觉有如下的三种变化：1）引入Multi-grid， 2）改进ASPP结构， 3）把CRFs后处理给移除掉了。</p><hr><h4 id="DeepLabV3两种模型结构"><a href="#DeepLabV3两种模型结构" class="headerlink" title="DeepLabV3两种模型结构"></a>DeepLabV3两种模型结构</h4><p>文章中, 穿插讲解两种模型的实验, 这两种模型分别是<font color="red">cascded model</font>和<font color="red">ASPP model</font>。在cascded model中没有使用ASPP模块， 在ASPP model中没有使用cascaded blocks模块。<strong>注意， 虽然文中提出了两种结构， 但作者说ASPP model比cascaded model略好点， 包括在Github上开源的一些代码， 大部分也是用的ASPP model。</strong></p><blockquote><p>Both our best cascaded model (in Tab. 4) and ASPP model in（Tab. 6) (in both cases without Dense CRF post-processing or MS-COCO pre-training) already outperform DeepLabv2.</p></blockquote><hr><h4 id="cascaded-model"><a href="#cascaded-model" class="headerlink" title="cascaded model"></a>cascaded model</h4><p>文中， 大部分的实验都是围绕cascaded model做的。 如下图所示， 论文中提出的cascaded model指的是图(b)， 其中Block1, Block2, Block3, Block4是原始ResNet网络中的层结构， 但Block4中将第一个残差结构里的3×3卷积层以及捷径分支上的1×1卷积层步距stride由2改为1（即不再进行下采样），并且所有的残差结构里3×3的普通卷积层都换成了膨胀卷积层。Block5, Block6和Block7是额外新增的层结构， 它们的结构和Block4是一模一样的， 即由三个残差结构组成。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110093801711.png" alt="image-20231110093801711"></p><p>原论文中说在训练cascaded model时output_stride=16（即特征层相对输入图片的下采样率），但验证时使用的output_stride=8， 原因估计是把Block3中的下采样取消了。 因为output_stride=16时最终得到的特征层H和W会更小， 这意味着可以设置更大的batch_size并且能够加快训练速度。但特征层H和W变小导致特征层丢失细节信息（文章中说变得更“粗糙“）， 所以在验证时采用的是output_stride=8。其实只要设备的GPU显存足够大， 算力足够强也可以直接把output_stride设置成8。</p><blockquote><p>Also note that training with output stride = 16 is several times faster than output stride = 8 since the intermediate   feature maps are spatially four times smaller, but at a sacrifice of accuracy since output stride = 16 provides coarser feature maps.</p></blockquote><p><strong>另外需要注意的是，图中标注的rate并不是膨胀卷积真正采用的膨胀系数</strong>。真正采用的膨胀系数应该是图中的rate乘上Multi-Grid参数， 比如Block4中的rate=2， Multi-Grid=(1, 2, 4)， 那么真正采用的膨胀系数是2×（1， 2， 4）=（2， 4， 8）。关于Multi-Grid参数后面会提到。 </p><blockquote><p>The final atrous rate for the convolutional layer is equal to the multiplication of the unit rate and the corresponding rate, For example, when output stride = 16 and Multi Grid = (1, 2, 4), the three convolutions will have rates=2 ×（1， 2， 4）= （2， 4， 8） in block4, respectively.</p></blockquote><p>虽然论文大篇幅的内容都在讲cascaed model以及对应的实验， 但实际使用的最多的还是ASPP model, ASPP model结构如下图所示。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110095336130.png" alt="image-20231110095336130"></p><p><strong>注意</strong>， 和cascaded model一样， 原论文中说在训练时output_stride=16（即特征层相对输入图片的下采样率）， 但验证时使用的output_stride=8, pytorch官方实现的DeepLabV3源码中就直接把output_stride设置成8进行进行训练的。 </p><p>接下来分析DeepLab V3中ASPP 结构。 首先回顾一下DeepLab V2中的ASPP结构， DeepLab V2中的ASPP结构其实就是通过4个并行的膨胀卷积， 每个分支上的膨胀卷积层所采用的膨胀系数不同（注意，这里的膨胀卷积层后没有跟BatchNorm并且使用了偏置bias）。 接着通过add相加的方式融合成4个分支的输出。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110104235405.png" alt="image-20231110104235405"></p><p>接着再来看一下DeepLab V3中的ASPP结构， 这里的ASPP结构有5个分支， 分别是1×1的卷积层， 三个3×3的膨胀卷积，以及一个全局平均池化层（后面还跟有一个1×1的卷积层， 然后通过双线性插值的方法还原回到输入的W和H）。关于最后一个全局池化分支文中说是为了增加一个全局上下文信息global context information。 然后通过Concat的方式将这5个分支的输出进行拼接（沿着channels方向）， 最后通过一个1×1的卷积层进一步融合信息。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/DeepLab_V3_aspp.jpg" alt="DeepLab_V3_aspp"></p><p>论文中的ASPP结构介绍， 可以看下面这段话。 </p><blockquote><p>Specifically, we apply global average pooling on the last feature map of the model, feed the resulting image-level features to a 1×1 convolution with 256 filters （and batch normalization）, and then bilinearly upsample the feature to the desired spatial dimension. In the end, our improved ASPP consists of (a) one 1×1 convolution and three 3×3 convolitions with rates = (6, 12, 18) when output stride = 16 (all with 2256 filters and batch normalization), and (b) the image-level features, as shown in Fig.5. Note that the rates are doubled when output stride=8. The resulting features from all the branches are then concatenated and pass through another 1×1 convolution (also with 256 filters and batch normalization) before the final 1×1 convolution which generates the final logits. </p></blockquote><hr><h4 id="Multi-grid"><a href="#Multi-grid" class="headerlink" title="Multi-grid"></a>Multi-grid</h4><p>在之前的DeepLab模型中虽然一直在使用膨胀卷积， 但设置的膨胀系数都比较随意。 在DeepLab V3中作者有去做一些相关实验看如何设置的更合理。 下表是以cascaded model (ResNat101作为backbone为例)为实验对象， 研究采用不同数量的cascaded blocks模型以及cascaded blocks采用不同的Multi-Grid参数的效果。 <strong>注意</strong>，刚刚在讲cascaded model时有提到，blocks中真正采用的膨胀系数应该是图中的rate乘上这里的Multi-Grid参数。通过实验发现，当采用三个额外的Block时（即额外添加Block5，Block6和Block7）将Multi-Grid设置成(1, 2, 1)效果最好。另外如果不添加任何额外Block（即没有Block5，Block6和Block7）将Multi-Grid设置成(1, 2, 4)效果最好，因为在ASPP model中是没有额外添加Block层的，后面讲ASPP model的消融实验时采用的就是Multi-Grid等于(1, 2, 4)的情况。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110112312143.png" alt="image-20231110112312143"></p><hr><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><h5 id="cascaded-model消融实验"><a href="#cascaded-model消融实验" class="headerlink" title="cascaded model消融实验"></a>cascaded model消融实验</h5><p>下表有关cascaded model的消融实验， 其中：</p><p>MG代表Multi-Grid， 刚刚上面也有说在cascaded model中采用MG(1, 2, 1)是最好的。 </p><p>OS表示output_stride， 刚刚也在上面有提到过验证时将output_stride设置为8效果会更好</p><p>MS表示多尺度， 和DeepLabV2中类似， 不过在DeepLab V3中采用的多尺度更多scales={0.5, 0.75, 1.0, 1.25, 1.5, 1.75}</p><p>Flip代表增加一个水平翻转后的图像输入。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110112818613.png" alt="image-20231110112818613"></p><h5 id="ASPP-model消融实验"><a href="#ASPP-model消融实验" class="headerlink" title="ASPP model消融实验"></a>ASPP model消融实验</h5><p>下表有关ASPP model的消融实验， 其中：</p><p>MG表示Multi-Grid， 刚刚也在上面有说在ASPP model中采用MG(1, 2, 4)是最好的。</p><p>Image Pooling代表在ASPP中加入全局平均池化层分支。</p><p>OS代表output_stride, 刚刚在上面也有提到将output_stride设置为8效果更好。</p><p>MS代表多尺度， 和DeepLabV2中类似， 不过在DeepLab V3中采用的尺度更多scales={0.5, 0.75, 1.0, 1.25, 1.75}</p><p>Flip代表增加一个水平翻转后的图像输入</p><p>COCO代表在COCO数据集上进行预训练。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110113258448.png" alt="image-20231110113258448"></p><hr><h4 id="Pytorch官方中DeepLab-V3模型结构"><a href="#Pytorch官方中DeepLab-V3模型结构" class="headerlink" title="Pytorch官方中DeepLab V3模型结构"></a>Pytorch官方中DeepLab V3模型结构</h4><p>在pytorch官方实现的DeepLab V3中： 1）并没有使用Multi-Grid；2）多了一个FCNHead辅助训练分支， 可以选择不使用；3）无论是训练还是验证output_size都设置为8；4）ASPP中三个膨胀卷积分支的膨胀系数是12， 24， 36</p><blockquote><p>In the end, out improved ASPP consists of (a) one 1×1 convolution and three 3×3 convolutions with rates=(6, 12, 18) when output stride=16 (all with 256 filters and batch normalization), and (b) the image-level features , as shown in Fig.5. Note that the rates are doubled when output stride=8.</p></blockquote><p><img src="https://gitee.com/guudman/blog_images/raw/master/DeepLab_V3_pytorch.jpg" alt="DeepLab_V3_pytorch"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;论文原文： &lt;a href=&quot;[</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>DeepLab_V2</title>
    <link href="https://guudman.github.io/2023/11/09/DeepLab-V2/"/>
    <id>https://guudman.github.io/2023/11/09/DeepLab-V2/</id>
    <published>2023-11-09T11:34:38.000Z</published>
    <updated>2023-11-10T04:26:40.990Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>论文原文：<a href="[1606.00915.pdf (arxiv.org">论文原文</a>](<a href="https://arxiv.org/pdf/1606.00915.pdf">https://arxiv.org/pdf/1606.00915.pdf</a>))</p><p>这是一篇2016年发表在CVPR上的文章， 其实对比模型结构图， 发现DeepLab v1与Deep Lab v2相比，其实就是换了一个backbone（VGG-&gt;ResNet， 换个backbone大概能涨3个点）， 然后引入了一个新的模块ASPP（Atros Spatial Pyramid Pooling）， 其他的没有太大区别。</p><hr><h4 id="DCNNs应用在语义分割任务中的问题"><a href="#DCNNs应用在语义分割任务中的问题" class="headerlink" title="DCNNs应用在语义分割任务中的问题"></a>DCNNs应用在语义分割任务中的问题</h4><p>和上一篇文章一样， 在文章的引言部分作者提出了DCNNs应用在语义分割任务中遇到的问题。 </p><p>分辨率被降低（主要由于下采样stride&gt;1的层导致）</p><p>目标的多尺度问题</p><p>DCNNs的不变性（invariance)会降低定位精度</p><hr><h4 id="文中对应的解决方法"><a href="#文中对应的解决方法" class="headerlink" title="文中对应的解决方法"></a>文中对应的解决方法</h4><p>针对分辨率被降低的问题， 一般就是将最后的几个Maxpooling层的stride设置成1（如果是通过卷积下采样的， 比如resnet， 同样将stride设置成1即可)， 然后再配合使用膨胀卷积。 </p><blockquote><p>In order to overcome this hurdle and efficiently produce denser feature maps, we remove the downsampling operator from the last few max pooling layers of DCNNs and instead umsample the filters in subsequent convolutional layers, resuliting in feature maps computed at a higher sampling rate, Filter upsampling amounts to inserting holes between nonzero filter taps</p></blockquote><p>针对目标多尺度问题， 最容易想到的就是将图像缩放到多个尺寸分别通过网络进行推理， 最后将多个结果进行融合即可， 这样做虽然有用但是计算量太大了， 为了解决这个问题， DeepLab V2中提出了ASPP模块（astrous spatial pyramid pooling).</p><blockquote><p>A standard way to deal with this is to present to the DCNN rescaled versions of the same image and then aggregate the feature or score maps. We show that this approach indeed increases the performance of our system, but comes at the cost of computing feature responses at all DCNN layers for multiple scaled versions of the input image. Instead, motivated by spatial pyramid pooling, we propose a computationally efficient scheme of resampling a given feature layer at multiple rates prior to convolution. This amounts to probing the original image with multiple filters that have complementary effective fields of view, thus capturing objects as well as useful image context at multiple scales. Rather than actually resampling features, we efficiently implement this mapping using multiple parallel atrous convolutional layers with different sampling rates; we call the proposed technique “atrous spatial pyramid pooling” (ASPP).</p></blockquote><p>针对DCNNs不变性导致定位精度降低的问题， 和DeepLab V1差不多还是通过CRFs解决， 不过这里用的是fully connected pairwise CRF, 相比V1里的fully connected CRF要更高效。在DeepLab v2中CRF涨点就没有DeepLab v1猛了， 在DeepLab V1中大概能提升4个点， 在DeepLab V2中通过Table4可以看到大概只能提升1个多点了。 </p><blockquote><p>Our work explores an alternative approach which we show to be highly effective. In particular, we boost our model’s ability to capture fine details by employing a fully-connected Conditional Random Field (CRF) [22]. CRFs have been broadly used in semantic segmentation to combine class scores computed by multi-way classifiers with the low-level information captured by the local interactions of pixels and edges [23], [24] or superpixels [25]. Even though works of increased sophistication have been proposed to model the hierarchical dependency [26], [27], [28] and/or high-order dependencies of segments [29], [30], [31], [32], [33], we use the fully connected pairwise CRF proposed by [22] for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies.</p></blockquote><hr><h4 id="DeepLab-V2的优势"><a href="#DeepLab-V2的优势" class="headerlink" title="DeepLab V2的优势"></a>DeepLab V2的优势</h4><p>和DeepLab V1中写的一样：</p><p>速度更快</p><p>准确率更高（当时的state-of-the-art)</p><p>模型结构简单， 还是DCNN和CRFs联级</p><blockquote><p>From a practical standpoint, the three main advantages of our DeepLab system are: (1) Speed: by virtue of atrous convolution, our dense DCNN operates at 8 FPS on an NVidia Titan X GPU, while Mean Field Inference for the fully-connected CRF requires 0.5 secs on a CPU. (2) Accuracy: we obtain state-of-art results on several challenging datasets, including the PASCAL VOC 2012 semantic segmentation benchmark [34], PASCAL-Context [35], PASCAL-Person-Part [36], and Cityscapes [37]. (3) Simplicity: our system is composed of a cascade of two very well-established modules, DCNNs and CRFs.</p></blockquote><hr><h4 id="ASPP-astrous-spatial-pyramid-pooling"><a href="#ASPP-astrous-spatial-pyramid-pooling" class="headerlink" title="ASPP(astrous spatial pyramid pooling)"></a>ASPP(astrous spatial pyramid pooling)</h4><p>感觉这个ASPP就是DeepLab V1中LargeFOV的升级版（加了多尺度的特性）。下图是原论文中介绍ASPP的示意图， 就是在backbone输出的Feature Map上并联4个分支， 每个分支的第一层都是使用的膨胀卷积， 但不同的分支使用的膨胀卷积系数不同（即每个分支的感受野不同， 从而具有解决目标多尺度的问题）。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231109165134383.png" alt="image-20231109165134383"></p><p>下图中有更加详细的ASPP结构（<strong>这里针对VGG网络为例</strong>）， 将Pool5输出的特征层（这里以VGG为例）并联4个分支，每个分支分别通过一个3×3的膨胀卷积层， 1×1的卷积层， 1×1的卷积层（卷积核的个数等于num_classes)。最后将四个分支的结果进行add融合即可。<strong>如果以ResNet101作为Backbone的话，每个分支只有一个3×3的膨胀卷积层， 卷积核的个数等于num_classes</strong>)</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231109165540154.png" alt="image-20231109165540154"></p><p>在论文中有给出两个ASPP的配置， ASPP-S（四个分支膨胀系数分别为2， 4， 8， 12）和ASPP-L（四个分支膨胀系数分别为6， 12， 18， 24）, 下表是对比LargeFOV, ASPP-S以及ASPP-L的效果， 这里只看了CRF之前的（before CRF)对比， ASPP-L优于ASPP-S优于LargeFOV</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231109165908278.png" alt="image-20231109165908278"></p><hr><h4 id="DeepLab-V2网络结构"><a href="#DeepLab-V2网络结构" class="headerlink" title="DeepLab V2网络结构"></a>DeepLab V2网络结构</h4><p>这里以ResNet101作为backbone为例， 下图是根据官方源码绘制的网络结构（<strong>这里不考虑MSC即多尺度</strong>）。在ResNet的Layer3中的Bottleneck1中原本是需要下采样的（3×3的卷积层的stride=2）， 但在DeepLab v2中将stride设置为1， 即不进行下采样。而且3×3卷积层全部采用膨胀卷积膨胀系数为2， 在Layer4中也是一样， 取消了下采样， 所有的3×3卷积层全部采用膨胀卷积膨胀系数为4。 最后需要注意的是ASPP模块， 在以ResNet101作为Backbone时， 每个分支只有一个3×3膨胀卷积层， 且卷积核的个数都等于num_classes。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/DeepLab_v2.jpg" alt="DeepLab_v2"></p><hr><h4 id="Learning-rate-policy"><a href="#Learning-rate-policy" class="headerlink" title="Learning rate policy"></a>Learning rate policy</h4><p>在DeepLab V2中训练时采用的学习率策略叫poly, 相比普通的step策略(即每间隔一定步数就降低一次学习率)效果要更好， 文中说最高提升了3.63个点。poly学习率变化策略公式如下：</p><script type="math/tex; mode=display">lr \times (1 - \frac{iter}{max_iter})^{power}</script><p>其中lr为初始学习率， iter为当前迭代的step数， max_iter为训练过程中总的迭代步数。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231109174954798.png" alt="image-20231109174954798"></p><hr><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p>下表是原论文中给出的一些消融实验对比</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231109175138075.png" alt="image-20231109175138075"></p><p>其中</p><p>MSC表示多尺度输入， 即先将图像缩放到0.5， 0.7和1.0三个尺度， 然后分别送入到网络预测得到score maps， 最后融合这三个score maps（对每个位置取三个score maps的最大值)</p><blockquote><p>Multi-scale inputs: We separately feed to the DCNN images at scale = {0.5, 0.75, 1} ， fusing their score maps by taking the maximum response across scales for each position separately</p></blockquote><p>COCO表示在COCO数据集上进行预训练</p><blockquote><p>Models pretrained on MS-COCO</p></blockquote><p>Aug代表数据增强， 这里就是对输入的图片从0.5到1.5之间随机缩放。</p><blockquote><p>Data augmentation by randomly scaling the input images (from 0.5 to 1.5) during training</p></blockquote><p>LargeFOV是在DeepLab V1中讲到过的结构</p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;论文原文：&lt;a href=&quot;[1</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>DeepLab_V1</title>
    <link href="https://guudman.github.io/2023/11/09/DeepLab-V1/"/>
    <id>https://guudman.github.io/2023/11/09/DeepLab-V1/</id>
    <published>2023-11-09T11:31:54.000Z</published>
    <updated>2023-11-09T11:33:32.588Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h3 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h3><p>论文原文： <a href="[1412.7062.pdf (arxiv.org">论文原文</a>](<a href="https://arxiv.org/pdf/1412.7062.pdf">https://arxiv.org/pdf/1412.7062.pdf</a>))</p><p>这篇文章最早发表于2014年， 是Google和UCLA等共同的杰作， 也是一篇很经典的论文。 DeepLab系列的第一篇论文， 因为已经过了很久，所以只是做简单的记录。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231109161905683.png" alt="image-20231109161905683"></p><hr><h4 id="2、语义分割中存在的问题"><a href="#2、语义分割中存在的问题" class="headerlink" title="2、语义分割中存在的问题"></a>2、语义分割中存在的问题</h4><p>在论文的引言中首先抛出了两个问题（针对语义分割任务）：<strong>信号下采样导致分辨率降低和空间不敏感</strong>问题。 </p><p>对于第一个问题<strong>信号下采样</strong>， 作者说主要是采用Maxpooling导致的， 为了解决这个问题， 作者引入了’atrous’ (with holes) algorithm（空洞卷积/膨胀卷积/扩张卷积）</p><p>对于第二个问题<strong>空间不敏感</strong>, 作者说分类器自身的问题（分类器本身具备一定空间不变性）。为解决这个问题作者采用了fully-connected CRF(Conditional Random Field)方法， 这个方法只在DeepLabv1-v2中使用到了， 从V3之后即不去使用了， 而且这个方法很耗时。</p><hr><h4 id="DeepLabV1的优势"><a href="#DeepLabV1的优势" class="headerlink" title="DeepLabV1的优势"></a>DeepLabV1的优势</h4><p>相比于之前的一些网络， 本文提出的网络具有以下优势：</p><ol><li><p>速度更快， 论文说是采用了膨胀卷积的原因， 但fully-connected CRF很耗时。 </p></li><li><p>准确率更高， 相比之前最好的网络提升了7.2个点</p></li><li>模型更简单， 主要由DCNN和CRG联级构成</li></ol><hr><h4 id="网络构建细节"><a href="#网络构建细节" class="headerlink" title="网络构建细节"></a>网络构建细节</h4><p>LargeFOV</p><p>首先网络的backbone是当时比较火的VGG-16, 并且和FCN网络一样将全连接层的权重转成了卷积层的权重， 构成全卷积网络。关于膨胀卷积的使用， 论文中这么说的。</p><blockquote><p>We skip subsampling after the last two max-pooling layers in the network of Simnyan &amp; Zisserman（2014）and modify the convolitional filters in the layers that follow them by intorducing zeros to increase their length (2×in the last three convolutional layers and 4× in the first fully connected layer)</p></blockquote><p>根据代码绘制了如下的网络结构（DeepLab-LargeFOV)</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/DeepLab-LargeFOV.jpg" alt="DeepLab-LargeFOV"></p><p>通过分析发现虽然backbone是VGG-16但所使用Maxpool略有不同， VGG论文中是kernel=2, stride=2, 但在DeepLabV1中是kernel=3, stride=2, padding=1。接着就是最后两个Maxpool层的stride全部设置成1了（这样才采样的倍率就从32变成了8）。最后三个3×3的卷积层采用了膨胀卷积，膨胀系数r=2。然后关于将全连接层卷积化过程中， 对于第一个全连接层（FC1)在FCN网络中是直接转换成卷积核大小7×7， 卷积核个数为4096的卷积层， 但在DeepLabv1中作者说是对参数进行了下采样最终得到的是卷积核大小3×3， 卷积核个数为1-24的卷积层，（这样不仅可以减少参数还可以减少计算量， 详情可以看论文的table2）， 对于第二个全连接层(FC2)卷积核个数也由4096采样成1024。</p><blockquote><p>After converting the network to a fully convolutional one, the first fully connected layer has 4096 filters of larget 7x7 spatial size and becomes the computianal bottleneck in our dense score map computation. We have addressed this practical problem by spatially subsampling (by simple decimation) the first FC layer to 4×4（or 3×3） spatial size.</p></blockquote><p>将FC1卷积化后， 还设置了膨胀系数， 论文3.1中说的是r=4但在Experimental Evaluation中Large of View章节里设置的是r=12对应LargeFOV。对于FC2卷积化后就是卷积核1x1， 卷积核个数为1024的卷积层， 接着再通过一个卷积核1x1, 卷积核个数为num_classes（包含背景）的卷积层， 最后通过8倍上采样还原成原图大小。 </p><p>下表是关于是否使用LargeFOV(Field of View)的对比。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231109151723458.png" alt="image-20231109151723458"></p><p>第一行DeepLab-CRF-7x7就是直接将FC1按照FCN论文中的方法转换成7x7的卷积层，并且膨胀因子r=4(receptive field=224)</p><p>第二行DeepLab-CRF是将7×7下采样到4×4大小的卷积层， 同样膨胀因子r=4(receptive filed=128), 可以看到参数减半， 训练速度翻倍， 但是mean IOU下降了约4点。 </p><p>第三行DeepLab-CRF-4X4， 是在DeepLab-CRF的基础上膨胀因子r改成了8（receptive field=224）, mean IOU又提升了回去了 </p><p>第四行DeepLab-CRF-LargeFOV, 是将7x7下采样到3×3的卷积层， 膨胀因子r=12(receptive field=224), 相比DeepLab-CRF-7×7， 参数减少了6倍， 训练速度提升了3倍多， mean IOU不变。 </p><hr><p>MSc(Multi-Scale)</p><p>其实在论文中的4.3还提到了Multi-Scale Prediction, 即融合多个特征层的输出， 关于MSc(Multi-Scale)的结构论文中是这么说的</p><blockquote><p>Specifically, we attach to the input image and the output of each of the first four max pooling layers a two-alyer MLP (first layer:128 3×3 convolutional filters, second layer:128 1×1 convolutional filters) whose feature map is concatenated to the main network’s last layer feature map, The aggregate feature map fed into the softmax layer is thus enhanced by 5*128=640 channles</p></blockquote><p>即， 除了使用之前主分支上输出外， 还融合了来自原图尺度以及前4个Maxpool层的输出， 更详细的结构参考下图， 论文中说使用Msc大概能提升1.5个点， 使用fully-connected CRF大概能提升4个点。但是在源码中作者建议使用的是不带MSc的版本以及github上的一些开源实现都没有使用MSc, 可能得原因是MSc不仅费时而且很吃显存， 根据参考源码绘制了DeepLab-MSc-LargeFOV结构。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/DeepLab1_MSc_FOV.jpg" alt="DeepLab1_MSc_FOV"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h3 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h3&gt;&lt;p&gt;论文原文： &lt;a href=&quot;[</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SwinTransformer</title>
    <link href="https://guudman.github.io/2023/11/08/SwinTransformer/"/>
    <id>https://guudman.github.io/2023/11/08/SwinTransformer/</id>
    <published>2023-11-08T10:07:06.000Z</published>
    <updated>2023-11-08T10:10:53.353Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>Swin Transformer是2012年微软研究院在ICCV上发表的一篇文章， 并荣获2021 ICCV最佳论文称号。 Swin Transformer网络是Transformer模型在视觉领域的又一次碰撞。 </p><p>论文名称：Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</p><p>论文地址： <a href="https://arxiv.org/pdf/2103.14030v2.pdf">Swin Transformer</a></p><hr><h4 id="2、论文整体框架"><a href="#2、论文整体框架" class="headerlink" title="2、论文整体框架"></a>2、论文整体框架</h4><p>在正文开始之前， 先对比一下Swin transformer与Vision Transformers， 下图是swin Transformers文章中给出的图， 左边是Swin Transformers，右边是之前的Vision Transformers，通过对比可以发现有两点不同。</p><ol><li>Swin Transformers使用了之前类似卷积神经网络的层次化构建方法（Hierachical feature maps）, 比如特征图尺寸有对图像下采样4倍的， 8倍的以及16倍的， 这样的backbone有助于在此基础上构建目标检测， 实例分割等任务。 而之前的Vision Transformer中一开始就是直接下采样16倍， 后面的特征图也是维持这样的下采样率不变。 </li><li>在Swin Transformer中使用了Window Multi-Head Self-Attention(W-MSA)的概念， 比如在4倍下采样和8倍下采样中， 将特征图划分成多个不相交的区域（windows）， 并且Multi-Head Self-Attention只在每个窗口内进行， 相对于Vision Transformer中直接对整个特征图进行Multi-Head Self-Attention， 这样做的目的是能够减少计算量， 尤其在浅层特征图很大的时候。这样做虽然减少了计算量， 但是也会隔绝不同窗口之间的信息传递， 所以在论文中作者又提出Shifted Windows Multi-Head Self-Attention(SW-MSA)的概念， 通过该方法能够让信息在相邻的窗口中进行传递。 </li></ol><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231107151900499.png" alt="image-20231107151900499"></p><p>接下来，简单看下原论文中给出的关于Swin Transformer网络的架构图， 通过图(a)可以看到整个架构的基本历程如下：</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231107152115158.png" alt="image-20231107152115158"></p><p>首先将图片输入到Patch Partition模块中进行分块， 即每4×4相邻的像素为一个Patch, 然后再channel方向展平（flatten)。假设输入的是RGB三通道图片， 那么每个patch就有4×4=16个像素， 然后每个像素有R/G/B三个值， 所以展平后是16×3=48， 所以通过patch partition后图像shape由[H, W, 3]变成了[H/4, W/4, 48]。 然后通过Linear Embedding层对每个像素的channel数据做线性变换， 由48变成C， 即图像shape再由[H/4, W/4, 48]变成了[H/4 ,W/4, C]。 其实在源码中Patch Patittion和Linear Embedding就是直接通过一个卷积实现的， 和之前VisionTransformer中讲的Embedding层结构一模一样。 </p><p>然后通过四个Stage构建不同大小的特征图， 除了Stage1中先通过Linear Embedding层外， 其他三个都是先通过一个Patch Merging层进行下采样。 然后都是重复堆叠Swin Transformer Block。注意这里的Block有两种结构， 如图（b)所示， 这两种结构的不同之处在于一个使用了W-MSA结构， 一个使用了SW-MSA结构。 而且这两种结构都是成对使用的，先使用一个W-MSA结构再使用一个SW-MSA结构。 所以会发现堆叠SwinTransformer Block的次数都是偶数(成对使用)</p><p>最后对于分类网络， 还会接上一个Layer Norm层， 全局池化层以及全连接层得到最终输出。 </p><p>下面分别对Patch Merging , W-MSA, SW-MAS以及使用到的相对位置偏置(relative position bias)进行详解。 关于Swin Transformer Block中的MLP结构和Vision Transformer中的结构一样。</p><hr><h4 id="3、Patch-Merging详解"><a href="#3、Patch-Merging详解" class="headerlink" title="3、Patch Merging详解"></a>3、Patch Merging详解</h4><p>前面提到， 在每个Stage中首先经过一个Patch Mering层进行下采样(stage1除外). 如下图所示， 假设输入Patch Merging的是一个4×4大小的单通道特征图（feature map）， Patch Merging会将每个2×2的相邻像素划分为一个patch, 然后将每个patch中相同位置（同一颜色）像素拼接到一起就得到了4个feature map。接着将这四个feature map在深度方向进行concat拼接， 然后通过一个LayerNorm层， 最后通过一个全连接层在feature map的深度方向做线性变换， 将feature map的深度有C变成C/2, 通过这个简单的例子可以看出， 通过Patch Merging层后， feature map的高和宽会减半， 深度会翻倍。 </p><p> <img src="https://gitee.com/guudman/blog_images/raw/master/image-20231107182524433.png" alt="image-20231107182524433"></p><hr><h4 id="4、W-MSA详解"><a href="#4、W-MSA详解" class="headerlink" title="4、W-MSA详解"></a>4、W-MSA详解</h4><p>引入Windows Multi-Head Self-Attention（W-MSA)模块是为了减少计算量， 如下如所示， 左侧使用的是普通的Multi-Head Self-Attention（MSA)模块， 对于feature map中的每个像素（或称作token， patch)， 在Self-Attention计算过程中需要和所有的像素去计算， 但在图右侧， 在使用Windows Multi-Head Self-Attention （W-MSA)模块时， 首先将feature map按照MxM(例子中M=2)大小划分成一个个Windowns, 然后单独对每个windows内部进行Self-Attention。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108103028326.png" alt="image-20231108103028326"></p><p>两者的计算量具体差多少呢？原论文中给出了下面两个公式， 这里忽略了Softmax的计算复杂度：</p><script type="math/tex; mode=display">MSA = 4hwC^2 + 2(hw)^2C</script><script type="math/tex; mode=display">W_MSA = 4hwC^2 + 2M^2hwC</script><p>h， w代表feature map的高度和宽度</p><p>C代表feature map的深度</p><p>M代表每个窗口（Windows）的大小</p><p>回顾一下Self-Attention的公式</p><script type="math/tex; mode=display">Attentino(Q, K, V) = SoftMax(\frac{QK^T}{\sqrt(d)})V</script><hr><p>MSA模块计算量</p><p>对于feature map中的每个像素（或者称为token, patch)， 都要通过<script type="math/tex">W_q, W_k, W_v</script>生成对应的query(q)， key(q)以及value(v), 这里假设q, k, v的向量长度与feature map的深度C保持一致， 那么对应所有像素生成Q的过程如下式：</p><script type="math/tex; mode=display">A^{hw\times C}.W_q^{C\times C}= Q^{hw\times C}</script><script type="math/tex; mode=display">A^{hw\times C}$$对所有像素（token)拼接在一起得到的矩阵(一共有hw个像素， 每个像素的深度为C)$$W_q^{C \times C}$$为生成query的变换矩阵$$Q^{hw\times C}$$对所有像素通过$$W_q^{C \times C}$$得到的query拼接后的矩阵根据矩阵运算的计算量公式可以得到生成Q的计算量为$$hw \times C \times C$$， 生成K和V， 同理都是$$hwC^2$$,那么总共是$$3hwC^2$$。接下来Q和$$K^T$$， 对应计算量为$$(hw)^2C$$:</script><p>Q^{hw \times C}.K^{T(c\times hw)}=X^{hw \ time hw}</p><script type="math/tex; mode=display">接下来忽略除以$$\sqrt{d}$$以及softmax的计算量， 假设得到$$$A^{hw \times hw}$$ , 最后还要乘以V， 对应的计算量为 $$(hw)^2C</script><script type="math/tex; mode=display">A^{hw \times hw}.V^{hw\times C}= B^{hw \times C}</script><p>那么对应单头的Self-Attention模块， 总共需要<script type="math/tex">3hwC^2 + (hw)^2C + (hw)^2C = 3hwC^2 + 2(hw)^2C</script>。而实际使用过程中， 使用的是多头的Multi-Head Self-Attention模块， 在之前的文章中尽显过实验对比， 多头注意力模块相比单头注意力模块的计算量仅多了最后一个融合矩阵<script type="math/tex">W_O</script>的计算量<script type="math/tex">hwC^2</script></p><script type="math/tex; mode=display">B^{hw \times C}.W_O^{hw\times C}= O^{hw \times C}</script><p>所以总共加起来是：<script type="math/tex">4hwC^2 + 2(hw)^2C</script></p><hr><h4 id="5、W-MSA模块计算量"><a href="#5、W-MSA模块计算量" class="headerlink" title="5、W-MSA模块计算量"></a>5、W-MSA模块计算量</h4><p>对于W-MSA模块首先要将feature map划分到一个窗口（windows)中， 假设每个窗口的高宽都是M， 那么总共会得到<script type="math/tex">\frac{h}{M} \times \frac{w}{M}</script>, 对于每个窗口内使用多头注意力模块， 刚刚计算高为h， 宽为w， 深度为C的feature map的计算量为<script type="math/tex">4hwC^2 + 2(hw)^2C</script>, 这里每个窗口的高为M， 宽为M， 带人公式得：</p><script type="math/tex; mode=display">4(MC)^2 + 2(M)^4C</script><p>又因为有<script type="math/tex">\frac{h}{M} \times \frac{w}{M}</script>个窗口， 则：</p><script type="math/tex; mode=display">\frac{h}{M} \times \frac{w}{M} \times (4(MC)^2 + 2(M)^4C = 4hwC^2 + 2M^2hwc</script><p>故使用W-MAS模块的计算量为：<script type="math/tex">4hwC^2 + 2M^2hwc</script></p><p>假设feature map的h， w都是112， M=7， C=112, 采用W-MSA模块相比MSA模块能够减少40124743680FLOPs</p><script type="math/tex; mode=display">2(hw)^2C - 2M^2hwc = 2 \times 112 ^ 4 \times 128 - 2 \times 7^2 \times 112^2 \times 128 = 40124743680</script><hr><h4 id="6、SW-MAS详解"><a href="#6、SW-MAS详解" class="headerlink" title="6、SW-MAS详解"></a>6、SW-MAS详解</h4><p>采用W-MSA模块时， 只会在每个窗口内进行注意力计算， 所以窗口与窗口之间是无法进行消息传递的。 为了解决这问题， 作者引入了shifted windows Multi-Head Self-Attention（SW-MSA)模块， 即进行偏移的W-MSA。如下图所示， 左侧使用的是刚刚讲的W-MSA(假设是第L层), 那么根据之前介绍的W-SMA和SW-MAS是称帝使用的， 那么第L+1层使用的就是SW-MSA(右侧图)。 根据左右两幅图对比能够发信啊窗口发生了偏移（可以理解为创库从左上角分别向右侧和下方各偏移了<script type="math/tex">[\frac{M}{2}]</script>个像素)。 看下偏移后的窗口， 比如对于第一行第2列的2x4的窗口， 它能够使用第L层的第一排的两个窗口信息进行交流。 再比如， 第二行第二列的4x4的窗口， 它能够使第L层的四个窗口信息进行交流， 其他的同理， 那么这就解决了不同窗口之间无法进行信息交流的问题。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108114622550.png" alt="image-20231108114622550"></p><p>根据上图， 可以发现通过将窗口进行偏移后， 由原来的4个窗口变成9个窗口了， 后面又要对每个窗口内部进行MSA， 这样做感觉又变麻烦了， 为解决这个麻烦， 作者又提出了Efficient batch computation for shifted configuration， 一种更加高效的计算方法， 下面是原论文给出的示意图。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108114945850.png" alt="image-20231108114945850"></p><p>为更好的解释， 作如下示意图， 下图左侧是刚刚通过偏移窗口后得到的新窗口， 右侧是为了方便大家理解， 在每个窗口上加 另一个标识， 然后0对应的窗口表示区域为A， 3和6对应的窗口标记区域为B， 1和2对应的窗口标记为区域C。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108141757256.png" alt="image-20231108141757256"></p><p>将A和C移到最下方</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108141834985.png" alt="image-20231108141834985"></p><p>然后再将A和B移到最右侧</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108142740693.png" alt="image-20231108142740693"></p><p>移动完成后，4是一个单独的窗口， 将5和3合并到一起， 7和1合并成一个窗口， 8， 6， 2， 0合并成一个窗口。 这样又和原来一样是4个4×4的窗口了。 所以保证计算量是一样的。 这里肯定会有人疑惑， 把不同的区域合并在一起（比如5和3）进行Multi-Head Self-Attention，这信息不就乱窜了吗？是的， 为了放置这个问题， 实际计算中使用的是masked MSA即带蒙板mask的Multi-Head Self-Attention， 这样就能够设置蒙板来隔绝不同区域的信息了。 关于mask如何使用， 可以看下图， 下图以上图的区域5和区域3为例。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108144754091.png" alt="image-20231108144754091"></p><p>对于该窗口内的每一个像素（或称token， patch）在进行Multi-Head Self-Attention计算时， 都要先生成对应的query(q), key(k), value(v)。 假设对于上图的像素0而言， 得到<script type="math/tex">q^0</script>后要与每一个像素的k进行匹配（match）， 假设<script type="math/tex">a_{0,0}</script>代表<script type="math/tex">q^0</script>与像素0对应的<script type="math/tex">k^0</script>进行匹配的结果， 那么同理可以得到<script type="math/tex">a_{0, 0}</script>至<script type="math/tex">a_{0, 15}</script>。按照普通的MSA计算， 接下来就是Softmax操作了， 但对于这里的masked Multi-Head Self-Attention， 像素0属于区域5的， 我们只想让它和区域5的像素进行匹配。那么将像素0与区域3中的所有像素匹配结果都减去100（例如<script type="math/tex">a_{0, 2}, a_{0, 3}, a_{0, 6}, a_{0,7}</script>等等）。 由于<script type="math/tex">\alpha</script>的值很小， 一般都是零点几的数， 将其中一些数减去100后再通过softmax得到对应的权重都等于0了。 所以对于像素0而言实际上还是只和区域5内的像素进行了Multi-Head Self-Attention， 那么对于其他像素也是同理。 <strong>注意，在计算完后还要把数据给挪回到原来的位置上（例如上述的A, B, C区域</strong></p><hr><h4 id="7、Relative-Position-Bias详解"><a href="#7、Relative-Position-Bias详解" class="headerlink" title="7、Relative Position Bias详解"></a>7、Relative Position Bias详解</h4><p>关于相对位置偏置， 论文中没有细讲， 只是说使用了相对位置偏置后能够带来明显的提升。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108150114591.png" alt="image-20231108150114591"></p><p>相对位置偏置是如何使用的，论文中提供了如下的公式：</p><script type="math/tex; mode=display">Attention(Q, K, V) = SoftMax(\frac{QK^T}{\sqrt{d} +B})V</script><p>由于论文中并没有详细讲解这个相对位置偏置， 所以根据阅读源码做了简单的总结。 如下图， 假设输入的feature map高和宽都是2， 那么首先可以通过构建每个像素的相对位置（左下方的矩阵）， 对于每个像素的绝对位置是使用行号和列号表示。 比如蓝色的像素对应的是第0行和第0列， 所以绝对位置索引是（0， 0）。</p><p>下面来看相对位置索引， 首先看蓝色的像素， 在蓝色像素使用q与所有像素k进行匹配过程中， 是以蓝色像素为参考点。然后使用蓝色像素的绝对位置索引与其他位置索引进行相减， 就得到其他位置相对于蓝色像素的<strong>相对位置索引</strong>。例如黄色像素的绝对位置索引是（0， 1）， 则它相对于蓝色像素的相对位置索引为（0， 0） - （0， 1）= （0， -1）。同理可以得到其他位置相对于蓝色像素的相对位置索引矩阵。 同样， 也能得到相对黄色， 红色以及绿色像素的相对位置索引矩阵。 接下来将每个相对位置索引矩阵<strong>按行展开</strong>， 并<strong>拼接</strong>在一起可以得到下面的4×4矩阵。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108153312906.png" alt="image-20231108153312906"></p><p>注意， 这里描述的都是<strong>相对位置所以</strong>， 并不是<strong>相对位置偏置参数</strong>。因为后面后面会根据相对位置索引去取对应的参数。比如黄色像素在蓝色像素的右边， 所以相对蓝色像素相对位置索引为（0， -1）。绿色像素在红色像素的右边， 所以相对红色像素的相对位置索引为（0,-1）。可以发现二者的相对位置索引都是（0， -1）， 所以它们使用相同的<strong>相对位置偏置参数</strong>。</p><p>在源码中作者为了方便吧二维索引转成一维索引， 具体怎么转的呢？， 简单的想法是直接把行、列索引相加不就可以变成一维了吗？比如上面的相对位置索引中有（0， -1）和（-1， 0）, 在二维的相对位置索引中明显代表不同的位置， 但如果简单相加都等于-1那不就出问题了吗， 下面看看源码中是怎么做到的。 首先在原始的相对位置索引上加上M-1（M为窗口的大小， 在本例中M=2）, 加上之后索引中就不会有负数了。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108154623062.png" alt="image-20231108154623062"></p><p>然后将所有的行标都乘以2M-1</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108154937473.png" alt="image-20231108154937473"></p><p>最后将行标和列标都相加， 这样即保证了相对位置关系， 而且不会出现上述0 + （-1） = （-1） + 0的问题了。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108155226820.png" alt="image-20231108155226820"></p><p>上面提到， 之前计算的是<strong>相对位置索引</strong>， 并不是<strong>相对位置偏置参数</strong>, 真正使用到的可训练参数B保存在relative position bias table表中的额， 这个表的长度等于（2M-1)×(2M-1)的。 那么上述公式中的相对位置偏置参数B是根据上面的相对位置索引表查relative position bias table表得到的。 如下图所示。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108160222893.png" alt="image-20231108160222893"></p><hr><h4 id="8、模型配置参数"><a href="#8、模型配置参数" class="headerlink" title="8、模型配置参数"></a>8、模型配置参数</h4><p>回顾一下swin-transformer的网络架构</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231107152115158.png" alt="image-20231107152115158"></p><p>下表是原论文给出的关于不同swin Transformers的配置， T（tiny）， S（small), B(Base), L(Large)， 其中：</p><p>win. sz. 7×7表示使用的窗口（Windows)的大小</p><p>dim表示feature map的channel深度（或者说token的向量长度）</p><p>head表示多头注意力模块中head的个数</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231108160608602.png" alt="image-20231108160608602"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;Swin Transformer</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>TransposeConv</title>
    <link href="https://guudman.github.io/2023/11/07/TransposeConv/"/>
    <id>https://guudman.github.io/2023/11/07/TransposeConv/</id>
    <published>2023-11-07T02:02:47.000Z</published>
    <updated>2023-11-07T02:12:50.570Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>首先回顾一下普通卷积， 下图是以stride=1， padding=0， kernel_size=3为例， 假设输入特征图大小为4×4(假设输入输出都是单通道), 通过卷积后的特征图大小为2×2， 一般使用卷积的情况中， 要是特征图变小（stride&gt;1)， 要么保持不变(stride=1)， 当然也可以通过四周padding让特征图变大但是没有意义。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/conv1.gif" alt="conv1"></p><hr><h4 id="2、转置卷积"><a href="#2、转置卷积" class="headerlink" title="2、转置卷积"></a>2、转置卷积</h4><p>转置卷积主要起到上采样的作用， 但转置卷积不是卷积的逆运算(一般卷积操作是不可逆的), 它只能恢复到原来的大小， 数值与原来的不一样。转置卷积的运算步骤可以归纳为以下几点：</p><p>在输入特征图元素间填充s-1行， 列0(其中s是转置卷积的步距)</p><p>在输入特征图四周填充p-1行， 列0（其中k表示转置卷积的kernel_size大小， p为转置卷积的padding， 注意这里的padding和卷积操作有些不同)</p><p>将卷积核参数上下、左右翻转</p><p>做正常卷积运算(填充0， 步距1)</p><p>下面假设输入的特征图为2×2（假设输入输出为单通道)， 通过转置卷积后的的得到4×4大小的特征图。 这里使用的转置卷积核大小k=3， stride=1， padding=0的情况(忽略偏置bias)</p><p>首先在元素间填充s-1=0行，列0</p><p>然后再特征图四周填充k-p-1=2行， 列0</p><p>接着对卷积核参数进行上下， 左右翻转</p><p>最后做正常卷积(填充0， 步距1)  transpose_conv.jpg</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/transpose_conv.jpg" alt="transpose_conv"></p><p>下面展示了不同转置卷积中不同s和p的情况  transpose_conv_s2_p0_k3.gif  transpose_conv_s1_p0_k3.gif  transpose_conv_s2_p1_k3.gif</p><table style="text-align: center;">    <tr>    <td><img src="https://gitee.com/guudman/blog_images/raw/master/transpose_conv_s1_p0_k3.gif" width="200">    <div>s=1, p=0, k=3 </div>    <div>间隙填充s-1=0,四周填充k-p-1=2</div>    </td>   <td>     <img src="https://gitee.com/guudman/blog_images/raw/master/transpose_conv_s2_p0_k3.gif" width="200">         <div>s=2, p=0, k=3</div>      <div>间隙填充s-1=1,四周填充k-p-1=2</div>    </td>   <td>        <img src="https://gitee.com/guudman/blog_images/raw/master/transpose_conv_s2_p1_k3.gif" width="200">       <div> s=2, p=1, k=3</div>       <div>间隙填充s-1=1,四周填充k-p-1=1</div>    </td>    </tr></table><p>转置卷积操作后特征图的大小可以通过如下公式进行计算：</p><script type="math/tex; mode=display">H_{out}=(H_{in} - 1) \times stride[0] -2 \times padding[0] + kernel\_size[0]</script><script type="math/tex; mode=display">W_{out}=(W_{in} - 1) \times stride[1] -2 \times padding[1] + kernel\_size[1]</script><p>其中stride[0]表示高度方向的stride， padding[0]表示高度方向的padding， kernel_size[0]表示高度方向的kernel_size, 同理[1]表示宽度方向上的。 通过上面的公式可以看出， padding越大，输出特征矩阵的高宽越小。 你可以理解为正向传播过程中进行了padding然后得到了特征图， 现在使用转置卷积还原到原来的高宽后要把原来的padding减掉。 </p><hr><h4 id="3、pytorch中转置卷积参数"><a href="#3、pytorch中转置卷积参数" class="headerlink" title="3、pytorch中转置卷积参数"></a>3、pytorch中转置卷积参数</h4><p>pytorch官方关于转置卷积的ConvTranspose2d的文档。 </p><p>Applied a 2D transposed convolution operator over an input image composed of several input planes. This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (althought it is not an actual deconvolution operation).   image-20231107093032555.png</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231107093032555.png" alt="image-20231107093032555"></p><p>官方的介绍中还有如下几个参数：</p><p>output_padding: 在计算得到的输出特征图的高、宽各方向各填充几行或列0(注意：这里只是在上下以及左右的一侧one side填充，并不是两侧头填充)</p><p>groups: 当使用到组卷积时才会用到的参数， 默认为1即为普通卷积</p><p>bias： 是否使用偏置， 默认为True使用</p><p>dilation： 当使用空洞卷积（膨胀卷积）时才会使用到的参数， 默认为1即为普通卷积。</p><p>输出特征图高、宽计算</p><script type="math/tex; mode=display">H_{out}=(H_{in} - 1) \times stride[0] -2 \times padding[0] \\ + dilation[0] \ times (kernel_size[0] - 1) + output_padding[0] + 1</script><script type="math/tex; mode=display">W_{out}=(W_{in} - 1) \times stride[1] -2 \times padding[1] + \\dilation[1] \ times (kernel_size[1] - 1) + output_padding[1] + 1</script><hr><h4 id="4、pytorch转置卷积实验"><a href="#4、pytorch转置卷积实验" class="headerlink" title="4、pytorch转置卷积实验"></a>4、pytorch转置卷积实验</h4><p>下面使用pytorch模拟s=1, p=0, k=3的转置卷积操作</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/transpose_conv_s1_p0_k3.gif" alt="conv_s1k0p3"></p><p>在代码中transpose_conv_official函数是通过官方的转置卷积进行计算， transpose_conv_self是按照上面的介绍对输入特征图进行填充并通过卷积得到的结果。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : convTranspose_demo.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transposed_conv_official</span>():</span><br><span class="line">    feature_map = torch.as_tensor([[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                                   [<span class="number">2</span>, <span class="number">1</span>]], dtype=torch.float32).reshape([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">    <span class="built_in">print</span>(feature_map)</span><br><span class="line">    trans_conv = nn.ConvTranspose2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>,</span><br><span class="line">                                    kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    trans_conv.load_state_dict(&#123;<span class="string">&quot;weight&quot;</span>:</span><br><span class="line">                                    torch.as_tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                                                     [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                                     [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">                                                    dtype=torch.float32)</span><br><span class="line">                               .reshape([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>])&#125;)</span><br><span class="line">    <span class="built_in">print</span>(trans_conv.weight)</span><br><span class="line">    output = trans_conv(feature_map)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output, \n&quot;</span>, output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transposed_conv_self</span>():</span><br><span class="line">    feature_map = torch.as_tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                   [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                   [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                   [<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                   [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                   [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                   ], dtype=torch.float32).reshape([<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line">    <span class="built_in">print</span>(feature_map)</span><br><span class="line">    conv = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>,</span><br><span class="line">                     kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    conv.load_state_dict(&#123;<span class="string">&quot;weight&quot;</span>: torch.as_tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                                                     [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                                                     [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]], dtype=torch.float32).reshape([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>])&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(conv.weight)</span><br><span class="line">    output = conv(feature_map)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output \n&quot;</span>, output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    transposed_conv_official()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;--------&#x27;</span>)</span><br><span class="line">    transposed_conv_self()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>终端输出：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">feature<span class="built_in">_</span>map </span><br><span class="line"> tensor([[[[1., 0.],</span><br><span class="line">          [2., 1.]]]])</span><br><span class="line">trans<span class="built_in">_</span>conv.weight </span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([[[[1., 0., 1.],</span><br><span class="line">          [0., 1., 1.],</span><br><span class="line">          [1., 0., 0.]]]], requires<span class="built_in">_</span>grad=True)</span><br><span class="line">output, </span><br><span class="line"> tensor([[[[1., 0., 1., 0.],</span><br><span class="line">          [2., 2., 3., 1.],</span><br><span class="line">          [1., 2., 3., 1.],</span><br><span class="line">          [2., 1., 0., 0.]]]], grad<span class="built_in">_</span>fn=&lt;ConvolutionBackward0&gt;)</span><br><span class="line">--------</span><br><span class="line">self feature<span class="built_in">_</span>map </span><br><span class="line"> tensor([[[[0., 0., 0., 0., 0., 0.],</span><br><span class="line">          [0., 0., 0., 0., 0., 0.],</span><br><span class="line">          [0., 0., 1., 0., 0., 0.],</span><br><span class="line">          [0., 0., 2., 1., 0., 0.],</span><br><span class="line">          [0., 0., 0., 0., 0., 0.],</span><br><span class="line">          [0., 0., 0., 0., 0., 0.]]]])</span><br><span class="line">conv.weight </span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([[[[0., 0., 1.],</span><br><span class="line">          [1., 1., 0.],</span><br><span class="line">          [1., 0., 1.]]]], requires<span class="built_in">_</span>grad=True)</span><br><span class="line">output </span><br><span class="line"> tensor([[[[1., 0., 1., 0.],</span><br><span class="line">          [2., 2., 3., 1.],</span><br><span class="line">          [1., 2., 3., 1.],</span><br><span class="line">          [2., 1., 0., 0.]]]], grad<span class="built_in">_</span>fn=&lt;ConvolutionBackward0&gt;)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;首先回顾一下普通卷积， 下图是以</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>VisionTransformer</title>
    <link href="https://guudman.github.io/2023/11/05/VisionTransformer/"/>
    <id>https://guudman.github.io/2023/11/05/VisionTransformer/</id>
    <published>2023-11-05T03:13:51.000Z</published>
    <updated>2023-11-05T03:15:16.157Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>论文原文：<a href="[[2010.11929\] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (arxiv.org">Vit</a>](<a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>))</p><p>Transformer最初是针对NLP领域提出的， 并且在NLP领域大获成功。Vision-Transformer尝试将Transformer应用到CV领域。通过原论文中的实验发现，论文给出的模型在ImageNet1k上能够达到88.55%的准确率， 说明Transformer在CV领域确实能取得不错的效果。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231105093844437.png" alt="image-20231105093844437"></p><hr><h4 id="2、模型详解"><a href="#2、模型详解" class="headerlink" title="2、模型详解"></a>2、模型详解</h4><p>原论文中，作者主要拿ResNet， Vit(纯Transformer模型)以及Hybrid（卷积和Transformer混合模型）三个模型进行比较。</p><p>下图是原论文中Vit的模型架构， 简单而言， 模型由三部分组成：</p><p>1、Linear Project of Flattened Patches(Embedding层)</p><p>2、Transformers Encoder(图右侧给出更加详细的结构)</p><p>3、MLP Head（用于分类的层结构）</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231105094454791.png" alt="image-20231105094454791"></p><h5 id="Embedding层结构"><a href="#Embedding层结构" class="headerlink" title="Embedding层结构"></a>Embedding层结构</h5><p>对于标准的Transformers模块， 要求输入的是token（向量）序列， 即二维矩阵[num_token, token_dim]， 如下图， token0-9对应的都是向量， 以Vit-B/16为例， 每个token向量长度为768。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231105094748766.png" alt="image-20231105094748766"></p><p>对于图像数据而言， 其数据格式为[H, W, C]， 它是三维矩阵形式， 该形式明显不是Transformer想要的， 所以首先要通过Embedding层对数据做变换。如下图所示， 首先将一张图片按给定大小分成一堆patches。以Vit-B/16为例， 将输入图片(224×224)按照16×16大小的Patch进行划分， 划分后得到<script type="math/tex">(224/16)^2=196</script>个Patches。接着通过线性映射将每个Patch映射到一维向量中， 每个Patches数据shape[16, 16, 3]， 通过映射得到长度为768的向量（后面直接称为token)。<font color="red">[16, 16, 3]-&gt;[768]</font>&gt;。</p><p><strong>在代码实现中，直接通过一个卷积层来实现</strong>, 以Vit-B/16为例， 直接使用一个卷积核大小为16×16， 步距为16， 卷积核个数为768的卷积来实现。通过卷积<font color="red">[224, 224, 3] -&gt;[14, 14, 768]</font>, 然后把H以及W两个维度展平即可：<font color="red">[14, 14, 768]-&gt;[194, 768]</font>,此时正好变成了一个二维矩阵， 正式Transformers想要的。</p><p><strong>在输入Transformer Encoder之前需要加上[class]token以及Position Embedding</strong>。原论文中， 作者参考了Bert， 在刚刚得到的一堆tokens中插入一个专门用于分类的[class] token, 这个[class]token是一个可训练的参数， 数据格式和其他token一样都是一个向量， 以Vit-B/16为例， 就是一个长度为768的向量， 与之前从图片中生成的tokens拼接在一起。<font color="red">Cat([1, 768], [196, 768]) - &gt; [197, 768]</font>。Position Embedding与之前在Transformers中的Position Embedding一样， 直接叠加在tokens上(add)， 所以shape是一样的。刚刚拼接了[class] token之后的shape是<font color="red"> [197, 768]</font>, 所以这里的Position Embedding也是<font color="red"> [197, 768]</font>。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231105100949257.png" alt="image-20231105100949257"></p><p>对于Position Embedding作者也做了一系列对比实验， <strong>在源码中默认使用的是</strong> <font color="red">1D Pos. Emb.</font>, 对比不使用Position Embedding准确率大概提升了3个点。和<font color="red">2D Pos. Emb.</font>相比， 效果没有太大的差别。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231105101257899.png" alt="image-20231105101257899"></p><hr><h5 id="Transformer-Encoder详解"><a href="#Transformer-Encoder详解" class="headerlink" title="Transformer Encoder详解"></a>Transformer Encoder详解</h5><p>Transformer Encode其实就是重复Encoder Block L次， 如下图所示，主要由以下几个部分组成。 </p><p>Layer Norm， 这种Normalization方法主要是针对NLP领域提出的， 这里是对每个token进行Norm处理。</p><p>Mliti-Head Attention， 与之前Transformers中讲解的一样， 这里不再赘述。 </p><p>Dropout/Droppath. 原论文的代码中直接使用的是Dropout层， 但在<font color="red">rwightman</font>实现的代码中使用的是DropPath(stochastic depth), 可能后者的效果更好一点。 </p><p>MLP Block， 就是全连接层+GELU激活函数+Dropout组成。 全连接层节点个数变化如下： <font color="red">[197, 768] -&gt; [197, 3072] -&gt; [197, 768]</font></p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231105102438613.png" alt="image-20231105102438613"></p><hr><h5 id="MLP-Head详解"><a href="#MLP-Head详解" class="headerlink" title="MLP Head详解"></a>MLP Head详解</h5><p>上面通过Transformers Encoder之后输入与输出的shape保持不变， 以Vit-B/16为例， 输入的是[197, 768]输出还是[197, 768]。 这里我们只需要分类的信息， 所以我们只需要提取出[class]token生成对应的结果捷星， 即从[197, 768]中提取[class token]对应的[1, 768]。 然后通过MLP Head得到最终的分类结果。 原论文中是由Linear + tanh激活函数+Linear组成。但是迁移到别的数据集上时只用一个Linear即可。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231105103058460.png" alt="image-20231105103058460"></p><hr><h5 id="绘制自己的Vision-Transformers结构"><a href="#绘制自己的Vision-Transformers结构" class="headerlink" title="绘制自己的Vision Transformers结构"></a>绘制自己的Vision Transformers结构</h5><p><img src="https://gitee.com/guudman/blog_images/raw/master/VIT_16_b_2.jpg" alt="image-20231105103058460"></p><h5 id="Hybrid模型详解"><a href="#Hybrid模型详解" class="headerlink" title="Hybrid模型详解"></a>Hybrid模型详解</h5><p>原论文中详细介绍了Hybird混合模型， 就是将传统的CNN提取特征和Transformer进行结合。下图绘制的是以Resnet50作为特征提取器的混合模型，但是这里的resnet50与之前的有所不同，这里的resnet50采用的是stdconv2d而不是传统的conv2d， 然后将所有的batch Normalization换成了group norm层。 在原来的resnet50中， stage1重复堆叠3次， stage2重复堆叠4次， stage3重复堆叠6次， stage4重复堆叠3次。 但在这里的resnet50中， 把stage4中的3个Block移到了stage3中， 所以stage3中总共重复堆叠9次。 </p><p>通过R50 Backbone提取特征后， 得到的特征矩阵shape是[14, 14, 1014]， 接着再输入Patch Embedding层， 注意Patch Embedding中卷积层的Conv2d的kernel size都变成1， 只是用来调整channel， 后面的部分与前面的vit讲的完全一样，这里不再赘述。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/VIT_16_b_Hybrid.jpg" alt="image-20231105103058460"></p><p>下表是论文中对比Vit, Resnet以及Hybrid模型的效果， 对比发现， 在训练epoch较少时， Hybrid效果优于Vit。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231105111111146.png" alt="image-20231105111111146"></p><h5 id="Vit模型参数"><a href="#Vit模型参数" class="headerlink" title="Vit模型参数"></a>Vit模型参数</h5><p>Layers是Transformer Encoder中重复堆叠Encoder Block的次数</p><p>Hidden Size是通过Embedding层后每个token的dim</p><p>MLP size是Transformer Encoder中MLP Block第一个全连接的节点个数(是Hidden Size的四倍)</p><p>Heads代表Transformer中Multi-Head Attention的heads数</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231105111157839.png" alt="image-20231105111157839"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;论文原文：&lt;a href=&quot;[[</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Transofrmer</title>
    <link href="https://guudman.github.io/2023/11/04/Transofrmer/"/>
    <id>https://guudman.github.io/2023/11/04/Transofrmer/</id>
    <published>2023-11-04T09:29:10.000Z</published>
    <updated>2023-11-04T09:38:10.571Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>论文原文：<a href="[arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf">attention原文</a>)</p><p>Transformer是2017年Google提出的， 当时主要是针对自然语言处理领域提出的。之前的RNN模型记忆长度有限且无法并行化，只有计算完<script type="math/tex">t_i</script>时刻后的数据才能计算<script type="math/tex">t_{i+1}</script>时刻的数据， 但Transformer都可以做到。在原论文中作者首先提出了self-attention的概念， 然后在此基础上提出Multi-Head Attention。本文针对self-attention以及multi-head attention的理论进行深入分析。</p><hr><h5 id="1、Self-Attention"><a href="#1、Self-Attention" class="headerlink" title="1、Self-Attention"></a>1、Self-Attention</h5><p>论文中的Transformer模型如下图所示：</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231104153901555.png" alt="images"></p><p>下面针对Self-Attention展开说明。为了方便理解， 假设输入的序列长度为22， 输入就是两个节点x1, x2, 然后通过input Embedding也就是图中的f(x)， 将输入映射到a1， a2， 紧接着分别将a1， a2分别通过三个变换矩阵<script type="math/tex">W_q</script>, <script type="math/tex">W_k</script>, <script type="math/tex">W_v</script>（这三个参数是可训练的， 是共享的）， 得到对应的<script type="math/tex">q^i</script>, <script type="math/tex">k^i</script>, <script type="math/tex">v^i</script>, (在源码中，这部分直接使用全连接层实现的， 这里为了方便理解)</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231104154225254.png" alt="images"></p><p>其中， </p><p>q代表query， 后续会和每一个k进行匹配</p><p>k代表key， 后续会被每个qpp</p><p>v代表从a中提取得到的信息</p><p>后续q和k匹配的过程可以理解成计算二者的相关性， 相关性越大， 对应v的权重也越大。</p><p>假设a1 = (1, 1), a2 = (1, 0), <script type="math/tex">W_q=\left(\begin{matrix}  1, 1\\  0, 1 \end{matrix} \right)</script>， 那么， </p><script type="math/tex; mode=display">q^1=(1, 1)\left(\begin{matrix}  1, 1\\  0, 1 \end{matrix} \right) = (1, 2)</script><script type="math/tex; mode=display">q^2=(1, 0)\left(\begin{matrix}  1, 1\\  0, 1 \end{matrix} \right) = (1, 1)</script><p>transformers是可以并行计算的， 所以直接写成</p><script type="math/tex; mode=display">\left(\begin{matrix}  q^1\\  q^2 \end{matrix} \right) = \left(\begin{matrix}  1, 1\\  1, 0 \end{matrix} \right)\left(\begin{matrix}  1, 1\\  0, 1 \end{matrix} \right) = \left(\begin{matrix}  1, 2\\  1, 1 \end{matrix} \right)</script><p>同理可以得到<script type="math/tex">\left(\begin{matrix}  k^1\\  k^2 \end{matrix} \right)</script>和<script type="math/tex">\left(\begin{matrix}  v^1\\  v^2 \end{matrix} \right)</script>，那么求得的<script type="math/tex">\left(\begin{matrix}  q^1\\  q^2 \end{matrix} \right)</script>就是原论文中的Q， <script type="math/tex">\left(\begin{matrix}  k^1\\  k^2 \end{matrix} \right)</script>就是原论文中的K， <script type="math/tex">\left(\begin{matrix}  v^1\\  v^2 \end{matrix} \right)</script>就是原论文中的V。接着拿q1和每个k进行match，  点乘操作， 接着除以<script type="math/tex">\sqrt{d}</script>得到对应的<script type="math/tex">\alpha</script>， 其中d代表向量<script type="math/tex">k^i</script>的长度。 在本实例中等于2， 除以<script type="math/tex">\sqrt{d}</script>的原因在原论文中的解释是”进行点乘后的数值很大， 导致通过softmax后梯度变的很小”, 所以通过除以<script type="math/tex">\sqrt{d}</script>进行缩放， 比如计算<script type="math/tex">\alpha_{1, i}</script>:</p><script type="math/tex; mode=display">\alpha_{1, 1} = \frac{q^1 \cdot k^1}{\sqrt{d}} = \frac{1 \times 1 + 2 \times 0}{\sqrt{2}} = 0.71</script><script type="math/tex; mode=display">\alpha_{1, 2} = \frac{q^1 \cdot k^2}{\sqrt{d}} = \frac{1 \times 0 + 2 \times 1}{\sqrt{2}} = 1.41</script><p>同理拿<script type="math/tex">q^2</script>去匹配所有的k能得到<script type="math/tex">a_{2, i}</script>, 统一写成矩阵乘法形式：</p><script type="math/tex; mode=display">\left(\begin{matrix}  a_{1, 1}, a_{1, 2}\\  a_{2, 1}, a_{2, 2} \end{matrix} \right) = \frac{\left(\begin{matrix}  q^1\\  q^2 \end{matrix} \right)\left(\begin{matrix}  k^1\\  k^2 \end{matrix} \right)^T}{\sqrt{d}}</script><p>然后对每一行即<script type="math/tex">(a_{1, 1}, a_{1, 2})</script>和<script type="math/tex">(a_{2, 1}, a_{2, 2})</script>分别进行softmax处理得到<script type="math/tex">(a_{1, 1}, a_{1, 2})</script>和<script type="math/tex">(\hat{a}_{2, 1}, \hat{a}_{2, 2})</script>, 这里的<script type="math/tex">\hat{\alpha}</script>相当于计算得到针对每个v的权重， 到这里就完成了Attention(Q, K, V)公式中<script type="math/tex">softmax(\frac{QK^T}{\sqrt{d_k}})</script>部分。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231104161800790.png" alt="images"></p><p>上面已经计算得到<script type="math/tex">\alpha</script>， 即针对每个v的权重， 接着进行加权得到最终的结果。</p><script type="math/tex; mode=display">b_1 = \hat{\alpha_{1, 1} \times v^1} + \hat{\alpha_{1, 2} \times v^2} = (0.33, 0.67)</script><script type="math/tex; mode=display">b_2 = \hat{\alpha_{2, 1} \times v^1} + \hat{\alpha_{2, 2} \times v^2} = (0.50, 0.50)</script><p>统一写成矩阵乘法形式</p><script type="math/tex; mode=display">\left(\begin{matrix}  b_1\\  b_2 \end{matrix} \right)=\left(\begin{matrix}  \hat{a_{1, 1}}, \hat{a_{1, 2}}\\  \hat{a_{2, 1}}, \hat{a_{2, 2}} \end{matrix} \right)\left(\begin{matrix}  v^1\\  v^2 \end{matrix} \right)</script><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231104162524348.png" alt="images"></p><p>到这里， Self-Attention的内容就讲完了， 总结下来就是论文的一个公式。</p><script type="math/tex; mode=display">Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><hr><h5 id="2、Multi-Head-Attention"><a href="#2、Multi-Head-Attention" class="headerlink" title="2、Multi-Head Attention"></a>2、Multi-Head Attention</h5><p>下面看一下Multi-Head Attention模块， 实际使用中基本使用的还是Multi-Head Attention模块。原论文中说使用多头注意力机制能够联合来自不同head部分学习到的信息。</p><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. </p><p>首先还是和Self-Attention模块一样，将<script type="math/tex">a_i</script>分别通过<script type="math/tex">W^q, W^k, W^v</script>得到对应的<script type="math/tex">q^i, k^i, v^i</script>， 然后再根据使用的head数据h进一步把得到的<script type="math/tex">q^i, k^i, v^i</script>均分成h份。 比如下图假设h=2， 然后<script type="math/tex">q^1</script>拆分成<script type="math/tex">q^{1, 1}</script>和<script type="math/tex">q^{1, 2}</script>,那么<script type="math/tex">q^{1, 1}</script>就属于head1， <script type="math/tex">q^{1, 2}</script>就属于head2。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231104163517018.png" alt="images"></p><p>这里，如果读过原论文， 会发现论文中写的是：通过<script type="math/tex">W^Q_i, W^K_i, W^V_i</script>映射得到每个head的<script type="math/tex">Q_i, K_i, V_i</script></p><script type="math/tex; mode=display">head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</script><p>但是github上的源码就是简单的进行拆分， 其实也可以将<script type="math/tex">W^Q_i, W^K_i, W^V_i</script>设置成对应值来实现均分，比如下图中的Q通过<script type="math/tex">W_1^Q</script>就能得到均分后的<script type="math/tex">Q_1</script>。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231104164053018.png" alt="images"></p><p>通过上述方法就能得到每个headi对应的<script type="math/tex">Q_i, K_i, V_i</script>， 接下来对每个head使用和Self-Attention相同的方法即可得到对应的结果。</p><script type="math/tex; mode=display">Attention(Q_i, K_i, V_i) = softmax(\frac{Q_iK_i^T}{\sqrt{d_k}})V_i</script><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231104164500863.png" alt="images"></p><p>然后将每个head得到的结果进行concat拼接， 比如下图中<script type="math/tex">b_{1, 1}</script>（head1得到的b1）和<script type="math/tex">b_{1, 2}</script>（head2得到的b1）拼接在一起， <script type="math/tex">b_{2, 1}</script>（head1得到的b2）和<script type="math/tex">b_{2, 2}</script>（head2得到的b2）拼接在一起。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231104164821986.png" alt="images"></p><p>接着将拼接后的结果通过<script type="math/tex">W^O</script>（可学习的参数）进行融合， 融合后得到的结果b1, b2</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231104164948145.png" alt="images"></p><p>到这里，Multi-Head Attention的内容就讲完了， 总结下来就是论文中的来个公式</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231104165049910.png" alt="images"></p><hr><h5 id="3、Self-Attention与Multi-Head-Attention计算量对比"><a href="#3、Self-Attention与Multi-Head-Attention计算量对比" class="headerlink" title="3、Self-Attention与Multi-Head Attention计算量对比"></a>3、Self-Attention与Multi-Head Attention计算量对比</h5><p>原论文中作者说其实二者的计算量差不多。 Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.</p><p>下面做了一个简单的实验。</p><p>首先创建一个Self-Attention模块(单头)a1, 然后把proj变量置为identity(Identity对应的是Multi-Head Attention中最后那个<script type="math/tex">W^O</script>的映射， 单头中是没有， 所以单头中identity表示不做任何操作)</p><p>再创建一个Multi-Head Attention模块(多头)a2， 然后设置8个head</p><p>创建一个随机变量， 注意shape</p><p>使用fvcore分别计算两个模块的FLOPS</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : demo_flops.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> fvcore.nn <span class="keyword">import</span> FlopCountAnalysis</span><br><span class="line"><span class="keyword">from</span> model_vit <span class="keyword">import</span>  Attention</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment"># Self-Attention</span></span><br><span class="line">    a1 = Attention(dim=<span class="number">512</span>, num_heads=<span class="number">1</span>)</span><br><span class="line">    a1.proj = torch.nn.Identity()  <span class="comment"># remove Wo</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Multi-Head Attention</span></span><br><span class="line">    a2 = Attention(dim=<span class="number">512</span>, num_heads=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    t = (torch.rand(<span class="number">32</span>, <span class="number">1024</span>, <span class="number">512</span>))</span><br><span class="line"></span><br><span class="line">    flops1 = FlopCountAnalysis(a1, t)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Self-Attention FLOPs:&quot;</span>, flops1.total())</span><br><span class="line"></span><br><span class="line">    flops2 = FlopCountAnalysis(a2, t)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Multi-Head Attention FLOPs:&quot;</span>, flops2.total())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Self-Attention FLOPs: 60,129,542,144</span></span><br><span class="line"><span class="comment"># Multi-Head Attention FLOPs: 68,719,476,736</span></span><br></pre></td></tr></table></figure><p>从终端输出中发现二者的FLOPs差不多， Multi-Head Attention比Self-Attention略高一点</p><p>但其实二者的差异在最后的Wo上，如果把Multi-Head Attention的Wo也设置为Identity， 可以发现二者的FLOPs相等。</p><hr><h5 id="4、Position-Embedding"><a href="#4、Position-Embedding" class="headerlink" title="4、Position Embedding"></a>4、Position Embedding</h5><p>其实上面讲的Self-Attention和Multi-Head Attention模块，在计算中没有考考位置信息。假设在Self-Attention模块中， 输入a1, a2, a3得到b1, b2, b3, 对于a1而言， a2和a3离它都是一样近的而且没有先后顺序。假设将输入的顺序改为a1， a3， a2， 对结果b1是没有任何影响的。 </p><p>下面使用pytorch做一个实验， 首先创建一个Self-Attention模块， 注意这里在正向传播过程中直接传入QKV, 然后创建两个顺序不同的QKV变量t1和t2， 主要是将qkv的顺序换了以下， 分别将这两个变量输入Self-Attention进行正向传播。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : demo_pos_emb.py</span></span><br><span class="line"><span class="string"># @Time       ：2023/11/4 17:15</span></span><br><span class="line"><span class="string"># @Author     ：0399</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.MultiheadAttention(embed_dim=<span class="number">2</span>, num_heads=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">t1 = [[[<span class="number">1.</span>, <span class="number">2.</span>],  <span class="comment"># q1, k1, v1</span></span><br><span class="line">       [<span class="number">2.</span>, <span class="number">3.</span>],   <span class="comment"># q2, k2, v2</span></span><br><span class="line">       [<span class="number">3.</span>, <span class="number">4.</span>]]]  <span class="comment"># q3, k3, v3</span></span><br><span class="line"></span><br><span class="line">t2 = [[[<span class="number">1.</span>, <span class="number">2.</span>],  <span class="comment"># q1, k1, v1</span></span><br><span class="line">       [<span class="number">3.</span>, <span class="number">4.</span>],   <span class="comment"># q3, k3, v3</span></span><br><span class="line">       [<span class="number">2.</span>, <span class="number">3.</span>]]]  <span class="comment"># q2, k2, v2</span></span><br><span class="line"></span><br><span class="line">q, k, v = torch.as_tensor(t1), torch.as_tensor(t1), torch.as_tensor(t1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;result1: \n&quot;</span>, m(q, k, v))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">q, k, v = torch.as_tensor(t2), torch.as_tensor(t2), torch.as_tensor(t2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;result2: \n&quot;</span>, m(q, k, v))</span><br></pre></td></tr></table></figure><p>对比结果发现改变了顺序， 但是对于b1是没有影响的</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">result1: </span><br><span class="line"> (tensor([[[-0.2706,  0.7209],</span><br><span class="line">         [-0.2261,  1.0880],</span><br><span class="line">         [-0.1816,  1.4551]]], grad<span class="built_in">_</span>fn=&lt;AddBackward0&gt;), </span><br><span class="line">         tensor([[[1.]],</span><br><span class="line">        [[1.]],</span><br><span class="line">        [[1.]]], grad<span class="built_in">_</span>fn=&lt;DivBackward0&gt;))</span><br><span class="line">result2: </span><br><span class="line"> (tensor([[[-0.2706,  0.7209],</span><br><span class="line">         [-0.1816,  1.4551],</span><br><span class="line">         [-0.2261,  1.0880]]], grad<span class="built_in">_</span>fn=&lt;AddBackward0&gt;), </span><br><span class="line">         tensor([[[1.]],</span><br><span class="line">        [[1.]],</span><br><span class="line">        [[1.]]], grad<span class="built_in">_</span>fn=&lt;DivBackward0&gt;))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>为了引入位置信息， 论文中加入了位置编码position embedding， To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. </p><p>如下图所示，位置编码直接加在输入的a = {a1, a2, …an}中， 即pe={pe1, pe2, …pen}和a = {a1, a2, …an}拥有相同的维度大小。关于位置编码， 在论文中提出了两种方案， 一种是固定编码， 即论文中给出的sine and cosine functions方法，按照该方法计算出位置编码， 另外一种是可训练的位置编码， 作者尝试了两种方法发现结果差不多。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231104172450804.png" alt="images"></p><hr><h5 id="5、超参数对比"><a href="#5、超参数对比" class="headerlink" title="5、超参数对比"></a>5、超参数对比</h5><p>原论文中给出了一些超参数，如下表所示</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231104172559286.png" alt="images"></p><p>N表示重复堆叠Transformer Block的次数</p><p>dmodel表示Multi-Head Attention输入输出token的维度</p><p>dff表示在MLP中隐层的节点个数</p><p>h表示head的个数</p><p>dk, dv表示在Multi-Head Attention中每个head的key（K)以及query(Q)的维度</p><p>pdrop表示dropout中的随机失活比例。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;论文原文：&lt;a href=&quot;[a</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>MobileNet</title>
    <link href="https://guudman.github.io/2023/11/04/MobileNet/"/>
    <id>https://guudman.github.io/2023/11/04/MobileNet/</id>
    <published>2023-11-04T01:32:47.000Z</published>
    <updated>2023-11-04T07:08:29.148Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>在之前的文章中讲的AlexNet， VGG， GoogleNet以及ResNet网络， 它们都是传统卷积神经网络（都是使用传统卷积层），缺点是<strong>内存需求大、运算量大而导致无法在移动设备以及嵌入式设备上运行</strong>， 这里要讲的MobileNet网络就是专门为移动端，嵌入式端而设计的。</p><h5 id="1、MobileNetv1"><a href="#1、MobileNetv1" class="headerlink" title="1、MobileNetv1"></a>1、<strong>MobileNetv1</strong></h5><p>MobileNet模型是Google在2017年针对手机或嵌入式提出的轻量级模型， 专注于移动端或嵌入式设备中的轻量级CNN网络。相比于传统卷积神经网络， 在准确率小幅度降低的前提下大大减少模型参数与运算量。(相比VGG16准确率减少了0.9%,但模型参数只有VGG的1/32)</p><p>要说MobileNet网络的优点， 无疑是其中的Depthwise  Convolution结构（大大减少了运算量和参数数量)。下图展示了传统卷积与DW卷积的差异。在传统卷积中， 每个卷积核的channel与输入特征矩阵的channel相等(每个卷积核都会与输入特征矩阵的每一个维度进行卷积运算)。 </p><p>而在DW卷积中， 每个卷积核的channel都是等于1的(每个卷积核只负责输入特征矩阵的一个channel， 故卷积核的个数必须等于输入特征矩阵的channel， 从而使得输出特征矩阵的channel数也等于输入特征矩阵的channel数)</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103154737433.png" alt="image"></p><p>刚刚说了使用DW卷积后输出特征矩阵的channel是与输入特征矩阵的channel是相等的， 如果想改变/自定义输出特征矩阵的channel， 那只需要在DW卷积后接一个PW卷积即可。如下图所示， 其实PW卷积就是普通的卷积而已(只不过卷积核大小为1)。通常DW卷积和PW卷积是放在一起使用的， 一起叫作Depthwise Separable Convolution（深度可分离卷积）</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103155138897.png" alt="image"></p><p>那Depthwise Separable Convolution（深度可分离卷积)与传统的卷积相比到底能节省多少计算量呢？</p><p>下面对比了这两个卷积方式的计算量， 其中Df是输入特征矩阵的宽高（这里假设宽和高相等）, Dk是卷积核的大小， M是输入特征矩阵的channel， N是输出特征矩阵的channel， 卷积计算量近似等于卷积核的高 × 卷积核的宽 × 卷积核的channel ×输入特征矩阵的高 × 输入特征矩阵的宽（假设stride=1）。在mobilenet网络中，DW卷积都是使用3x3大小的卷积核， 所以理论上普通卷积计算量是DW+PW卷积的8到9倍。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103155948407.png" alt="image"></p><p>下面分析一下mobilenet v1的网络结构， 左侧的表格是mobilenet v1的网络结构， 表中Conv表示普通卷积， Conv dw代表上面提到的DW卷积， s表示步距， 根据表格信息就能搭建出mobilenet v1网络。 在mobilenet v1原论文中， 还提出了两个超参数， 一个是<script type="math/tex">\alpha</script>， 一个是<script type="math/tex">\beta</script>,  <script type="math/tex">\alpha</script>参数是一个倍率因子， 用来调整卷积核的个数， <script type="math/tex">\beta</script>是控制输入网络的图像尺寸参数。下图右侧给出了使用不同<script type="math/tex">\alpha</script>和<script type="math/tex">\beta</script>网络的分类准确率、计算量以及模型参数。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103160611687.png" alt="image"></p><h5 id="2、MobileNet-V2"><a href="#2、MobileNet-V2" class="headerlink" title="2、MobileNet V2"></a>2、<strong>MobileNet V2</strong></h5><p>在MobileNet v1的网络结构中能够发现， 网络的结构就像VGG一样是个直筒型的， 不像ResNet网络有shortcut连接方式， 而有人反映说MobileNet v1网络中DW卷积在训练时很容易废掉， 效果并没有那么理想。下一接下来看MobileNet v2网络。 </p><p>MobileNet v2网络是由google团队在2018年提出，相比MobileNet v1网络，准确率更高， 模型更小。 上面提到， MobileNet v1网络的亮点是DW卷积， 那么在MobileNet v2中的亮点是Inverted residual block（倒残差结构）, 如下图所示， 左侧是ResNet网络中的残差结构，右侧是Mobile Net v2中的倒残差结构。在残差结构中是1x1卷积降维-&gt;3x3卷积-&gt;1x1卷积升维， 在倒残差结构中正好相反， 是1x1卷积升维-&gt;3x3卷积-&gt;1x1卷积降维。 </p><p>为什么要这么做， 原文的解释是高维信息通过ReLU激活函数后丢失的信息更少（注意倒残差结构中基本使用的是ReLU6激活函数， 但是最后一个1x1的卷积层使用的是线性激活函数）</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103170530054.png" alt="image"></p><p>在使用倒残差结构时需要注意， 并不是所有的倒残差结构都有shortcut连接， 只有当stride=1且输入特征矩阵与输出特征矩阵shape相同时才有shortcut连接（只有当shape相同时， 两个矩阵才有加法运算， 当stride=1时并不能保证输入特征矩阵的channel与输出特征矩阵的channel相同）</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103171132298.png" alt="image"></p><p>下图是MobileNet v2网络的结构表， 其中t代表的是扩展因子（倒残差结构中第一个1x1卷积的扩展因子）， c代表输出特征矩阵的channel， n代表倒残差结构重复的次数， s代表步距(注意：这里的步距只是针对重复n次的第一层倒残差结构，后面的都默认为1)</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231104091906806.png" alt="image"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;在之前的文章中讲的AlexNet</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>ResNet</title>
    <link href="https://guudman.github.io/2023/11/04/ResNet/"/>
    <id>https://guudman.github.io/2023/11/04/ResNet/</id>
    <published>2023-11-04T01:27:40.000Z</published>
    <updated>2023-11-04T07:08:38.669Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>Resnet由微软实验室于2015年提出， 获得当年ImageNet竞赛分类任务第一名， 目标检测第一名。获得COCO数据集目标检测第一名， 图像分割第一名。</p><p>下图是ResNet34的简图。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103142747004.png" alt="image"></p><p>网络的亮点</p><ul><li><p>超深的网络结构(突破1000层)</p></li><li><p>提出residual模块(残差结构)</p></li><li><p>使用batch normalization加速训练(放弃使用dropout)</p></li></ul><p>在ResNet网络提出之前， 传统的卷积神经网络都是通过一系列卷积层与下采样层进行堆叠得到的， 但是当网络堆叠到一定网络深度时， 就会出现如下两个问题：</p><ol><li>梯度消失或梯度爆炸</li><li>退化问题(degradation problem)</li></ol><p>在ResNet论文中说通过数据的预处理以及在网络中使用BN(batch normalization)层能够解决梯度消失或者梯度爆炸问题。但是对于退化问题（随着网络层数的加深， 效果还会变差）并无很好的解决方法。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103143453837.png" alt="image"></p><p>所以ResNet论文提出了residual结构(残差结构)来减轻退化问题。下图是使用residual结构的卷积网络， 可以看到随着网络的不断加深， 效果并没有变差，反而变得更好了。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103143719018.png" alt="image"></p><p>下面来分析一下论文中的残差结构(residual)。下图是论文中给出的两种残差结构，左边的残差结构是针对层数较少的网络， 例如ResNet18层和ResNet34层网络， 右边是针对网络层数较多的网络， 例如ResNet101, ResNet152等。</p><p>为什么深层网络要用右边的残差结构， 因为右边的残差结构能够减少网络参数与运算量。同样输入一个channel为256的特征矩阵， 如果使用左侧的残差结构大约需要1170648个参数， 但如果使用右侧的残差结构只需要69632个参数，因此在搭建深层网络时， 使用右侧的残差结构更合适。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103144556292.png" alt="image"></p><p>先对左边的残差结构(针对ResNet18/34)进行分析， 如下图所示，该残差结构的主要分支是由两层3×3的卷积层组成， 而残差结构右侧的连接线是shortcut分支也叫做捷径分支(注意， 为了让主分支上的输出矩阵能够与捷径分支上的输出矩阵进行相加，必须保证这两个输出特征矩阵有相同的shape)。</p><p>仔细观察ResNet34网络结构，可以发现图中有一些虚线的残差结构， 在原论文中作者只是简单说这些虚线残差结构具有降维的作用。下图右侧给出了详细的虚线残差结构，注意<strong>每个卷积层的步距stride以及捷径分支上的卷积核的个数(与主分支上的卷积核个数相同)</strong></p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103144740973.png" alt="image"></p><p>接着再来分析针对ResNet50/101/152的残差结构， 如下图所示 ， 在该残差结构中，主分支使用了三个卷积层， 第一个是1x1的卷积层， 用来压缩channel维度， 第二个是3x3的卷积层， 第三个是1x1的卷积层用来还原channel维度（注意主分支上第一层卷积层和第二层卷积层所使用的卷积核个数是相同的，第三层是第一层的4倍)。该残差结构所对应的虚线残差结构如右侧图所示， 同样在捷径分支上有一个1x1的卷积层，它的卷积核个数与主分支上的第三层卷积核个数相同，注意每个卷积层的步距。<strong>(注意： 原论文中， 在下图右侧虚线残差结构的主分支上， 第一个1×1卷积层的步距是2， 第二个3x3卷积层的步距是1。但是在pytorch官方实现过程中第一个1x1卷积层的步距是1， 第二个3x3卷积层步距是2， 这样做的好处是能够在top1上提升大概0.5%的准确率。可参考Resnet v1.5  <a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch">https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch</a>)</strong> </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103145804618.png" alt="image"></p><p>pytorch官方说明</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103150713341.png" alt="image"></p><p>下表是原论文给出的不同深度的ResNet网络结构配置， 注意表中的残差结构给出了主分支上卷积核的大小与卷积核个数， 表中xN表示该残差结构重复N次</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103150928585.png" alt="image"></p><p>那到底哪些残差结构是虚线残差结构呢？</p><p>对于ResNet18/34/50/101/151, 表中conv3_x, conv4_x, conv5_x所对应的一系列残差结构的<strong>第一层残差结构都是虚线残差结构</strong>。引文这一系列残差结构的第一层都有调整输入特征矩阵shape的使命(将特征矩阵的高和宽缩减为原来的一半， 将深度channel调整成下一层残差结构所需的channel)。 </p><p>为了方便理解， 下面给出了ResNet34的网络结构图， 图中简单标注了一些信息。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231103151759454.png" alt="image"></p><p>对于ResNet50/101/152，其实在conv2_x所对应的一系列残差结构的第一层也是虚线残差结构， 因为它需要调整输入特征矩阵的channel， 根据表格可知， 通过3x3的maxpool之后输出的特征矩阵的shape应该是[56, 56, 64],但是conv2_x所对应的一系列残差结构中实线残差结构的期望输入特征矩阵的shape是[56, 56, 256], (因为这样才能保证输入输出特征矩阵shape相同，才能将捷径分支的输出与主分支的输出进行相加)。所以第一层残差结构需要将shape从[56, 56, 64]调整为-&gt;[56, 56, 256]。注意，这里只调整channel维度， 高和宽不变(而conv3_x, conv4_x, conv5_x所对饮的一系列残差结构的第一层虚线残差结构不仅要调整channel，还要将高度和宽度缩减为原来的一半。）</p><h4 id="2、实现"><a href="#2、实现" class="headerlink" title="2、实现"></a>2、实现</h4><h5 id="1、pytorch实现"><a href="#1、pytorch实现" class="headerlink" title="1、pytorch实现"></a>1、pytorch实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_resnet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    注意：在原论文中，在虚线残差结构的主分支上， 第一个1x1卷积层的strid是2，第二个3x3卷积层的stride是1。</span></span><br><span class="line"><span class="string">    但是在pytorch官方实现过程中是第一个1x1卷积层的stride1， 第二个3x3卷积层的stride是2</span></span><br><span class="line"><span class="string">    这么做的好处是能够在top1上提升大约0.5%的准确率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                groups=<span class="number">1</span>, width_per_group=<span class="number">64</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line">        width = <span class="built_in">int</span>(out_channel * (width_per_group / <span class="number">64.</span>)) * groups</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,</span><br><span class="line">                               groups=groups, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>,</span><br><span class="line">                               bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(width)</span><br><span class="line">        <span class="comment"># -----------------------------------------------</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=width, out_channels=width,</span><br><span class="line">                               groups=groups, kernel_size=<span class="number">3</span>, stride=stride,</span><br><span class="line">                               bias=<span class="literal">False</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(width)</span><br><span class="line">        <span class="comment"># -----------------------------------------------</span></span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel * self.expansion,</span><br><span class="line">                               kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block,</span></span><br><span class="line"><span class="params">                 block_num,</span></span><br><span class="line"><span class="params">                 num_classes=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 include_top=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 width_per_group=<span class="number">64</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">        self.include_top = include_top</span><br><span class="line">        self.in_channel = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">        self.groups = groups</span><br><span class="line">        self.width_per_group = width_per_group</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=self.in_channel,</span><br><span class="line">                               kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(self.in_channel)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.layer1 = self._make_layer(block, <span class="number">64</span>, block_num[<span class="number">0</span>])</span><br><span class="line">        self.layer2 = self._make_layer(block, <span class="number">128</span>, block_num[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer(block, <span class="number">256</span>, block_num[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer(block, <span class="number">512</span>, block_num[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> self.include_top:</span><br><span class="line">            self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># output size = (1, 1)</span></span><br><span class="line">            self.fc = nn.Linear(<span class="number">512</span> * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, channel, block_num, stride=<span class="number">1</span></span>):</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.in_channel != channel * block.expansion:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.in_channel, channel * block.expansion,</span><br><span class="line">                          kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(channel * block.expansion))</span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.in_channel,</span><br><span class="line">                            channel,</span><br><span class="line">                            downsample=downsample,</span><br><span class="line">                            stride=stride,</span><br><span class="line">                            groups=self.groups,</span><br><span class="line">                            width_per_group=self.width_per_group))</span><br><span class="line">        self.in_channel = channel * block.expansion</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num):</span><br><span class="line">            layers.append(block(self.in_channel,</span><br><span class="line">                                channel,</span><br><span class="line">                                groups=self.groups,</span><br><span class="line">                                width_per_group=self.width_per_group))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line">        <span class="keyword">if</span> self.include_top:</span><br><span class="line">            x = self.avgpool(x)</span><br><span class="line">            x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">            x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet34</span>(<span class="params">num_classe=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet34-333f7ec4.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(BasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classe,</span><br><span class="line">                  include_top=include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet50-19c8e357.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet101</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet101-5d3b4d8f.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50_32x4d</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth</span></span><br><span class="line">    groups = <span class="number">32</span></span><br><span class="line">    width_per_group = <span class="number">4</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top,</span><br><span class="line">                  groups=groups,</span><br><span class="line">                  width_per_group=width_per_group)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50_32x8d</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth</span></span><br><span class="line">    groups = <span class="number">32</span></span><br><span class="line">    width_per_group = <span class="number">8</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top,</span><br><span class="line">                  groups=groups,</span><br><span class="line">                  width_per_group=width_per_group)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.rand((<span class="number">4</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">resnet = resnet101(<span class="number">5</span>)</span><br><span class="line">out = resnet(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2、TensorFlow实现"><a href="#2、TensorFlow实现" class="headerlink" title="2、TensorFlow实现"></a>2、TensorFlow实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_resnet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：0399</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, Model, Sequential</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(layers.Layer):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_channel, strides=<span class="number">1</span>, downsample=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = layers.Conv2D(out_channel, kernel_size=<span class="number">3</span>, strides=strides,</span><br><span class="line">                                   padding=<span class="string">&quot;SAME&quot;</span>, use_bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>)</span><br><span class="line">        <span class="comment"># ----------------------------------------</span></span><br><span class="line">        self.conv2 = layers.Conv2D(out_channel, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                                   padding=<span class="string">&quot;SAME&quot;</span>, use_bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>)</span><br><span class="line">        <span class="comment"># ----------------------------------------</span></span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.relu = layers.ReLU()</span><br><span class="line">        self.add = layers.Add()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">False</span></span>):</span><br><span class="line">        identity = inputs</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(inputs)</span><br><span class="line"></span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        x = self.bn1(x, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x, training=training)</span><br><span class="line"></span><br><span class="line">        x = self.add([x, identity])</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottlenect</span>(layers.Layer):</span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_channel, strides=<span class="number">1</span>, downsample=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottlenect, self).__init__()</span><br><span class="line">        self.conv1 = layers.Conv2D(out_channel, kernel_size=<span class="number">1</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&quot;conv1&quot;</span>)</span><br><span class="line">        self.bn1 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&quot;conv1/BatchNorm&quot;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------</span></span><br><span class="line">        self.conv2 = layers.Conv2D(out_channel, kernel_size=<span class="number">3</span>, use_bias=<span class="literal">False</span>,</span><br><span class="line">                                   strides=strides, padding=<span class="string">&quot;SAME&quot;</span>, name=<span class="string">&quot;conv2&quot;</span>)</span><br><span class="line">        self.bn2 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&quot;conv2/BatchNorm&quot;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------</span></span><br><span class="line">        self.conv3 = layers.Conv2D(out_channel * self.expansion, kernel_size=<span class="number">1</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&quot;conv3&quot;</span>)</span><br><span class="line">        self.bn3 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&quot;conv3/BatchNorm&quot;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------</span></span><br><span class="line">        self.relu = layers.ReLU()</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.add = layers.Add()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">False</span></span>):</span><br><span class="line">        identity = inputs</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(inputs)</span><br><span class="line"></span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        x = self.bn1(x, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.bn3(x, training=training)</span><br><span class="line">        x = self.add([identity, x])</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">block, in_channel, channel, block_num, name, strides=<span class="number">1</span></span>):</span><br><span class="line">    downsample = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> strides != <span class="number">1</span> <span class="keyword">or</span> in_channel != channel * block.expansion:</span><br><span class="line">        downsample = Sequential([</span><br><span class="line">            layers.Conv2D(channel * block.expansion, kernel_size=<span class="number">1</span>, strides=strides,</span><br><span class="line">                          use_bias=<span class="literal">False</span>, name=<span class="string">&quot;conv1&quot;</span>),</span><br><span class="line">            layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1.001e-5</span>, name=<span class="string">&quot;BatchNorm&quot;</span>)</span><br><span class="line">        ], name=<span class="string">&quot;shortcut&quot;</span>)</span><br><span class="line">    layers_list = []</span><br><span class="line">    layers_list.append(block(channel, downsample=downsample, strides=strides, name=<span class="string">&quot;unit_1&quot;</span>))</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num):</span><br><span class="line">        layers_list.append(block(channel, name=<span class="string">&quot;unit_&quot;</span> + <span class="built_in">str</span>(index + <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">return</span> Sequential(layers_list, name=name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_resnet</span>(<span class="params">block, block_num, im_width, im_height, num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># TensorFlow中tensor的通道顺序 NHWC</span></span><br><span class="line">    input_image = layers.Input(shape=(im_height, im_width, <span class="number">3</span>), dtype=<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;SAME&quot;</span>,</span><br><span class="line">                      use_bias=<span class="literal">False</span>, name=<span class="string">&quot;conv1&quot;</span>)(input_image)</span><br><span class="line">    x = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&quot;conv1/BatchNorm&quot;</span>)(x)</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;SAME&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = _make_layer(block, x.shape[-<span class="number">1</span>], <span class="number">64</span>, block_num[<span class="number">0</span>], name=<span class="string">&quot;block1&quot;</span>)(x)</span><br><span class="line">    x = _make_layer(block, x.shape[-<span class="number">1</span>], <span class="number">128</span>, block_num[<span class="number">1</span>], strides=<span class="number">2</span>, name=<span class="string">&quot;block2&quot;</span>)(x)</span><br><span class="line">    x = _make_layer(block, x.shape[-<span class="number">1</span>], <span class="number">256</span>, block_num[<span class="number">2</span>], strides=<span class="number">2</span>, name=<span class="string">&quot;block3&quot;</span>)(x)</span><br><span class="line">    x = _make_layer(block, x.shape[-<span class="number">1</span>], <span class="number">512</span>, block_num[<span class="number">3</span>], strides=<span class="number">2</span>, name=<span class="string">&quot;block4&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> include_top:</span><br><span class="line">        x = layers.GlobalAvgPool2D()(x)</span><br><span class="line">        x = layers.Dense(num_classes, name=<span class="string">&quot;logits&quot;</span>)(x)</span><br><span class="line">        predict = layers.Softmax()(x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        predict = x</span><br><span class="line">    model = Model(inputs=input_image, outputs=predict)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet34</span>(<span class="params">im_width=<span class="number">224</span>, im_height=<span class="number">224</span>, num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">return</span> _resnet(BasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], im_height, im_width, num_classes, include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50</span>(<span class="params">im_width=<span class="number">224</span>, im_height=<span class="number">224</span>, num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">return</span> _resnet(Bottlenect, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], im_height, im_width, num_classes, include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet101</span>(<span class="params">im_width=<span class="number">224</span>, im_height=<span class="number">224</span>, num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">return</span> _resnet(Bottlenect, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], im_height, im_width, num_classes, include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="built_in">input</span> = tf.random.uniform((<span class="number">8</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line">model = resnet34()</span><br><span class="line"><span class="built_in">print</span>(model(<span class="built_in">input</span>))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;Resnet由微软实验室于201</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>BatchNormalization</title>
    <link href="https://guudman.github.io/2023/11/04/BatchNormalization/"/>
    <id>https://guudman.github.io/2023/11/04/BatchNormalization/</id>
    <published>2023-11-04T01:22:37.000Z</published>
    <updated>2023-11-04T07:25:30.087Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>BatchNormalization原文： <a href="[arxiv.org/pdf/1502.03167.pdf](https://arxiv.org/pdf/1502.03167.pdf">BN原论文</a>)</p><p>Batch Normalization是google团队在2015年提出的， 该方法能够加速网络的收敛并提高准确率。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102153151497.png" alt="image"></p><p>本文分为以下几个部分：</p><ol><li>BN的原理</li><li>使用pytorch验证本文观点</li><li>BN使用注意事项</li></ol><h4 id="2、Batch-Normalization原理"><a href="#2、Batch-Normalization原理" class="headerlink" title="2、Batch Normalization原理"></a>2、Batch Normalization原理</h4><p>在图像预处理中通常会对图像进行标准化处理， 这样能够加速网络的收敛， 对于Conv1来说， 输入就是满足某一分布的特征矩阵， 但是对Conv2而言的feature map就不一定满足某一分布规律了(<strong>注意这里所说的满足某一分布规律并不是指某一个feature map的数据要满足分布规律， 理论上指整个训练样本集所对应的feature map的数据要满足分布规律</strong>)。而我们的Batch Normalization的目的就是使我们的feature map满足均值为0，方差为1的分布规律。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102161021114.png" alt="image"></p><p>下面是从原论文中截取的原话</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102163727872.png" alt="image"></p><p>对于一个拥有d维的输入x， 我们将对它的每一个维度进行标准化处理， 假设我们输入的x是RGB三通道的彩色图像，这里的d就是输入图像的channels即d=3， <script type="math/tex">x=(x^{(1)}, x^{(2)}, x^{(3)})</script>, 其中<script type="math/tex">x^{(1)}</script>代表的就是R通道对应的特征矩阵， 以此类推。标准化处理也是<strong>分别</strong>对我们的R通道， G通道， B通道进行处理。</p><p>原文中提供了更加相似的计算公式。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102164825714.png" alt="image"></p><p>上面提到让<strong>feature map满足某一分布规律， 理论上指整个训练样本集所对应的feature map的数据要满足分布规律</strong>，也就是说要算出整个训练集的feature map然后再进行标准化处理。对于一个大型的数据集明显是不可能的，所以论文中说的是batch normalization， 也就是我们计算一个batch数据的feature map然后再进行标准化(batch越大越接近整个数据集的分布， 效果越好) .</p><p>根据上面的公式可以知道<script type="math/tex">\mu_\beta</script>代表计算的feature map每个维度（channel)的均值，<strong>注意<script type="math/tex">\mu_\beta</script>是一个向量，而不是一个值</strong>， <script type="math/tex">\mu_\beta</script>向量的每个元素代表一个维度(channel)的均值。<script type="math/tex">\sigma^2_\beta</script>代表计算的feature map每个维度(channels)的方差， <strong><script type="math/tex">\sigma^2_\beta</script>是一个向量， 而不是一个值</strong>， <script type="math/tex">\sigma^2_\beta</script>向量的每一个元素代表一个维度(channel)的方差， 然后根据<script type="math/tex">\mu_\beta</script>和<script type="math/tex">\sigma^2_\beta</script>计算标准化处理得到的值。下图给出了一个计算<script type="math/tex">\mu_\beta</script>和<script type="math/tex">\sigma^2_\beta</script>的示例。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102171333525.png" alt="image"></p><script type="math/tex; mode=display">x^{(1)}=\{1, 1, 1, 2, 0, -1, 2, 2\}$$  序列(1, 1, 1, 2, 0, -1, 2, 2)的均值为1， 方差为1$$x^{(2)}=\{-1, 1, 0, 1, 0, -1, 3, 2\}$$ 序列(-1, 1, 0, 1, 0, -1, 3, 2)的均值为0.5， 方差为1.5$$\mu_1=\frac{1}{m}\sum_{i=1}^{m}x^{(1)}_i=1$$            $$\sigma^2_1=\frac{1}{m}\sum_{i=1}^{m}(x^{(1)}_i - \mu_1)^2=1</script><script type="math/tex; mode=display">\mu_2=\frac{1}{m}\sum_{i=1}^{m}x^{(2)}_i=0.5$$            $$\sigma^2_2=\frac{1}{m}\sum_{i=1}^{m}(x^{(2)}_i - \mu_2)^2=1.5</script><p>所以可以得出</p><script type="math/tex; mode=display">\mu=\left[\begin{matrix}  1\\  0.5 \end{matrix} \right]</script><script type="math/tex; mode=display">\sigma^2=\left[\begin{matrix}  1\\  1.5 \end{matrix} \right]</script><p>上面的示例展示了一个batch为2(两张图片)的Batch Normalization的计算过程， 假设feature1， feature2分别是由image1、image2经过一系列卷积池化后的得到的特征矩阵， feature的channel为2， 那么<script type="math/tex">x^{(1)}</script>代表该batch的所有feature的channel1的数据， 同理<script type="math/tex">x^{(2)}</script>代表该batch的所有feature的channel2的数据。然后分别计算<script type="math/tex">x^{(1)}</script>和<script type="math/tex">x^{(2)}</script>的均值和方差， 得到<script type="math/tex">\mu</script>和<script type="math/tex">\sigma^2</script>两个向量。</p><p>然后再根据标准差计算公式<strong>分别</strong>计算每个channel的值(公式中的<script type="math/tex">\varepsilon</script>是一个很小的常量， 防止分母为零的情况)。 </p><p>batch normalization之后，每个元素的计算公式为：</p><script type="math/tex; mode=display">x'_i=\frac{x_i - \mu}{\sqrt{\sigma^2 + \varepsilon}}</script><p>网络训练过程中， 通过一个batch一个batch的数据进行训练， 但是在预测过程中通常是输入一张图片进行预测，因此预测是batch size=1， 如果再通过上述方法计算均值和方差就没有意义了。</p><p>所以在训练过程中要去不断地计算每个batch的均值和方差， 并使用移动平均(moving average)的方法记录统计的均值和方差。在训练完成后我们可以近似认为所统计的均值和方法就等于整个训练集的均值和方差。然后在验证以及预测过程中， 就使用统计得到的均值和方差进行标准化处理。</p><p>其实还可以发现论文中还有<script type="math/tex">\gamma</script>和<script type="math/tex">\beta</script>两个参数， <script type="math/tex">\gamma</script>用来调整数值分布的方差大小， <script type="math/tex">\beta</script>用来调整数据均值的位置。这两个参数是在反向传播过程中学习得到的， <script type="math/tex">\gamma</script>默认为1， <script type="math/tex">\beta</script>默认为0。</p><h4 id="2、使用pytorch进行试验"><a href="#2、使用pytorch进行试验" class="headerlink" title="2、使用pytorch进行试验"></a>2、使用pytorch进行试验</h4><p>上面提到，在训练过程中， 均值<script type="math/tex">\mu_{now}</script>和方差<script type="math/tex">\sigma^2_{now}</script>是通过计算当前批次数据得到的,而在<strong>验证和预测</strong>过程中使用的均值和方差是一个统计量，记为<script type="math/tex">\mu_{statistic}</script>和<script type="math/tex">\sigma^2_{statistic}</script>。</p><script type="math/tex; mode=display">\mu_{statistic}$$和$$\sigma^2_{statistic}$$的更新策略如下， 其中momentum默认取0.1</script><p>\mu_{statistic + 1} = (1 - momentum) <em> \mu_{statistic} + momentum </em> \mu_{now} </p><script type="math/tex; mode=display"></script><p>\sigma^2_{statistic + 1} = (1 - momentum)<em> \sigma^2_{statistic} + momentum</em>\sigma^2_{statistic}</p><script type="math/tex; mode=display">这里要注意， 在pytorch中对当前批次feature进行bn处理时使用的$$\sigma^2_{now}$$是**总体标准差**， 计算公式如下：</script><p>\sigma^2_{now} = \frac{1}{m}\sum_{i=1}^{m}(x_i - \mu_{now})^2</p><script type="math/tex; mode=display">在更新统计量$$\sigma^2_{statistic}$$时采用的$$\sigma^2_{now}$$是**样本标准差**, 计算公式如下：</script><p>\sigma^2_{now} = \frac{1}{m-1}\sum_{i=1}^{m}(x_i - \mu_{now})^2</p><script type="math/tex; mode=display">下面是使用pytorch做的测试， bn_process函数是自定义的bn处理方法， 验证是否和使用官方bn处理方法结果一致。在bn_process中计算输入batch数据的每个维度(这里的维度是channel维度)的均值和标准差(标准差等于方差开平方)。然后通过计算得到的均值和**总体标准差**对feature每个维度进行标准化， 然后使用均值和**样本标准差**更新计算均值和标准差。初始化统计均值是一个元素为0的向量， 元素个数等于channel的深度， 初始化统计方差是一个元素为1的向量， 元素个数等于channel的深度， 初始化为$$\gamma=1, \beta=0</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : pytorch_batch_normalization.py</span></span><br><span class="line"><span class="string"># @Time       ：2023/11/3 11:13</span></span><br><span class="line"><span class="string"># @Author     ：0399</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bn_process</span>(<span class="params">feature, mean, var</span>):</span><br><span class="line">    feature_shape = feature.shape</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(feature_shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="comment"># [batch, channel, height, width]</span></span><br><span class="line">        feature_t = feature[:, i, :, :]</span><br><span class="line">        mean_t = feature_t.mean()</span><br><span class="line">        <span class="comment"># 总体标准差</span></span><br><span class="line">        std_t1 = feature_t.std()</span><br><span class="line">        <span class="comment"># 样本标准差  当ddof=1时，计算的是样本的标准差</span></span><br><span class="line">        std_t2 = feature_t.std(ddof=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># bn process</span></span><br><span class="line">        <span class="comment"># 这里记得加上eps和pytorch保持一致</span></span><br><span class="line">        feature[:, i, :, :] = (feature[:, i, :, :] - mean_t) / np.sqrt(std_t1 ** <span class="number">2</span> + <span class="number">1e-5</span>)</span><br><span class="line">        <span class="comment"># update calculating mean and var</span></span><br><span class="line">        mean[i] = mean[i] * <span class="number">0.9</span> + mean_t * <span class="number">0.1</span></span><br><span class="line">        var[i] = var[i] * <span class="number">0.9</span> + (std_t2 ** <span class="number">2</span>) * <span class="number">0.1</span></span><br><span class="line">    <span class="built_in">print</span>(feature)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成一个batch为2， channel为2， height, width均为2的特征向量</span></span><br><span class="line">feature1 = torch.randn(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 初始化均值和方差</span></span><br><span class="line">calculate_mean = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">calculate_var = [<span class="number">1.0</span>, <span class="number">1.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意使用copy()深拷贝</span></span><br><span class="line">bn_process(feature1.numpy().copy(), calculate_mean, calculate_var)</span><br><span class="line"></span><br><span class="line">bn = nn.BatchNorm2d(<span class="number">2</span>, eps=<span class="number">1e-5</span>)</span><br><span class="line">output = bn(feature1)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># feature</span></span><br><span class="line"><span class="string">[[[[ 0.648684   -0.85507095]</span></span><br><span class="line"><span class="string">   [ 0.58417344  1.8898206 ]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[ 0.97886676  1.3034139 ]</span></span><br><span class="line"><span class="string">   [-1.1044223  -0.48206437]]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[[-0.20273271  0.27278942]</span></span><br><span class="line"><span class="string">   [-1.3778524  -0.95981157]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[ 1.0157013  -1.6016374 ]</span></span><br><span class="line"><span class="string">   [-0.43428668  0.324429  ]]]]</span></span><br><span class="line"><span class="string"># output</span></span><br><span class="line"><span class="string">tensor([[[[ 0.6487, -0.8551],</span></span><br><span class="line"><span class="string">          [ 0.5842,  1.8898]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 0.9789,  1.3034],</span></span><br><span class="line"><span class="string">          [-1.1044, -0.4821]]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[[-0.2027,  0.2728],</span></span><br><span class="line"><span class="string">          [-1.3779, -0.9598]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 1.0157, -1.6016],</span></span><br><span class="line"><span class="string">          [-0.4343,  0.3244]]]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>结果明显一样，只是精度不同。</p><h4 id="4、使用BN时需要注意的问题"><a href="#4、使用BN时需要注意的问题" class="headerlink" title="4、使用BN时需要注意的问题"></a>4、使用BN时需要注意的问题</h4><p>训练时training参数设置为true， 验证时设置为false。在pytorch中可通过创建模型的model.train()和model.eval()方法控制。</p><p>batch size尽可能设置大一点， 设置很小后表现很糟糕， 设置越大，求解出来的均值和方差越接近整个训练集的均值和方差。</p><p>建议将bn层凡在conv和激活层之间，且卷积层不要使用偏置bias， 因为设置了偏置，最后的结果也一样。</p><script type="math/tex; mode=display">y_i = \frac{x_i - \mu(x)}{\sqrt{\sigma^2(x)}}</script><script type="math/tex; mode=display">y_i^b = \frac{x_i^b - \mu(x^b)}{\sqrt{\sigma^2(x^b)}}</script><script type="math/tex; mode=display">x_i^b = x_i + b</script><script type="math/tex; mode=display">\mu(x^b) = \mu(x) + b</script><script type="math/tex; mode=display">\sigma^2(x^b) = \frac{1}{m}\sum_{i=1}^{m}[x_i^b-\mu(x^b)]^2</script><script type="math/tex; mode=display">= \frac{1}{m}\sum_{i=1}^{m}[x_i + b - \mu(x) - b]^2</script><script type="math/tex; mode=display">  =\frac{1}{m}\sum_{i=1}^{m}[x_i - \mu(x)]^2</script><script type="math/tex; mode=display">  =\sigma^2(x)</script><script type="math/tex; mode=display">y_i^b = \frac{x_i^b - \mu(x^b)}{\sqrt{\sigma^2(x^b)}}=\frac{x_i + b - \mu(x) - b}{\sqrt{\sigma^2(x^b)}} = y_i</script>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;BatchNormalizati</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的卷积</title>
    <link href="https://guudman.github.io/2023/11/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF/"/>
    <id>https://guudman.github.io/2023/11/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF/</id>
    <published>2023-11-02T12:07:13.000Z</published>
    <updated>2023-11-04T07:08:11.984Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="深度学习中什么是卷积，-它是如何工作的"><a href="#深度学习中什么是卷积，-它是如何工作的" class="headerlink" title="深度学习中什么是卷积， 它是如何工作的"></a>深度学习中什么是卷积， 它是如何工作的</h4><h5 id="1、数学物理层面"><a href="#1、数学物理层面" class="headerlink" title="1、数学物理层面"></a>1、数学物理层面</h5><p>数学上，连续卷积的定义如下：</p><p>(f <em> g)(t) = ∫[f(τ) </em> g(t - τ)] dτ</p><p>其中，<em> 表示卷积操作，f(t) 和 g(t) 是两个函数，(f </em> g)(t) 是它们的卷积结果。</p><p>它的物理含义可以理解为：系统某一时刻的输出是由多个输入共同作用（叠加）的结果。</p><p>具体解释如下：</p><p>假设有两个物理量（例如信号，场或系统响应等），可用公式中的函数f(t) 和 g(t)表示。在物理上，卷积大概可以理解为：系统某一时刻的输出是由多个输入共同作用(叠加)的结果。</p><p>g(t) 是系统的响应函数（或滤波器）。它描述了系统对输入信号 f(t) 的响应方式。g(t) 中的每一个值表示了在时间 t 时，系统对输入信号的加权响应。</p><p>在卷积过程中，我们考虑了 f(t) 中的每个时间点 τ，并将其与 g(t - τ) 相乘。这相当于在时间轴上对 g(t - τ) 进行了平移，然后与 f(t) 相乘。这个过程捕捉了在不同时间点上，系统响应函数 g(t) 对输入信号 f(t) 的影响。</p><p>最后，通过对所有时间点的乘积进行积分，我们将所有加权的响应值进行累加。这就得到了在每个时间点 t 上的卷积结果 (f * g)(t)。该结果描述了输入信号 f(t) 通过系统响应函数 g(t) 后的响应情况。</p><p>因此，卷积公式提供了一种描述系统响应、信号传播和叠加效应的数学工具。它在物理学中被广泛应用，例如信号处理、系统响应分析、场的传播和滤波器设计等领域。</p><h5 id="2、图像层面"><a href="#2、图像层面" class="headerlink" title="2、图像层面"></a>2、图像层面</h5><p>在深度学习中，卷积层是神经网络中一种常用的操作，它使用卷积运算对输入数据进行特征提取。卷积层的数学公式可以通过以下方式解释：</p><p>假设输入数据是一个三维张量，具有形状为 [H, W, C]，其中 H 表示高度，W 表示宽度，C 表示通道数。卷积层使用一组称为卷积核（或过滤器）的权重来对输入进行卷积操作。</p><p>假设卷积核的形状为 [FH, FW, C, K]，其中 FH 和 FW 表示卷积核的高度和宽度，C 表示输入的通道数，K 表示卷积核的数量（也称为输出通道数）。一般而言，在图像领域，卷积核的高度和宽度相等，常见的设置有3x3、5x5、7x7等。</p><p>卷积操作的数学公式可以表示为：</p><p>输出特征图的某个位置的数值 = sum(输入特征图的对应位置 * 卷积核的权重) + 偏置</p><p>具体而言，对于输出特征图的每个位置 (i, j, k)，其中 i 表示高度索引，j 表示宽度索引，k 表示通道索引，卷积操作可以表示为：</p><p>输出特征图[i, j, k] = sum(sum(sum(输入特征图[m, n, c] * 卷积核[m, n, c, k]))) + 偏置[k]</p><p>其中，m 和 n 表示卷积核的高度和宽度索引，c 表示输入特征图的通道索引。</p><p>这个公式说明了卷积操作的过程。在每个位置 (i, j, k)，输入特征图与卷积核的对应位置进行逐元素相乘，然后将所有乘积相加。这相当于对输入特征图的局部区域与卷积核进行加权叠加，得到输出特征图的对应位置。</p><p>最后，可以通过添加偏置项来调整输出特征图的整体偏移量。偏置项在公式中表示为偏置[k]，其中 k 表示输出通道索引。</p><p>下面是一个大小为3x3的卷积核，步长为1的二维卷积示意图：</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/1686225397566-562949ca-4f3c-406e-886d-2f002a45b604.gif" alt="image"></p><p>可以看到3x3的卷积核以滑动窗口的形式在输入通道上滑动，并通过某种计算得到上面的输出值，每一个卷积核在一个位置会得到一个输出值，按照卷积核的移动顺序形成的输出矩阵。这是最直观的卷积形式。</p><p>具体计算的过程如下所示：</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/1686225862279-5bd2416b-b451-42dd-b5ea-98c5a8fbc9d8.png" alt="image"></p><p>卷积核在原始输入矩阵所覆盖区域对应元素相乘然后相加。</p><p>对于一个图片而言，一般的图片是三通道图片，即包含RGB三个通道，对一个图片的卷积计算，它是如何完成的呢？</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/1686226136566-f82570d7-9a1e-4c36-aad3-f256de218fe0.gif" alt="image"></p><p>具体计算过程解释如下：</p><p>首先将三通道图片切分成三个单通道，这里标记为B、G、R三个通道。B、G、R三个通道分别与对应的卷积核进行卷积计算，然后再通过偏置叠加在一起。这个就得到了三通道图片的卷积。</p><h5 id="3、卷积中的参数"><a href="#3、卷积中的参数" class="headerlink" title="3、卷积中的参数"></a>3、卷积中的参数</h5><p>以一个3x3卷积核为例，该卷积核中有9个参数，那么这9个参数是怎么来的呢？卷积核的个数又是什么呢？</p><p>其实卷积核中有9个参数也是作为超参数通过模型训练得到的，一个训练好的卷积核可以更好的提取图像的特征。</p><p>常见的初始化方法可以为恺明初始化。</p><p>如下是pytorch中搭建模型时常用的初始化方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">            nn.init.kaiming_normal_(m.weight, mode=<span class="string">&quot;fan_out&quot;</span>, nonlinearity=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">            nn.init.normal(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>对单通道的图片卷积过程，卷积核的个数，其实就是卷积之后输出通道的个数。而对于三通道的卷积过程，卷积核的个数等于输出通道数*输入通道数。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;深度学习中什么是卷积，-它是如何工作的&quot;&gt;&lt;a href=&quot;#深度学习中什么是卷积，-它是如何工作的&quot; class=&quot;headerlink&quot; title=&quot;深度学习中什么是</summary>
      
    
    
    
    
    <category term="卷积" scheme="https://guudman.github.io/tags/%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>pytorch全连接层模拟回归</title>
    <link href="https://guudman.github.io/2023/11/02/pytorch%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E6%A8%A1%E6%8B%9F%E5%9B%9E%E5%BD%92/"/>
    <id>https://guudman.github.io/2023/11/02/pytorch%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E6%A8%A1%E6%8B%9F%E5%9B%9E%E5%BD%92/</id>
    <published>2023-11-02T11:58:58.000Z</published>
    <updated>2023-11-04T07:08:35.428Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h4><p>搭建两层全连接网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>预测的值画成曲线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : 301_regression.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class="string">&quot;TRUE&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x data (tensor), shape=(100, 1)</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># noisy y data (tensor), shappe(100, 1)</span></span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.2</span> * torch.rand(x.size())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch can only train on Variable, so convert them to Variable</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">1</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 针对回归的均方误差</span></span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line">plt.ion()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">    prediction = net(x)</span><br><span class="line">    loss = loss_func(prediction, y)</span><br><span class="line">    <span class="comment"># clear gradients for next train</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># backpropagation, comupte gradients</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># apply gradients</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        plt.cla()</span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        <span class="comment"># 绘制预测的值</span></span><br><span class="line">        plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">&#x27;r-&#x27;</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">&#x27;Loss=%.4f&#x27;</span> % loss.data.numpy(), fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>: <span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        <span class="comment"># plt.pause(0.1)</span></span><br><span class="line">        <span class="comment"># plt.ioff()</span></span><br><span class="line">        <span class="comment"># 保存为jpg图像</span></span><br><span class="line">        plt.savefig(<span class="string">f&#x27;./img/regression_<span class="subst">&#123;t&#125;</span>.jpg&#x27;</span>)</span><br><span class="line">        <span class="comment"># plt.show()</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>jgp图像转gif动图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : jpg_2_gif.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    root_path = <span class="string">&#x27;../img&#x27;</span></span><br><span class="line">    image_list = os.listdir(root_path)</span><br><span class="line">    gif_name = <span class="string">&#x27;./regression.gif&#x27;</span></span><br><span class="line">    <span class="comment"># duration between images</span></span><br><span class="line">    duration = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#### read images and write in gif</span></span><br><span class="line">    images = []</span><br><span class="line">    <span class="keyword">for</span> image_name <span class="keyword">in</span> image_list:</span><br><span class="line">        image_name = os.path.join(root_path, image_name)</span><br><span class="line">        images.append(imageio.imread(image_name))</span><br><span class="line">    imageio.mimwrite(gif_name, images, <span class="string">&#x27;GIF&#x27;</span>, duration=duration)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;success&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="结果可视化"><a href="#结果可视化" class="headerlink" title="结果可视化"></a>结果可视化</h4><p><img src="https://gitee.com/guudman/blog_images/raw/master/regression.gif" alt="image"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;实现过程&quot;&gt;&lt;a href=&quot;#实现过程&quot; class=&quot;headerlink&quot; title=&quot;实现过程&quot;&gt;&lt;/a&gt;实现过程&lt;/h4&gt;&lt;p&gt;搭建两层全连接网络&lt;/p&gt;
&lt;f</summary>
      
    
    
    
    
    <category term="pytorch" scheme="https://guudman.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>GoogleNet解析</title>
    <link href="https://guudman.github.io/2023/11/02/GoogleNet%E8%A7%A3%E6%9E%90/"/>
    <id>https://guudman.github.io/2023/11/02/GoogleNet%E8%A7%A3%E6%9E%90/</id>
    <published>2023-11-02T11:37:32.000Z</published>
    <updated>2023-11-04T07:08:22.741Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>GoogleNet在2014年由Google团队提出，斩获当年ImageNet竞赛中Classification Task分类任务第一名。</p><p>论文原文： <a href="[arxiv.org/pdf/1409.4842.pdf](https://arxiv.org/pdf/1409.4842.pdf">GoogleNet原文</a>)</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/googlenet1.jpg" alt="image"></p><p>首先介绍一下该网络的亮点：</p><ul><li><p>引入Inception结构(融合不同尺度的特征信息)</p></li><li><p>使用1×1的卷积核进行降维以及映射处理</p></li><li><p>添加两个辅助分类器帮助训练</p></li><li><p>丢弃全连接层，使用平均池化层(大大减少模型参数, 除去两个辅助分类器， 网络大小只有vgg的1/20)</p></li></ul><p>接着分析一下Inception结构：</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102141742833.png" alt="image"></p><p>​    左图是论文中提出的inception原始结构， 右图是inception加上降维功能的结构。</p><p>先看<strong>左图</strong>， inception一共有4个分支， 也就是说输入的特征矩阵并行通过这个4个分支得到四个输出， 然后在将这个四个输出在深度维度(channel维度)进行拼接得到最终的输出(<strong>注意：为了让四个分支的输出能够在深度方向进行拼接， 必须保证四个分支输出的特征矩阵高度和宽度都相同</strong>)</p><p>分支1是卷积核大小为1×1的卷积层， stride=1</p><p>分支2是卷积核大小为3×3的卷积层， stride=1， padding=1(保证输出特征矩阵的高和宽和输入特征矩阵相等)</p><p>分支3是卷积核大小为5×5的卷积层， stride=1， padding=2(保证输出特征矩阵的高和宽和输入特征矩阵相等)</p><p>分支4是池化核大小为3×3的最大池化下采样层， stride=1， padding=1(保证输出特征矩阵的高和宽和输入特征矩阵相等)</p><p>在看<strong>右图</strong>， 对比左图， 就是在分支2,3,4上加入了卷积核为1×1的卷积层， 目的是为了降维， 减少模型训练参数， 减少计算量。</p><p>下面看一下1×1卷积核如何减少模型参数的， 同样是对一个深度为512的特征矩阵使用64个大小为5×5的卷积核进行卷积， 不使用1×1卷积核进行降维一共需要819200个参数， 如果使用1×1卷积核进行降维， 一共需要50688个参数，明显减少了很多。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102143154611.png" alt="image"></p><p>每个卷积核的参数如何确定呢， 下面是原论文中给出的参数列表， 对于我们搭建的inception模块， 所需要使用的参数有<strong>#1x1, #3x3reduce, #3x3, #5x5reduce, #5x5, poolproj</strong>这6个参数， 分别对应着所需要的<strong>卷积核的个数</strong>。</p><p>下面将inception模块所用到的参数信息标注在每个分支上， #1x1对应着分支上1x1的卷积核个数， #3x3reduce对应着分支2上1x1的卷积核个数， #3x3对应着分支2上3x3的卷积核个数， #5x5reduce对应着分支3上1x1的卷积核个数， #5x5对应着分支3上5x5的卷积核个数，poolproj对应着分支4上1x1的卷积核个数。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102144232732.png" alt="image"></p><p>接下来看辅助分类器结构，网络中的两个辅助分类器结构一模一样的， 如下图所示：</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102145636207.png" alt="image"></p><p>这两个辅助分类器的输入分别来自Inception(4a)和inception(4d)。</p><p>辅助分类器的第一层是一个平均池化下采样层， 池化核大小为5x5， stride=3， </p><p>第二层是卷积层， 卷积核大小为1x1, stride=1, 卷积核个数是128</p><p>第三层是全连接层， 节点个数为1024</p><p>第四层是全连接层， 节点个数为1000(对应分类任务中分类类别数)</p><p>下面给出了GoogleNet网络结构图</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/Screen_Shot_googlenet.png" alt="image"></p><h4 id="2、代码实现"><a href="#2、代码实现" class="headerlink" title="2、代码实现"></a>2、代码实现</h4><h5 id="1、pytorch实现"><a href="#1、pytorch实现" class="headerlink" title="1、pytorch实现"></a>1、pytorch实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_googlenet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># pytorch官方参考代码：https://github.com/pytorch/vision/blob/main/torchvision/models/googlenet.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GoogleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, aux_logits=<span class="literal">True</span>, init_weights=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GoogleNet, self).__init__()</span><br><span class="line">        self.aux_logits = aux_logits</span><br><span class="line">        <span class="comment"># (224 - 7 + 2 * 3)/2 + 1 = 112 (3, 224, 224) -&gt; (64, 112, 112)</span></span><br><span class="line">        self.conv1 = BaseConv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">7</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># (64, 112, 112) -&gt; (64, 56, 56) 看一下这里的参数是如何计算的，</span></span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># (56 - 3 + 2 * 0)/1 + 1 = 56 (64, 56, 56) -&gt; (64, 56, 56)</span></span><br><span class="line">        self.conv2 = BaseConv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (56 - 3 + 2 * 0)/1 + 1 = 56 (64, 56, 56) -&gt; (192, 56, 56)</span></span><br><span class="line">        self.conv3 = BaseConv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">192</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (192, 56, 56) -&gt; (192. 28. 28)</span></span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Inception3a 具体参数可查看表</span></span><br><span class="line">        self.inception3a = Inception(<span class="number">192</span>, <span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">        self.inception3b = Inception(<span class="number">256</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">192</span>, <span class="number">32</span>, <span class="number">96</span>, <span class="number">64</span>)</span><br><span class="line">        self.maxpool3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.inception4a = Inception(<span class="number">480</span>, <span class="number">192</span>, <span class="number">96</span>, <span class="number">208</span>, <span class="number">16</span>, <span class="number">48</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4b = Inception(<span class="number">512</span>, <span class="number">160</span>, <span class="number">112</span>, <span class="number">224</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4c = Inception(<span class="number">512</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4d = Inception(<span class="number">512</span>, <span class="number">112</span>, <span class="number">144</span>, <span class="number">288</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4e = Inception(<span class="number">528</span>, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">        self.maxpool4 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.inception5a = Inception(<span class="number">832</span>, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">        self.inception5b = Inception(<span class="number">832</span>, <span class="number">384</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">48</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.aux_logits:</span><br><span class="line">            <span class="comment"># Inception4b  512</span></span><br><span class="line">            self.aux1 = InceptionAux(<span class="number">512</span>, num_classes)</span><br><span class="line">            <span class="comment"># Inception4e 528</span></span><br><span class="line">            self.aux2 = InceptionAux(<span class="number">528</span>, num_classes)</span><br><span class="line">        <span class="comment"># 指定输出固定尺寸</span></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.4</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">1024</span>, num_classes)</span><br><span class="line">        <span class="keyword">if</span> init_weights:</span><br><span class="line">            self._initialize_weights()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># [N, 3, 224, 224] -&gt; [N, 64, 112, 112]</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="comment"># [N, 64, 112, 112] -&gt; [N, 64, 56, 56]</span></span><br><span class="line">        x = self.maxpool1(x)</span><br><span class="line">        <span class="comment"># [N, 64, 56, 56] -&gt; [N, 56, 56, 64]</span></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="comment"># [N, 64, 56, 56] -&gt; [N, 56, 56, 192]</span></span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        <span class="comment"># [N, 56, 56, 192] -&gt; [N, 28, 28, 192]</span></span><br><span class="line">        x = self.maxpool2(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [N, 28, 28, 192] -&gt; [N, 28, 28, 256]</span></span><br><span class="line">        x = self.inception3a(x)</span><br><span class="line">        <span class="comment"># [N, 28, 28, 256] -&gt; [N, 28, 28, 480]</span></span><br><span class="line">        x = self.inception3b(x)</span><br><span class="line">        <span class="comment"># [N, 28, 28, 480] - &gt; [N, 14, 14, 480]</span></span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#  [N, 14, 14, 480] -&gt; [N, 14, 14, 512]</span></span><br><span class="line">        x = self.inception4a(x)</span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> self.aux_logits:</span><br><span class="line">            aux1 = self.aux1(x)</span><br><span class="line">        <span class="comment">#   [N, 14, 14, 512] -&gt; [N, 14, 14, 512]</span></span><br><span class="line">        x = self.inception4b(x)</span><br><span class="line">        <span class="comment">#   [N, 14, 14, 512] -&gt; [N, 14, 14, 512]</span></span><br><span class="line">        x = self.inception4c(x)</span><br><span class="line">        <span class="comment">#   [N, 14, 14, 512] -&gt; [N, 14, 14, 528]</span></span><br><span class="line">        x = self.inception4d(x)</span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> self.aux_logits:</span><br><span class="line">            aux2 = self.aux2(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#   [N, 14, 14, 528] -&gt; [N, 14, 14, 832]</span></span><br><span class="line">        x = self.inception4e(x)</span><br><span class="line">        <span class="comment"># [N, 14, 14, 832] - &gt; [N, 7, 7, 832]</span></span><br><span class="line">        x = self.maxpool4(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#  [N, 7, 7, 832] -&gt; [N, 7, 7, 832]</span></span><br><span class="line">        x = self.inception5a(x)</span><br><span class="line">        <span class="comment">#  [N, 7, 7, 832] -&gt; [N, 7, 7, 1024]</span></span><br><span class="line">        x = self.inception5b(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#  [N, 7, 7, 1024] -&gt; [N, 1, 1, 1024]</span></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        <span class="comment">#  [N, 1, 1, 1024] -&gt; [N, 1024]</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="comment"># [N, 1024] -&gt; [N, num_classes]</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> self.aux_logits:</span><br><span class="line">            <span class="keyword">return</span> x, aux2, aux1</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                    nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__()</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        ch1x1: </span></span><br><span class="line"><span class="string">        ch3x3red: ch3x3reduce</span></span><br><span class="line"><span class="string">        ch3x3:</span></span><br><span class="line"><span class="string">        ch5x5red: ch5x5reduce</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.branch1 = BaseConv2d(in_channels, ch1x1, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.branch2 = nn.Sequential(</span><br><span class="line">            BaseConv2d(in_channels=in_channels, out_channels=ch3x3red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            <span class="comment"># 保证输出大小等于输入大小</span></span><br><span class="line">            BaseConv2d(in_channels=ch3x3red, out_channels=ch3x3, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        self.branch3 = nn.Sequential(</span><br><span class="line">            BaseConv2d(in_channels=in_channels, out_channels=ch5x5red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            <span class="comment"># 保证输出大小等于输入大小</span></span><br><span class="line">            BaseConv2d(in_channels=ch5x5red, out_channels=ch5x5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.branch4 = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            BaseConv2d(in_channels=in_channels, out_channels=pool_proj, kernel_size=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1 = self.branch1(x)</span><br><span class="line">        branch2 = self.branch2(x)</span><br><span class="line">        branch3 = self.branch3(x)</span><br><span class="line">        branch4 = self.branch4(x)</span><br><span class="line"></span><br><span class="line">        outputs = [branch1, branch2, branch3, branch4]</span><br><span class="line">        <span class="comment"># [batch, channel, h, w] torch.cat(outputs, 1)表示在channel维度上拼接</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionAux</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionAux, self).__init__()</span><br><span class="line">        self.averagepool = nn.AvgPool2d(kernel_size=<span class="number">5</span>, stride=<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># output [batch, 128, 4, 4]</span></span><br><span class="line">        self.conv = BaseConv2d(in_channels, <span class="number">128</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">1024</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># aux1: [N, 512, 14, 14] aux2: [N, 528, 14, 14]</span></span><br><span class="line">        x = self.averagepool(x)</span><br><span class="line">        <span class="comment"># aux1: [N, 512, 4, 4], aux2: [N, 528, 4, 4]</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="comment"># [N, 128, 4, 4]</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.5</span>, training=self.training)</span><br><span class="line">        <span class="comment"># [N, 2048]</span></span><br><span class="line">        x = F.relu(self.fc1(x), inplace=<span class="literal">True</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.5</span>, training=self.training)</span><br><span class="line">        <span class="comment"># [N, 2014]</span></span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="comment"># [N, num_classes]</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaseConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BaseConv2d, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># input = torch.rand((16, 3, 224, 224))</span></span><br><span class="line"><span class="comment"># googlenet = GoogleNet(num_classes=5, aux_logits=False, init_weights=True)</span></span><br><span class="line"><span class="comment"># print(googlenet)</span></span><br><span class="line"><span class="comment"># output = googlenet(input)</span></span><br><span class="line"><span class="comment"># print(output)</span></span><br></pre></td></tr></table></figure><h5 id="2、TensorFlow实现"><a href="#2、TensorFlow实现" class="headerlink" title="2、TensorFlow实现"></a>2、TensorFlow实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_googlenet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：0399</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, Model, Sequential</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">GoogleNet</span>(<span class="params">im_height=<span class="number">224</span>, im_width=<span class="number">224</span>, class_num=<span class="number">1000</span>, aux_logits=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># tensorflow通道顺序 NHWC</span></span><br><span class="line">    <span class="comment"># [None, 224, 224, 3]</span></span><br><span class="line">    input_image = layers.Input(shape=(im_height, im_width, <span class="number">3</span>), dtype=<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">    <span class="comment"># [None, 224, 224, 3] -&gt; [None, 112, 112, 64]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;SAME&quot;</span>,</span><br><span class="line">                      activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&quot;conv2d_1&quot;</span>)(input_image)</span><br><span class="line">    <span class="comment"># [None, 112, 112, 64] -&gt; [None, 56, 56, 64]</span></span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;SAME&quot;</span>, name=<span class="string">&quot;maxpool_1&quot;</span>)(x)</span><br><span class="line">    <span class="comment">#  [None, 56, 56, 64] -&gt;  [None, 56, 56, 64]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">1</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&quot;conv2d_2&quot;</span>)(x)</span><br><span class="line">    <span class="comment">#  [None, 56, 56, 64] -&gt;  [None, 56, 56, 192]  (56 - 3 + 2 * 1)/1 + 1 = 56</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&quot;conv2d_3&quot;</span>)(x)</span><br><span class="line">    <span class="comment">#  [None, 56, 56, 192] -&gt;  [None, 28, 28, 192]</span></span><br><span class="line"></span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)</span><br><span class="line">    <span class="comment">#  [None, 28, 28, 192] -&gt;  [None, 28, 28, 256]</span></span><br><span class="line">    x = Inception(<span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">32</span>, name=<span class="string">&quot;inception3a&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 28, 28, 256] -&gt; [None, 28, 28, 480]</span></span><br><span class="line">    x = Inception(<span class="number">128</span>, <span class="number">128</span>, <span class="number">192</span>, <span class="number">32</span>, <span class="number">96</span>, <span class="number">64</span>, name=<span class="string">&quot;inception3b&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 28, 28, 480] -&gt; [None, 14, 14, 480]</span></span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;SAME&quot;</span>, name=<span class="string">&quot;maxpool_2&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [None, 14, 14, 480] -&gt; [None, 14, 14, 512]</span></span><br><span class="line">    x = Inception(<span class="number">192</span>, <span class="number">96</span>, <span class="number">208</span>, <span class="number">16</span>, <span class="number">48</span>, <span class="number">64</span>, name=<span class="string">&quot;inception4a&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> aux_logits:</span><br><span class="line">        aux1 = InceptionAux(class_num, name=<span class="string">&quot;aux1&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 14, 14, 512] -&gt; [None, 14, 14, 512]</span></span><br><span class="line">    x = Inception(<span class="number">160</span>, <span class="number">112</span>, <span class="number">224</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>, name=<span class="string">&quot;inception4b&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 14, 14, 512] -&gt; [None, 14, 14, 512]</span></span><br><span class="line">    x = Inception(<span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>, name=<span class="string">&quot;inception4c&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 14, 14, 512] -&gt; [None, 14, 14, 528]</span></span><br><span class="line">    x = Inception(<span class="number">112</span>, <span class="number">144</span>, <span class="number">288</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>, name=<span class="string">&quot;inception4d&quot;</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> aux_logits:</span><br><span class="line">        aux2 = InceptionAux(class_num, name=<span class="string">&quot;aux2&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [None, 14, 14, 528] -&gt; [None, 14, 14, 832]</span></span><br><span class="line">    x = Inception(<span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>, name=<span class="string">&quot;inception4e&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 14, 14, 832] -&gt; [None, 7, 7, 832]</span></span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;SAME&quot;</span>, name=<span class="string">&quot;maxpool_3&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 7, 7, 832] -&gt; [None, 7, 7, 832]</span></span><br><span class="line">    x = Inception(<span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>, name=<span class="string">&quot;inception5a&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 7, 7, 832] -&gt; [None, 7, 7, 1024]</span></span><br><span class="line">    x = Inception(<span class="number">384</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">48</span>, <span class="number">128</span>, <span class="number">128</span>, name=<span class="string">&quot;inception5b&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 7, 7, 1024] -&gt; [None, 1, 1, 1024]</span></span><br><span class="line">    x = layers.AvgPool2D(pool_size=<span class="number">7</span>, strides=<span class="number">1</span>, name=<span class="string">&quot;avgpool_1&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 1, 1, 1024] -&gt; [None, 1024*1*1]</span></span><br><span class="line">    x = layers.Flatten(name=<span class="string">&quot;output_flatten&quot;</span>)(x)</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    x = layers.Dropout(rate=<span class="number">0.4</span>, name=<span class="string">&quot;output_dropout&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, class_num]</span></span><br><span class="line">    x = layers.Dense(class_num, name=<span class="string">&quot;output_dense&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">    aux3 = layers.Softmax(name=<span class="string">&quot;aux_3&quot;</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> aux_logits:</span><br><span class="line">        model = models.Model(inputs=input_image, outputs=[aux1, aux2, aux3])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model = models.Model(inputs=input_image, outputs=aux3)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__()</span><br><span class="line">        self.branch1 = layers.Conv2D(filters=ch1x1, kernel_size=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.branch2 = Sequential([</span><br><span class="line">            layers.Conv2D(filters=ch3x3red, kernel_size=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            layers.Conv2D(filters=ch3x3, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)])</span><br><span class="line"></span><br><span class="line">        self.branch3 = Sequential([</span><br><span class="line">            layers.Conv2D(filters=ch5x5red, kernel_size=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            layers.Conv2D(filters=ch5x5, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)])</span><br><span class="line"></span><br><span class="line">        self.branch4 = Sequential([</span><br><span class="line">            <span class="comment"># caution: default stride=pool_size</span></span><br><span class="line">            layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">&quot;SAME&quot;</span>),</span><br><span class="line">            layers.Conv2D(filters=pool_proj, kernel_size=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, <span class="built_in">input</span>, **kwargs</span>):</span><br><span class="line">        branch1 = self.branch1(<span class="built_in">input</span>)</span><br><span class="line">        branch2 = self.branch2(<span class="built_in">input</span>)</span><br><span class="line">        branch3 = self.branch3(<span class="built_in">input</span>)</span><br><span class="line">        branch4 = self.branch4(<span class="built_in">input</span>)</span><br><span class="line">        outputs = layers.concatenate([branch1, branch2, branch3, branch4])</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionAux</span>(layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionAux, self).__init__()</span><br><span class="line">        self.avgpool = layers.AvgPool2D(pool_size=<span class="number">5</span>, strides=<span class="number">3</span>)</span><br><span class="line">        self.conv = layers.Conv2D(<span class="number">128</span>, kernel_size=<span class="number">1</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.fc1 = layers.Dense(units=<span class="number">1024</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(units=num_classes)</span><br><span class="line">        self.softmax = layers.Softmax()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, **kwargs</span>):</span><br><span class="line">        <span class="comment"># aux1 [None, 14, 14, 512] aux2 [None, 14, 14, 528]</span></span><br><span class="line">        <span class="comment"># aux1: [None, 14, 14, 512] -&gt; [None, 4, 4, 512]  (14 - 5)/3 + 1 = 4</span></span><br><span class="line">        <span class="comment"># axu2: [None, 14, 14, 528] -&gt; [None, 4, 4, 528]  (14 - 5)/3 + 1 = 4</span></span><br><span class="line">        x = self.avgpool(inputs)</span><br><span class="line">        <span class="comment"># aux1 [None, 4, 4, 512]-&gt; [4, 4, 512]  aux2 [None, 4, 4, 528]-&gt; [4, 4, 528]</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        x = layers.Flatten()(x)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = layers.Dropout(rate=<span class="number">0.5</span>)(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = layers.Dropout(rate=<span class="number">0.5</span>)(x)</span><br><span class="line">        x = self.softmax(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># input = tf.random.uniform((16, 224, 224, 3))</span></span><br><span class="line"><span class="comment"># googlenet = GoogleNet(class_num=5, aux_logits=False)</span></span><br><span class="line"><span class="comment"># output = googlenet(input)</span></span><br><span class="line"><span class="comment"># print(output)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;GoogleNet在2014年由</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>VggNet解析</title>
    <link href="https://guudman.github.io/2023/11/02/VggNet%E8%A7%A3%E6%9E%90/"/>
    <id>https://guudman.github.io/2023/11/02/VggNet%E8%A7%A3%E6%9E%90/</id>
    <published>2023-11-02T11:17:44.000Z</published>
    <updated>2023-11-05T03:57:01.191Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>论文原文： <a href="[1409.1556.pdf (arxiv.org">VggNet原文</a>](<a href="https://arxiv.org/pdf/1409.1556.pdf">https://arxiv.org/pdf/1409.1556.pdf</a>))</p><p>VGG由牛津大学视觉几何小组(Visual Geometry Group, VGG)提出的一种深层卷积网， 该网络在2014年获得定位任务的第一名， 分类任务的第二名。VGG可以看成是加深版的AlexNet， 都是conv + FC layer组成。</p><p>下图是VGG16模型的结构简图</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102101734336.png" alt="vgg34"></p><p><strong>网络的亮点</strong></p><ul><li>通过堆叠多个3×3的卷积代替大尺度卷积核 (在保证相同感受野的前提下能够减少所需的参数量)</li></ul><p>论文中提到，通过堆叠两个3×3的卷积核代替5×5的卷积核， 堆叠三个3×3的卷积核代替7×7的卷积核。It is easy to see that a stack of <strong>two</strong> <strong>3</strong> <strong>× 3</strong> conv layers (without spatial pooling in between) has an effective receptive field of <strong>5 × 5</strong>; <strong>three</strong> such layers have a <strong>7 × 7</strong> effective receptive field.</p><p>下面给出一个实例</p><p>使用7×7卷积核所需参数， 假设输入输出通道数均为C。</p><p>7×7×C×C = 49C²</p><p>堆叠3个3×3卷积核所需参数， 假设输入输出通道数均为C。</p><p>3×3×C×C + 3×3×C×C + 3×3×C×C = 27C²</p><p>经过对比发现使用3层3×3的卷积层比使用7×7的卷积核参数更少。</p><p>下图是从原论文中截取的几种VGG模型的配置表， 表中作者呈现了几种不同深度的配置(11层, 13层， 16层， 19层)是否使用<strong>LRN</strong>以及1×1卷积层与3×3卷积层的差异。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102102435922.png" alt="image"></p><h4 id="2、代码实现"><a href="#2、代码实现" class="headerlink" title="2、代码实现"></a>2、代码实现</h4><h5 id="1、pytorch实现"><a href="#1、pytorch实现" class="headerlink" title="1、pytorch实现"></a>1、pytorch实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_vgg.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先搭建vgg19</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VggNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(VggNet, self).__init__()</span><br><span class="line">        <span class="comment"># (224 - 3 + 2*1)/1 + 1 = 224 -&gt; (224, 224, 64)</span></span><br><span class="line">        self.conv1_64_1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (223 - 3 + 2*1)1 + 1 = 224 -&gt; (224, 224, 64)</span></span><br><span class="line">        self.conv1_64_2 = nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (224, 224, 64) -&gt; (112, 112, 64)</span></span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># (112 - 3 + 2*1)/1 + 1 = 112  (112, 112, 64) -&gt; (112, 112, 128)</span></span><br><span class="line">        self.conv2_128_1 = nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2_128_2 = nn.Conv2d(in_channels=<span class="number">128</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (112, 128, 128) -&gt; (56, 56, 128)</span></span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># (56 - 3 + 2 * 1)/ 1 + 1 = 56 (56, 56, 128) -&gt; (56, 56, 256)</span></span><br><span class="line">        self.conv3_256_1 = nn.Conv2d(in_channels=<span class="number">128</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3_256_2 = nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3_256_3 = nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># maxpool  (56, 56, 256) -&gt; (28, 28, 256)</span></span><br><span class="line">        self.maxpool3 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv4_512_1 = nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv4_512_2 = nn.Conv2d(in_channels=<span class="number">512</span>, out_channels=<span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv4_512_3 = nn.Conv2d(in_channels=<span class="number">512</span>, out_channels=<span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># maxpool (28, 28, 512) -&gt; (14, 14, 512)</span></span><br><span class="line">        self.maxpool4 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># (14, 14, 512)</span></span><br><span class="line">        self.conv5_512_1 = nn.Conv2d(in_channels=<span class="number">512</span>, out_channels=<span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv5_512_2 = nn.Conv2d(in_channels=<span class="number">512</span>, out_channels=<span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv5_512_3 = nn.Conv2d(in_channels=<span class="number">512</span>, out_channels=<span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># maxpool (14, 14, 512) -&gt; (7, 7, 512)</span></span><br><span class="line">        self.maxpool5 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">7</span> * <span class="number">7</span> * <span class="number">512</span>, <span class="number">4096</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>)</span><br><span class="line">        self.fc3 = nn.Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1_64_1(x)</span><br><span class="line">        x = self.conv1_64_2(x)</span><br><span class="line">        x = self.maxpool1(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2_128_1(x)</span><br><span class="line">        x = self.conv2_128_2(x)</span><br><span class="line">        x = self.maxpool2(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv3_256_1(x)</span><br><span class="line">        x = self.conv3_256_2(x)</span><br><span class="line">        x = self.conv3_256_3(x)</span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv4_512_1(x)</span><br><span class="line">        x = self.conv4_512_2(x)</span><br><span class="line">        x = self.conv4_512_3(x)</span><br><span class="line">        x = self.maxpool4(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv5_512_1(x)</span><br><span class="line">        x = self.conv5_512_2(x)</span><br><span class="line">        x = self.conv5_512_3(x)</span><br><span class="line">        x = self.maxpool5(x)  <span class="comment"># [batch, c, h, w] -&gt; [-1, c*h*w]</span></span><br><span class="line"></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">512</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># input = torch.rand([8, 3, 224, 224])</span></span><br><span class="line"><span class="comment"># vggnet = VggNet()</span></span><br><span class="line"><span class="comment"># print(vggnet)</span></span><br><span class="line"><span class="comment"># output = vggnet(input)</span></span><br><span class="line"><span class="comment"># print(output)</span></span><br></pre></td></tr></table></figure><h5 id="2、TensorFlow实现"><a href="#2、TensorFlow实现" class="headerlink" title="2、TensorFlow实现"></a>2、TensorFlow实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_vggnet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：0399</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, Model, Sequential</span><br><span class="line"></span><br><span class="line">CONV_KERNEL_INITIALIZER = &#123;</span><br><span class="line">    <span class="string">&#x27;class_name&#x27;</span>: <span class="string">&#x27;VarianceScaling&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;config&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;scale&#x27;</span>: <span class="number">2.0</span>,</span><br><span class="line">        <span class="string">&#x27;mode&#x27;</span>: <span class="string">&#x27;fan_out&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;distribution&#x27;</span>: <span class="string">&#x27;truncated_normal&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DENSE_KERNEL_INITIALIZER = &#123;</span><br><span class="line">    <span class="string">&#x27;class_name&#x27;</span>: <span class="string">&#x27;VarianceScaling&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;config&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;scale&#x27;</span>: <span class="number">1.</span> / <span class="number">3.</span>,</span><br><span class="line">        <span class="string">&#x27;mode&#x27;</span>: <span class="string">&#x27;fan_out&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;distribution&#x27;</span>: <span class="string">&#x27;uniform&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">VGG</span>(<span class="params">feature, im_height=<span class="number">224</span>, im_width=<span class="number">224</span>, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># tensorflow中的tensor通道顺序是NHWC</span></span><br><span class="line">    input_image = layers.Input(shape=(im_height, im_width, <span class="number">3</span>), dtype=<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">    x = feature(input_image)</span><br><span class="line">    x = layers.Flatten()(x)</span><br><span class="line">    x = layers.Dropout(rate=<span class="number">0.5</span>)(x)</span><br><span class="line">    x = layers.Dense(<span class="number">2048</span>, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                     kernel_initializer=DENSE_KERNEL_INITIALIZER)(x)</span><br><span class="line">    x = layers.Dropout(rate=<span class="number">0.5</span>)(x)</span><br><span class="line">    x = layers.Dense(<span class="number">2048</span>, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                     kernel_initializer=DENSE_KERNEL_INITIALIZER)(x)</span><br><span class="line">    x = layers.Dropout(rate=<span class="number">0.5</span>)(x)</span><br><span class="line">    x = layers.Dense(num_classes, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                     kernel_initializer=DENSE_KERNEL_INITIALIZER)(x)</span><br><span class="line">    output = layers.Softmax()(x)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=input_image, outputs=output)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_feature</span>(<span class="params">cfg</span>):</span><br><span class="line">    feature_layers = []</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> cfg:</span><br><span class="line">        <span class="keyword">if</span> v == <span class="string">&#x27;M&#x27;</span>:</span><br><span class="line">            feature_layers.append(layers.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            feature_layers.append(layers.Conv2D(v, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>,</span><br><span class="line">                                                kernel_initializer=CONV_KERNEL_INITIALIZER))</span><br><span class="line">    <span class="keyword">return</span> Sequential(feature_layers, name=<span class="string">&quot;feature&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cfgs = &#123;</span><br><span class="line">    <span class="string">&#x27;vgg11&#x27;</span>: [<span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg13&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg16&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg19&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">model_name=<span class="string">&quot;vgg16&quot;</span>, im_height=<span class="number">224</span>, im_width=<span class="number">224</span>, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    cfg = cfgs[model_name]</span><br><span class="line">    model = VGG(make_feature(cfg), im_height=im_height, im_width=im_width, num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># input = tf.random.uniform((4, 224, 224, 3))</span></span><br><span class="line"><span class="comment"># vggnet = vgg(&quot;vgg16&quot;, num_classes=5)</span></span><br><span class="line"><span class="comment"># print(vggnet.summary())</span></span><br><span class="line"><span class="comment"># print(vggnet(input))</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;论文原文： &lt;a href=&quot;[</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>AlexNet解析</title>
    <link href="https://guudman.github.io/2023/11/02/AlexNet%E8%A7%A3%E6%9E%90/"/>
    <id>https://guudman.github.io/2023/11/02/AlexNet%E8%A7%A3%E6%9E%90/</id>
    <published>2023-11-02T11:11:45.000Z</published>
    <updated>2023-11-04T07:08:18.047Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、AlexNet简介"><a href="#1、AlexNet简介" class="headerlink" title="1、AlexNet简介"></a>1、AlexNet简介</h4><p>Alexnet是2012年ILSVRC 2012(ImageNet Large Scale Visual Recognition Challenge)竞赛的冠军网络， 分类准确率由传统的70%提升到80%(当时传统方法已进入瓶颈期，所以这么大的提升是非常厉害的)。它是由Hinton和他的学生Alex设计的。也是在那年之后，深度学习模型开始迅速发展。下面的图就是Alexnet原论文中截取的网络结构图。</p><p>Alexnet论文原文， <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a></p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231102091945023.png" alt="image"></p><p>图中有上下两部分是因为作者使用两块GPU进行并行训练， 所以上下两部分的结果是一模一样的。我们直接看下面部分就行了。</p><p>接着说说该网络的亮点：</p><p>(1)首次使用了GPU进行网络加速训练</p><p>(2)使用了ReLU激活函数， 而不是传统的Sigmoid激活函数以及Tanh激活函数</p><p>(3)使用了LRN局部相应归一化</p><p>(4)在全连接层的前两层使用了Dropout方法按照一定比例随机失活神经元，以减少过拟合</p><p>接着给出卷积或池化后的矩阵尺寸大小计算公式</p><p>N = (W - F  + 2p)/s + 1</p><p>其中w是输入图片大小， F是 卷积核或池化核大小， p是padding的像素个数， s是步距。</p><p>接下来对每一层进行详细分析</p><h4 id="2、模型结构参数剖析"><a href="#2、模型结构参数剖析" class="headerlink" title="2、模型结构参数剖析"></a>2、模型结构参数剖析</h4><p><strong>卷积层1</strong></p><p>由于使用了两块GPU， 所以卷积核的个数需要乘以2：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Conv1:</span><br><span class="line">    input<span class="built_in">_</span>size: [224, 224, 3] -&gt; output：(224 – 11 + (1 + 2))/4 + 1=55 -&gt;(55, 55, 96)</span><br><span class="line">    kernels: 48 * 2</span><br><span class="line">    kernel<span class="built_in">_</span>size: 11</span><br><span class="line">    stride: 4</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Conv1: kernels=48 × 2 = 96， kernel_size=11, padding=[1, 2], stride=4 </strong></p><p>因此卷积核的个数为96， kernel_size代表卷积核的尺寸， padding代表特征矩阵上下左右补零的参数，stride代表步距。</p><p>输入图片的shape=[224, 224, 3]， 输出矩阵的计算公式为： (224  - 11 + (1 + 2)) / 4 + 1 = 55</p><p>所以输出矩阵的shape为[55, 55, 96]</p><p><strong>Conv1</strong>的实现过程【两个GPU计算过程一模一样，所以kernels就按照一块GPU来搭建】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytroch</span></span><br><span class="line">self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, kernel_size=<span class="number">11</span>, out_channels=<span class="number">48</span>, padding=<span class="number">2</span>, stride=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># tensorflow</span></span><br><span class="line">x = layers.Conv2D(filters=<span class="number">48</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br></pre></td></tr></table></figure><p><strong>最大池化下采样层1</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">maxpooling1: </span><br><span class="line">input<span class="built_in">_</span>size:(55, 55, 96) kernel<span class="built_in">_</span>size:3</span><br><span class="line">padding:0</span><br><span class="line">stride:2</span><br><span class="line">outpu<span class="built_in">_</span>size(27, 27, 96)</span><br></pre></td></tr></table></figure><p><strong>Maxpool1: kernel_size=3, padding=0, stride=2</strong></p><p>kernel_size表示池化核大小， padding表示矩阵上下左右补零的参数， stride代表步距。</p><p>输入特征矩阵的shape=[55, 55, 96], 输出特征矩阵的shape=[27, 27 , 96]</p><p>shape计算： (W -F + 2P)/S+1 = (55 - 3 + 2*0)/2 + 1=27</p><p><strong>Maxpool1</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch:</span></span><br><span class="line">self.maxpooling1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># tensorflow</span></span><br><span class="line">x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)  <span class="comment"># [None, 27, 27, 48]</span></span><br></pre></td></tr></table></figure><p><strong>卷积层2</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Conv2:</span><br><span class="line">input<span class="built_in">_</span>size:[27, 27, 96]</span><br><span class="line">kernel<span class="built_in">_</span>size:5</span><br><span class="line">kernels: 128 * 2</span><br><span class="line">padding:2</span><br><span class="line">stride:1</span><br><span class="line"></span><br><span class="line">output<span class="built_in">_</span>size:[27, 27, 256]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Conv2: kernel=128×2, kernel_size=5, padding=2, stride=1 </strong></p><p>输入特征矩阵的深度为[27, 27, 96], 输出特征矩阵尺寸计算公式为：(27 – 5 + 2 * 2)/1+ 1=27</p><p>所以输出特征矩阵的尺寸为[27, 27, 256]</p><p><strong>Conv2</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch</span></span><br><span class="line">self.conv2 = nn.Conv2d(in_channels=<span class="number">48</span>, kernel_size=<span class="number">5</span>, out_channels=<span class="number">128</span>, padding=<span class="number">2</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow</span></span><br><span class="line"><span class="comment"># 当stride=1且padding=same时， 表示输出尺寸与输入尺寸相同 -&gt;[None, 27, 27, 128]</span></span><br><span class="line">x = layers.Conv2D(filters=<span class="number">128</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br></pre></td></tr></table></figure><p><strong>最大池化下采样层2</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">maxpooling2:</span><br><span class="line">input<span class="built_in">_</span>size:[27, 27, 256] </span><br><span class="line">kernel<span class="built_in">_</span>size:3</span><br><span class="line">padding:0</span><br><span class="line">stride:2  </span><br><span class="line">output<span class="built_in">_</span>size:[13, 13, 256]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Maxpool2: kernel_size=3, padding=0, stride=2</strong></p><p>kernel_size表示池化核大小， padding表示矩阵上下左右补零的参数， stride代表步距。</p><p>输入特征矩阵的shape=[27, 27, 256], 输出特征矩阵的shape=[13, 13 , 256]</p><p>shape计算： (W -F + 2P)/S+1 = (27- 3 + 2*0)/2 + 1=13</p><p><strong>Maxpool2</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch:</span></span><br><span class="line">self.maxpooling2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># tensorflow</span></span><br><span class="line"><span class="comment"># [None, 27, 27, 128] -&gt; [None, 13, 13, 128]</span></span><br><span class="line">x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)</span><br></pre></td></tr></table></figure><p><strong>卷积层3</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conv3:</span><br><span class="line">input<span class="built_in">_</span>size:[13, 13, 256]</span><br><span class="line">kernels: 192*2 = 384</span><br><span class="line">kernel<span class="built_in">_</span>size:3</span><br><span class="line">padding:1</span><br><span class="line">stride:1</span><br><span class="line">output<span class="built_in">_</span>size:[13, 13, 384]</span><br></pre></td></tr></table></figure><p><strong>Conv3: kernel=192×2, kernel_size=3, padding=1, stride=1 </strong></p><p>输入特征矩阵的深度为[27, 27, 96], 输出特征矩阵尺寸计算公式为：(13– 3 + 2 * 1)/1+ 1=13</p><p>所以输出特征矩阵的尺寸为[13, 13, 384]</p><p><strong>Conv3</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch</span></span><br><span class="line">self.conv3 = nn.Conv2d(in_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, out_channels=<span class="number">192</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow</span></span><br><span class="line"><span class="comment"># stride=1, padding=same, 输出不变 -&gt;[None, 13, 13, 192]</span></span><br><span class="line">x = layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>卷积层4</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conv4:</span><br><span class="line">input<span class="built_in">_</span>size:[13, 13, 384]</span><br><span class="line">kernels: 192*2 = 384</span><br><span class="line">kernel<span class="built_in">_</span>size:3</span><br><span class="line">padding:1</span><br><span class="line">stride:1</span><br><span class="line"></span><br><span class="line">output<span class="built_in">_</span>size:[13, 13, 384]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Conv4: kernel=192×2, kernel_size=3, padding=1, stride=1 </strong></p><p>输入特征矩阵的深度为[13, 13, 384], 输出特征矩阵尺寸计算公式为：(13– 3 + 2 * 1)/1+ 1=13</p><p>所以输出特征矩阵的尺寸为[13, 13, 384]</p><p><strong>Conv4</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch</span></span><br><span class="line">self.conv4 = nn.Conv2d(in_channels=<span class="number">192</span>, kernel_size=<span class="number">3</span>, out_channels=<span class="number">192</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow</span></span><br><span class="line"><span class="comment"># -&gt;[None, 13, 13, 192]</span></span><br><span class="line">x = layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>卷积层5</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conv5:</span><br><span class="line"> input<span class="built_in">_</span>size:[13, 13, 384]</span><br><span class="line">kernels: 128*2 = 256</span><br><span class="line">kernel<span class="built_in">_</span>size:3</span><br><span class="line">padding:1</span><br><span class="line">stride:1</span><br><span class="line"></span><br><span class="line">output<span class="built_in">_</span>size:[13, 13, 256]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输入特征矩阵的深度为[13, 13, 384], 输出特征矩阵尺寸计算公式为：(13– 3 + 2 * 1)/1+ 1=13</p><p>所以输出特征矩阵的尺寸为[13, 13, 256]</p><p><strong>Conv5</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch</span></span><br><span class="line">self.conv5 = nn.Conv2d(in_channels=<span class="number">192</span>, kernel_size=<span class="number">3</span>, out_channels=<span class="number">128</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># tensorflow</span></span><br><span class="line"><span class="comment"># -&gt;[None, 13, 13, 128]</span></span><br><span class="line">x = layers.Conv2D(filters=<span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br></pre></td></tr></table></figure><p><strong>最大池化下采样3</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">maxpool3:</span><br><span class="line">input<span class="built_in">_</span>size:[13, 13, 256] </span><br><span class="line">kernel<span class="built_in">_</span>size:3</span><br><span class="line">padding:0</span><br><span class="line">stride:2</span><br><span class="line"></span><br><span class="line">output<span class="built_in">_</span>size:[6, 6, 256]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输入特征矩阵的shape=[13, 13, 256], 输出特征矩阵的shape=[6, 6, 256]</p><p>shape计算： (W -F + 2P)/S+1 = (13- 3 + 2*0)/2 + 1=6</p><p><strong>Maxpool3</strong>的实现过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch</span></span><br><span class="line">self.maxpooling3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># tensorflow</span></span><br><span class="line"><span class="comment"># -&gt;[None, 6, 6, 128]</span></span><br><span class="line">x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)</span><br></pre></td></tr></table></figure><p><strong>全连接层1</strong></p><p><strong>uni_size: 4096, unit_size为全连接层的节点个数， 两块GPU所以翻倍</strong></p><p><strong>全连接层2</strong></p><p><strong>uni_size: 4096, unit_size为全连接层的节点个数， 两块GPU所以翻倍</strong></p><p><strong>全连接层3</strong></p><p><strong>uni_size: 1000</strong>， 该层为输出层， 输出节点数对应分类任务中分类类别数。</p><h4 id="3、参数列表"><a href="#3、参数列表" class="headerlink" title="3、参数列表"></a>3、参数列表</h4><div class="table-container"><table><thead><tr><th><strong>名称</strong></th><th style="text-align:center"><strong>Input_size</strong></th><th><strong>Kernel_size</strong></th><th><strong>Kernel_num</strong></th><th><strong>padding</strong></th><th><strong>Stride</strong></th><th><strong>Output_size</strong></th><th><strong>尺寸计算</strong></th></tr></thead><tbody><tr><td>Conv1</td><td style="text-align:center">(224,  224,3)</td><td>11</td><td>48*2</td><td>[1,2]</td><td>4</td><td>(55, 55, 96)</td><td>(224-11+2*2)4+1=55</td></tr><tr><td>Maxpooling1</td><td style="text-align:center">(55, 55, 96)</td><td>3</td><td></td><td>0</td><td>2</td><td>(27,  27, 96)</td><td>(55-3+2*0)/2+1=27</td></tr><tr><td>Conv2</td><td style="text-align:center">(27,  27, 96)</td><td>5</td><td>128*2</td><td>2</td><td>1</td><td>(27,27, 256)</td><td>(27-5+2*2)/1+1=27</td></tr><tr><td>Maxpooling2</td><td style="text-align:center">(27,27, 256)</td><td>3</td><td></td><td>0</td><td>2</td><td>(13, 13, 256)</td><td>(27-3+2*0)/2+1=13</td></tr><tr><td>Conv3</td><td style="text-align:center">(13, 13, 256)</td><td>3</td><td>192*2</td><td>1</td><td>1</td><td>(13, 13, 384)</td><td>(13-3+2*1)/1+1=13</td></tr><tr><td>Conv4</td><td style="text-align:center">(13, 13, 384)</td><td>3</td><td>192*2</td><td>1</td><td>1</td><td>(13, 13, 384)</td><td>(13-3+2*1)/1+1=13</td></tr><tr><td>Conv5</td><td style="text-align:center">(13,13, 384)</td><td>3</td><td>128*2</td><td>1</td><td>1</td><td>(13, 13, 256)</td><td>(13-3+2*1)/1+1=13</td></tr><tr><td>Maxpooling3</td><td style="text-align:center">(13,  13, 256)</td><td>3</td><td></td><td>0</td><td>2</td><td>(6,6,256)</td><td>(13-3+2*0)/2+1=6</td></tr><tr><td>FC1</td><td style="text-align:center"></td><td>2048</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>FC2</td><td style="text-align:center"></td><td>2048</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>FC3</td><td style="text-align:center"></td><td>1000</td><td></td><td></td><td></td><td></td></tr></tbody></table></div><h4 id="4、代码实现"><a href="#4、代码实现" class="headerlink" title="4、代码实现"></a>4、代码实现</h4><h5 id="1、pytorch实现"><a href="#1、pytorch实现" class="headerlink" title="1、pytorch实现"></a>1、pytorch实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_alexnet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, kernel_size=<span class="number">11</span>, out_channels=<span class="number">48</span>, padding=<span class="number">2</span>, stride=<span class="number">4</span>)</span><br><span class="line">        self.maxpooling1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">48</span>, kernel_size=<span class="number">5</span>, out_channels=<span class="number">128</span>, padding=<span class="number">2</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.maxpooling2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, out_channels=<span class="number">192</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv4 = nn.Conv2d(in_channels=<span class="number">192</span>, kernel_size=<span class="number">3</span>, out_channels=<span class="number">192</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv5 = nn.Conv2d(in_channels=<span class="number">192</span>, kernel_size=<span class="number">3</span>, out_channels=<span class="number">128</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.maxpooling3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">128</span> * <span class="number">6</span> * <span class="number">6</span>, out_features=<span class="number">2048</span>)</span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">2048</span>)</span><br><span class="line">        self.fc3 = nn.Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.maxpooling1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.maxpooling2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.conv4(x)</span><br><span class="line">        x = self.conv5(x)</span><br><span class="line">        x = self.maxpooling3(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">128</span> * <span class="number">6</span> * <span class="number">6</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu((self.fc2(x)))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># input = torch.rand([32, 3, 224, 224])</span></span><br><span class="line"><span class="comment"># alexnet = AlexNet()</span></span><br><span class="line"><span class="comment"># print(alexnet)</span></span><br><span class="line"><span class="comment"># output = alexnet(input)</span></span><br><span class="line"><span class="comment"># print(output)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2、TensorFlow实现"><a href="#2、TensorFlow实现" class="headerlink" title="2、TensorFlow实现"></a>2、TensorFlow实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : model_alexnet.py</span></span><br><span class="line"><span class="string"># @Time       ：</span></span><br><span class="line"><span class="string"># @Author     ：0399</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, Model, Sequential</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">AlexNet_v1</span>(<span class="params">im_height=<span class="number">224</span>, im_width=<span class="number">224</span>, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># tensorflow中的通道顺序是NHWC</span></span><br><span class="line">    input_image = layers.Input(shape=(im_height, im_width, <span class="number">3</span>), dtype=<span class="string">&quot;float32&quot;</span>)  <span class="comment"># [None, 224, 224, 3]</span></span><br><span class="line">    <span class="comment"># x = layers.Conv2D()</span></span><br><span class="line">    x = layers.ZeroPadding2D(((<span class="number">1</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>)))(input_image)  <span class="comment"># [None, 227, 227, 3]</span></span><br><span class="line">    <span class="comment"># (227 - 11 + 2*0) / 4 + 1 = 55  -&gt; [None, 55, 55, 48]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">48</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)  <span class="comment"># [None, 27, 27, 48]</span></span><br><span class="line">    <span class="comment"># 当stride=1且padding=same时， 表示输出尺寸与输入尺寸相同 -&gt;[None, 27, 27, 128]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">128</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 27, 27, 128] -&gt; [None, 13, 13, 128]</span></span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)</span><br><span class="line">    <span class="comment"># stride=1, padding=same, 输出不变 -&gt;[None, 13, 13, 192]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># -&gt;[None, 13, 13, 192]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># -&gt;[None, 13, 13, 128]</span></span><br><span class="line">    x = layers.Conv2D(filters=<span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, strides=<span class="number">1</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    <span class="comment"># -&gt;[None, 6, 6, 128]</span></span><br><span class="line">    x = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = layers.Flatten()(x)  <span class="comment"># [None, 128*6*6]</span></span><br><span class="line">    x = layers.Dropout(<span class="number">0.2</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 2048]</span></span><br><span class="line">    x = layers.Dense(<span class="number">2048</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    x = layers.Dropout(<span class="number">0.2</span>)(x)</span><br><span class="line">    <span class="comment"># [None, 2048]</span></span><br><span class="line">    x = layers.Dense(<span class="number">2048</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">    x = layers.Dense(num_classes)(x)</span><br><span class="line"></span><br><span class="line">    predict = layers.Softmax()(x)</span><br><span class="line">    <span class="built_in">print</span>(predict)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># model = models.Model(inputs=input_image, outputs=predict)</span></span><br><span class="line">    model = models.Model(inputs=input_image, outputs=predict)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet_v2</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet_v2, self).__init__()</span><br><span class="line">        self.features = Sequential([</span><br><span class="line">            <span class="comment"># [None, 224, 224, 3] -&gt; [None, 227, 227, 3]</span></span><br><span class="line">            layers.ZeroPadding2D(((<span class="number">1</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>))),</span><br><span class="line">            <span class="comment"># padding=&quot;valid&quot;表示向上取整 (227 - 11)/4 + 1=55 [None, 227, 227, 3]-&gt;[None, 55, 55, 48]</span></span><br><span class="line">            layers.Conv2D(filters=<span class="number">48</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            <span class="comment"># [55, 55, 48] -&gt; [None, 27, 27, 48]</span></span><br><span class="line">            layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">            <span class="comment"># stride=1, padding=same, 尺寸不变 [None, 27, 27, 48] -&gt;[None, 27, 27, 128]</span></span><br><span class="line">            layers.Conv2D(filters=<span class="number">128</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            <span class="comment"># [None, 27, 27, 128] -&gt;[None, 13, 13, 128]</span></span><br><span class="line">            layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">            <span class="comment"># [None, 13, 13, 128] -&gt; [None, 13, 13, 192]</span></span><br><span class="line">            layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            <span class="comment"># [None, 13, 13, 192] -&gt; [None, 13, 13, 192]</span></span><br><span class="line">            layers.Conv2D(filters=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            <span class="comment"># [None, 13, 13, 192] -&gt; [None, 13, 13, 128]</span></span><br><span class="line">            layers.Conv2D(filters=<span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            <span class="comment"># [None, 13, 13, 128] -&gt; [None, 6, 6, 128]</span></span><br><span class="line">            layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [None, 128*6*6]</span></span><br><span class="line">        self.flatten = layers.Flatten()</span><br><span class="line">        self.classifier = Sequential([</span><br><span class="line">            layers.Dropout(<span class="number">0.2</span>),</span><br><span class="line">            layers.Dense(<span class="number">1024</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            layers.Dropout(<span class="number">0.2</span>),</span><br><span class="line">            layers.Dense(<span class="number">128</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">            layers.Dense(num_classes),</span><br><span class="line">            layers.Softmax()</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># input = tf.random.uniform(shape=(16, 224, 32, 3))</span></span><br><span class="line"><span class="built_in">input</span> = tf.random.uniform(shape=(<span class="number">8</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># alexnet = AlexNet_v2(num_classes=5)</span></span><br><span class="line"><span class="comment"># print(alexnet)</span></span><br><span class="line"><span class="comment"># print(alexnet.call(input))</span></span><br><span class="line">AlexNet_v1(<span class="number">224</span>, <span class="number">224</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、AlexNet简介&quot;&gt;&lt;a href=&quot;#1、AlexNet简介&quot; class=&quot;headerlink&quot; title=&quot;1、AlexNet简介&quot;&gt;&lt;/a&gt;1、AlexN</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://guudman.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
