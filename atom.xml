<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ai4Future</title>
  
  
  <link href="https://guudman.github.io/atom.xml" rel="self"/>
  
  <link href="https://guudman.github.io/"/>
  <updated>2023-12-10T10:52:06.451Z</updated>
  <id>https://guudman.github.io/</id>
  
  <author>
    <name>AI4Future</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MultiHead_Self-Attention</title>
    <link href="https://guudman.github.io/2023/12/10/MultiHead-Self-Attention/"/>
    <id>https://guudman.github.io/2023/12/10/MultiHead-Self-Attention/</id>
    <published>2023-12-10T10:50:02.000Z</published>
    <updated>2023-12-10T10:52:06.451Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>之前构造了attention layer和Self-Attention layer， 本文通过基本组件搭建深度神经网络。</p><p>首先通过attention构建multi-head attention。之前定义的Self-Attention层， 输入是一个序列， x1到xm， Self-Attention有三个参数矩阵， Wq， Wk， Wv， 输出是一个序列， 这样的Self-Attention被称为single head。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207092738528.png" alt="image-20231207092738528"></p><p>Multi-Head Self-Attention是由多个单头Self-Attention组成。它们各自有各自的参数， 不共享参数。每个单头Self-Attention有三个参数矩阵， 所以Multi-Head Self-Attention有3l个参数矩阵， 这样就得到了多头注意力。所有单头Self-Attention都有相同的输入， 输入都是x序列， 但它们的参数矩阵各不相同， 所以输出的c序列也各不相同。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207093155341.png" alt="image-20231207093155341"></p><p>把l个单头Self-Attention输出的序列做concatination堆叠起来， 作为多头Self-Attention的输出，堆叠起来的c向量变得更高。如果每个单头的输出都是d×m的矩阵， 那么多头的输出就是(ld)×m的矩阵。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207093420804.png" alt="image-20231207093420804"></p><p>上面用l个单头Self-Attention构造出多头Self-Attention， 也可以用l个单头attention构造多头attention。所有单头attention的输入都是两个序列， x1到xm， 以及x’1到x‘m。每个单头attention都有各自的参数矩阵， 它们不共享参数， 每个单头都有自己的输出序列c1到ct， 把单头输出的c堆叠起来就是多头的输出。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207093741980.png" alt="image-20231207093741980"></p><h4 id="2、网络搭建"><a href="#2、网络搭建" class="headerlink" title="2、网络搭建"></a>2、网络搭建</h4><h5 id="2-1-stacked-Self-Attention-layer"><a href="#2-1-stacked-Self-Attention-layer" class="headerlink" title="2.1 stacked Self-Attention layer"></a>2.1 stacked Self-Attention layer</h5><p>已经构造出多头Self-Attention与多头attention， 接下来用这两种层搭建一个深度神经网络。首先用多头Self-Attention与全连接层搭建一个encoder网络。</p><p>下面是刚才定义的多头Self-Attention层， 输入是序列x1到xm， 输出是序列c1到cm。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207094140540.png" alt="image-20231207094140540"></p><p>然后再搭建一个全连接层， 把向量c1作为输入， 全连接层把c1乘到参数矩阵Wu上， 然后再用Relu或其他激活函数得到输出向量u1。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207094351283.png" alt="image-20231207094351283"></p><p>把同一个全连接层用到c2上， 同样的操作得到输出u2。注意两个全连接才能够是完全相同的， 它们的参数矩阵都是Wu。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207094529036.png" alt="image-20231207094529036"></p><p>依次类推一直输出Um， 得到m个输出向量u， 这些全连接层都是一样的， 有同一个参数矩Wu。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207094643915.png" alt="image-20231207094643915"></p><p>上面搭建了两层， 一个多头Self-Attention， 一个全连接层。输入是m个向量x1到xm。输出也是m个向量u1到um。ui向量在xi向量之上， 但它不仅仅依赖于xi， ui依赖于所有的m个向量。改变任何一个x向量， ui都会发生变化， 当然对ui影响最大的还是xi。</p><p>可以继续搭建更多层。再搭一个多头Self-Attention层和一个全连接层。想搭多少层都可以， 这样就可以搭建出一个深度神经网络， 道理跟多层RNN是一样的。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207095307748.png" alt="image-20231207095307748"></p><h4 id="3、transformer-encoder网络"><a href="#3、transformer-encoder网络" class="headerlink" title="3、transformer encoder网络"></a>3、transformer encoder网络</h4><p>现在来搭建transformer模型的encoder网络。 一个block有两层， 一个多头Self-Attention层， 一个全连接层。输入是512×m的矩阵， 输出也是512×m的矩阵。这里的m是输入序列x的长度， 每个x向量都是512维的。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207095830644.png" alt="image-20231207095830644"></p><p>这是encoder网络的结构， 输入是512×m的矩阵x， x的每一列都是512维的词向量。下面是刚刚定义的block， 它有两层， 一个Self-Attention层， 一个全连接层。输出也是512×m的矩阵。输入和输出的大小一样， 所以可以用resnet中的捷径分支进行连接， 把输入加到输出上。</p><p>然后搭第二个block， 输出还是512乘以m的矩阵， 想搭多少个block都可以。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207095942388.png" alt="image-20231207095942388"></p><p>transformer的encoder网络一共搭了6个blocks， 每个block有两层， 每个block都有自己的参数。block之间不共享参数， 最终输出是512乘以m的矩阵。输出与输入的大小是一样的。</p><p>通过堆叠多个Self-Attention与全连接层搭建出encoder网络， encoder网络有6个block， 每个block有两层。现在进一步用attention层， transformer的decoder网络。</p><h4 id="4、transformer-decoder网络"><a href="#4、transformer-decoder网络" class="headerlink" title="4、transformer decoder网络"></a>4、transformer decoder网络</h4><p>transformer是一个seq2seq模型， 它有一个encoder和一个decoder。输入是两个序列， 如果要把英语翻译成德语， 那么x的序列就是英语的词向量， x’是德语的词向量。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207100532043.png" alt="image-20231207100532043"></p><p>上面搭建的encoder网络， 它有6个block， 每个block有两层， 包含一个Multi-Head Self-Attention层与一个全连接层。encoder的输入和输出都是512维的向量， 输入序列有m个词向量， x1到xm。输出序列也有m个词向量， u1到um。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207101039748.png" alt="image-20231207101039748"></p><p>现在开始搭建decoder网络的一个block。block的第一层是一个多头Self-Attention层， 输入是序列x1‘到xt’， 输出是序列c1到ct， 它们全都是512维的向量。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207101342981.png" alt="image-20231207101342981"></p><p>第二层是多头的attention层， 第一层的输入是两个序列。一个序列是u1到um。 他们是encoder网络的输出。另外一个序列是刚刚得到的c1到ct， 多头attention的输出是z1到zt， 它们也都是512维的向量。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207101554220.png" alt="image-20231207101554220"></p><p>最后再搭建一个全连接层， 输入是512维的向量z1， 输出是512维的向量s1。全连接层都一样， 都是把参数矩阵Ws与输入的z向量相乘， 然后用relu激活函数， 得到向量s1。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207101747445.png" alt="image-20231207101747445"></p><p>同理再把z2作为输入， 得到全连接层的输出s2。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207101848021.png" alt="image-20231207101848021"></p><p>依次类推， 把所有的z向量映射到s向量。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207101942372.png" alt="image-20231207101942372"></p><p>上面是搭建的decoder网络的一个block。 可以发现decoder的一个block包含三层， 分别是Self-Attention层， attention层以及全连接层。</p><p>再重复一遍， decode的一个block中包含三层， 分别是Self-Attention层， attention层以及全连接层。这三层组成decoder的一个block。这个block需要两个输入序列， 两个序列都是512维的向量， 两个序列的长度分别是m个t。如果把英语翻译成德语， 那么m是英语句子的长度， t是德语句子的长度。这个block输出序列的长度是t，每个向量都是512维的。</p><p> <img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207102410836.png" alt="image-20231207102410836"></p><h4 id="5、搭建transformer"><a href="#5、搭建transformer" class="headerlink" title="5、搭建transformer"></a>5、搭建transformer</h4><p>将上面的所有模块拼接起来得到最终的transformer模型。</p><p>首先是encoder网络。 encoder网络很简单， 依次叠加6个block， 每个block有两层（Self-Attention层+dense层）。 encoder网络的输入是矩阵x， 它有m列， 每列是512维的词向量。输出是矩阵u， 它跟输入矩阵x的大小完全一样。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207104301908.png" alt="image-20231207104301908"></p><p>之前已经搭建了decoder网络的一个block， 它的输入是两个序列， 输出是一个序列。block1是decoder网络的最底层模块， 左边输入序列是512×m的矩阵U， 也是encoder网络的输出。右边输入序列是512xt的矩阵x’。这个模块输出512×t的矩阵， 大小跟右边的输入x‘一样。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207104536860.png" alt="image-20231207104536860"></p><p>再堆叠decode中的一个block。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207104600988.png" alt="image-20231207104600988"></p><p>依次类推， 一共堆叠了6个block。 组成了decoder网络。每个网络有三层， 分别是Self-Attention层， attention层以及全连接层。每个block有两个输入序列和一个输出序列。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207104737291.png" alt="image-20231207104737291"></p><p>左边的encoder网络和右边的decoder网络组合起来就是transformer模型。最终输出的序列是t个向量， 每个向量都是512维的。</p><p>现在回顾一下RNN seq2seq模型。它有两个输入序列， encoder输入是x1到xm。 decoder输出是x1’到xt’。transformer模型也有两个输入序列， 输出也是t个向量。可以看出RNN seq2seq与transformer模型， 二者输入大小完全一样， 输出大小也完全一样。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207105304644.png" alt="image-20231207105304644"></p><p>所有RNN seq2seq模型能做的， transformer模型也能做。</p><p>以英语德语翻译为例。</p><p>输入是一系列单词，经过embedding层映射之后得到x1到xm向量， m个向量经过6个block堆叠而成的Encoder网络之后输出m个向量， u1、u2到um。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207105539101.png" alt="image-20231207105539101"></p><p>将encoder的输出向量u作为decoder网络的输入。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207105736380.png" alt="image-20231207105736380"></p><p>encoder的输出u加上新的输入x‘1经过6个block堆叠而成的decoder之后得到输出y1。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207105917333.png" alt="image-20231207105917333"></p><p>通过随机采样得到第一个德语单词， 并将其作为decoder网络的输入序列x2’</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207110033276.png" alt="image-20231207110033276"></p><p>依次类推完成整个句子的翻译。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207110102318.png" alt="image-20231207110102318"></p><h4 id="6、single-head-到Multi-Head"><a href="#6、single-head-到Multi-Head" class="headerlink" title="6、single head 到Multi-Head"></a>6、single head 到Multi-Head</h4><p>利用上述单头Self-Attention搭建多头Self-Attention。方法很简单， 就是用多个单头Self-Attention， 它们各自有各自的参数， 都把矩阵x作为输入， 分别输出各自的c矩阵。每个c矩阵的大小都是d×m， m是输入序列的长度。</p><p>把所有得到的c矩阵做concatination得到更高的矩阵， 这个矩阵就是多头Self-Attention的输出。同样的方法也可以得到多头的attention的输出。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207110732945.png" alt="image-20231207110732945"></p><p>encoder就是用多头Self-Attention和全连接层搭建的， 每个block有两层， 分别是多头Self-Attention层和全连接层。</p><p>decoder有三层， 分别是多头Self-Attention层， 多头attention层以及全连接层。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231207111218667.png" alt="image-20231207111218667"></p><p>transformer是seq2seq模型， 它有encoder网络和decoder网络， 可以用来做机器翻译。</p><p>transformer没有RNN， 不是循环结构。transformer完全基于attention和全连接层。transformer与RNN的输入输出大小完全都一样，transformer在nlp上的效果非常好， 所以transformer已经是业界的标准。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;之前构造了attention l</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://guudman.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Self-Attention</title>
    <link href="https://guudman.github.io/2023/12/10/Self-Attention-1/"/>
    <id>https://guudman.github.io/2023/12/10/Self-Attention-1/</id>
    <published>2023-12-10T10:48:19.000Z</published>
    <updated>2023-12-10T10:48:19.223Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer">]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="https://guudman.github.io/2023/12/09/Transformer/"/>
    <id>https://guudman.github.io/2023/12/09/Transformer/</id>
    <published>2023-12-09T06:34:33.000Z</published>
    <updated>2023-12-09T06:35:32.737Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>transformer模型完全基于attention， attention原本是用在RNN上的， 这里把RNN去掉， 只保留attention。</p><p>transformer是一种很新的模型， 2017年才发表， 比attention晚两年。论文的名字叫做attention is all you need。这里讲解transformer的模型结构， transformer模型很复杂， 这里主要重点解释attention模块。</p><p>transformer模型是一种seq2seq模型， 它有一个encoder和一个decoder。很适合做机器翻译， transformer不是循环神经网络。transformer没有循环结构， 只有attention和全连接层。transformer的实验效果非常惊人， 可以完爆最好的RNN+transformer。 机器翻译问题已经没有人使用RNN了， 业界都是用transformer+bert。</p><p>现在告诉你， 可以去掉RNN， 只保留attention， 然后用attention来搭建一个深度神经网络来代替RNN。那么你会怎么做， 现在按照这个思路， 从0开始搭建一个基于attention的神经网络。</p><h4 id="2、RNN-attention回顾"><a href="#2、RNN-attention回顾" class="headerlink" title="2、RNN+attention回顾"></a>2、RNN+attention回顾</h4><p>这里先从之前学过的RNN+attention模型入手。剥离RNN保留attention， 然后搭建attention层与Self-Attention层。然后将这些层组装起来， 搭建出来的模型就是transformer。</p><p>之前用attention改进seq2seq模型， seq2seq模型有一个encoder和一个decoder， encoder的输入是m个向量x1到xm。encoder把这些信息压缩到状态向量h中， 最后一个状态hm是对所有输入的概括。</p><p>decoder是一个文本生成器， 依次生成状态s， 然后根据状态s生成单词。把新生成的单词记为下一个输入x’。如果用attention的话， 还要计算context vector c， 每计算一个状态s就得到一个c。</p><p>具体是这样计算context vector c的， 首先把decoder当前状态sj与encoder所有m个状态h1到hn做对比， 用align函数计算它们的相关性。把计算的数值αij作为权重。这里的i是encoder的状态h的下标， j是decoder的状态h的下标。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206145554018.png" alt="image-20231206145554018"></p><p>每计算一个context vector c， 就要计算出m个权重αij到αmj， 每个α对应一个状态h。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206150058170.png" alt="image-20231206150058170"></p><p>首先看一下α权重是如何计算的。权重是hi与sj的函数， 把向量hi乘到矩阵Wk上， 得到向量k:i, 把向量sj乘到WQ上， 得到向量q:j。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206150335721.png" alt="image-20231206150335721"></p><p>Wk和WQ是align函数的参数， 参数需要从训练数据中学习。</p><p>把sj这个向量与encoder所有m个状态向量h做对比。有m个hi向量， 所以有m个ki向量。用这些ki向量组成大的k矩阵， 每个ki向量都是矩阵k的列。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206150908029.png" alt="image-20231206150908029"></p><p>计算矩阵大K转置与向量q:j的乘积， 结果是m维的向量， 用softmax函数输出m维的向量α:j。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206151152386.png" alt="image-20231206151152386"></p><p>把α:j的m个元素即α1j, α2j， 一直到αmj， 这些元素全部介于0到1之间，而且它们它们相加等于1， 这样得到m个权重。</p><p>上面把decoder的状态sj以及encoder状态h分别做线性变换， 得到向量q:j与k:i，它们被称为query和key。 query的意思是用来匹配key值， key的意思是用来被query匹配。我们拿一个query向量q:j去对比所有m个k向量， 算出m个权重αij。这个m个α说明query和每个key的匹配程度， 匹配程度越高， 权重α越大。</p><p>除此之外还要计算value向量vi， 把hi乘到矩阵Wv上， 得到向量vi。矩阵Wv也是个参数矩阵， attention中一共有三个参数矩阵， Wq， Wk， Wv。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206152114762.png" alt="image-20231206152114762"></p><p>上面提到的query， key和value， 这三个向量是怎么计算的。先把decoder当前状态sj映射到query向量， 具体是将参数矩阵Wq与decoder状态sj相乘， 得到query向量qj。</p><p>然后把encoder所有m的状态h1到hm映射到m个k向量。用参数矩阵Wk与第i个状态hi相乘得到k个向量ki。对于h1到hm所有m个状态向量做这种变换，得到m个k向量。 把这m个k向量表示为矩阵大k向量， 向量ki是矩阵大ki的第ki列。</p><p>用矩阵大k与向量qj计算m维的权重。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206152852828.png" alt="image-20231206152852828"></p><p>向量αj， 向量αj的m个元素分别是α1j， α2j一直到αmj， 每一个元素对应一个h向量。</p><p>接下来计算value向量vi， 拿encoder的第i个状态hi与参数矩阵Wv相乘， 得到一个value向量vi， 把所有的m个状态h1到hm都做这种变换， 得到m个value向量， 每个vi对应一个encoder状态hi。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206153443180.png" alt="image-20231206153443180"></p><p>现在有了m个权重α以及m个value向量v， 用α做权重。把m个value向量v做加权平均， 把结果作为新的context vector cj。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206153627435.png" alt="image-20231206153627435"></p><p>context vector cj等于α1j乘以vi一直加到αj乘以vm， 这种计算权重α和context vector cj的方法就是transformer中的attention。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206153815163.png" alt="image-20231206153815163"></p><h4 id="3、剥离RNN只保留attention"><a href="#3、剥离RNN只保留attention" class="headerlink" title="3、剥离RNN只保留attention"></a>3、剥离RNN只保留attention</h4><p>attention原本是用在RNN上， 如何剥离RNN只保留attention。</p><p>先来设计一个attention层， 用于seq2seq模型。我们移除了RNN， 现在开始搭建attention。还是考虑seq2seq模型， 它有一个encoder和一个decoder。encoder的输入向量是x1到xm， decoder的输入是x‘1到x’t。举个例子， 要把英语翻译成德语， 英语句子里面有m个单词， 变成了词向量x1到xm。 decoder依次生成德语单词x‘1到x’t， 把当前生成的德语单词作为下一轮的输入。现在已经生成了t个德语单词， 把它们作为输入来生成下一个单词，新生成的德语单词会作为第t+1个输入。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206160019959.png" alt="image-20231206160019959"></p><p>我们不用RNN只用attention， 首先拿encoder的输入x1， x2到xm计算key和value， 用矩阵Wk把向量xi变成ki， 用矩阵Wv把向量xi变成vi。</p><p>于是xi就变成了向量ki和vi。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206160429890.png" alt="image-20231206160429890"></p><p>然后把decoder的输入x‘t，x’2一直到x‘t做线性变换， 用矩阵Wq把x’j映射到q:j。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206160615507.png" alt="image-20231206160615507"></p><p>decoder有t个输入向量， 所以得到t个query向量。q1、q2、q3一直到qt。 注意一下， 一共有三个参数，encoder有两个，分别是矩阵Wk， Wv用来计算key和value。decoder中有一个， 用来计算query。</p><p>现在开始计算权重α。拿第一个query向量q1与所有m个k向量做对比， 通过比较q1与k1、k2、k3、…一直到km这m个向量的相关性。算出m个权重值即为这个m维的向量α1。具体是这样计算的：把所有m个k向量表示为矩阵大k， 大k的转置乘以向量q1， 再做softmax变换得到这个m维的向量α1。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206161353638.png" alt="image-20231206161353638"></p><p>然后计算context vector c1， 需要用到权重向量α1与所有m个value向量v1、v2、v3一直到vm。c1是指m个v向量的加权平均权重， 权重就是α1向量的m个元素。分别是α11， α21， 一直到αm1。把向量v1到vm作为矩阵大v的列， 那么这个加权平均就可写成矩阵大v乘以向量α1。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206161726037.png" alt="image-20231206161726037"></p><p>现在计算权重向量α2， 要用到第二个query向量q2以及所有m个k向量k1， k2， k3一直到km。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206161944923.png" alt="image-20231206161944923"></p><p>然后计算context vector c2， 要用到权重α2， 以及所有m个value向量v1、v2、v3一直到vm。 c2是m个v向量的加权平均权重， 权重就是向量α2的m个元素。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206162143868.png" alt="image-20231206162143868"></p><p>依次类推计算所有的context vector c， 每个c对应一个x‘， decoder输入一共有t个x’向量， 所以计算出t个c向量。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206162344590.png" alt="image-20231206162344590"></p><p>这t个context vector c就是最终的输出，可以用矩阵大C来表示这些向量， c1、c2、c3一直到ct都是矩阵大c的列。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206162533643.png" alt="image-20231206162533643"></p><p>要计算一个向量cj， 要用到所有的v， 所有的k以及一个qj向量。比如向量c2依赖于q2以及所有的k向量和v向量。c2依赖于decoder的一个输入x‘2以及encoder全部输入x1， x2一直到xm。</p><p>举个例子， 把英语翻译成德语。所以显然可以用attention layer代替RNN， attention layer的好处是不会遗忘。向量c2是直接用所有m个英文词向量x1到xm算出来的， 所以c2知道整句英语。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206163112769.png" alt="image-20231206163112769"></p><p>把attention层记为函数attn， 输入是矩阵x和x’， x的列是encoder的m个输入向量x1、x2…xm。x‘的列是x’1、x‘2, …, x’t。 attention层有三个参数矩阵， Wq， Wk， Wv，它们通过训练数据来学习。attention层的输出是矩阵C， 它的列向量是c1， c2， c3…ct。</p><p>可以这样表示attention层， 有两个输入序列， x和x’。 有一个输出序列cm， 每个c向量对应一个x‘向量。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206163714309.png" alt="image-20231206163714309"></p><p>上面研究了seq2seq模型， 删掉了RNN并且搭建了一个attention层。可以用attention层来做机器翻译。接下来搭建一个self attention层。原理完全一样， 可以用Self-Attention代替RNN。</p><h4 id="4、用Self-Attention代替RNN"><a href="#4、用Self-Attention代替RNN" class="headerlink" title="4、用Self-Attention代替RNN"></a>4、用Self-Attention代替RNN</h4><p>下图是上面搭建的attention层， attention层用于seq2seq， 它有两个输入序列， x和x’， 用attn函数表示attention层。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206164109220.png" alt="image-20231206164109220"></p><p>Self-Attention层不是seq2seq模型， 它只有一个输入序列。这就像普通的RNN一样。Self-Attention层也可以用attn函数来表示。这个函数跟前面的attn完全一样， 区别在于函数的输入。 attention层的输入是x和x‘， 而Self-Attention层的输入只有x， 两个输入都是x， 输出序列是c1到cm。输出序列的长度跟输入是一样的， 都是m。 每个c向量对应一个x向量， 但要注意ci并非只依赖于xi， 而是依赖于所有m个x向量。改变任何一个x， ci都会发生变化。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206164859036.png" alt="image-20231206164859036"></p><p>Self-Attention层的原理跟前面的attention层完全一样， 只是输入不一样。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206165038130.png" alt="image-20231206165038130"></p><p>原理是这样的， 第一步做三种变换， 把xi映射到qi, ki和vi， 参数矩阵还是WQ， Wk， Wv。</p><p>线性变换之后x1被映射到q1， k1， v1， q2， k2， v2。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206165346732.png" alt="image-20231206165346732"></p><p>然后计算权重向量α， 公式还是一样的， 矩阵k转置乘以qj向量， 然后做softmax， 得到m维向量αj。α1依赖于q1以及所有的k向量， k1， k2， k3， …，km。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206165705515.png" alt="image-20231206165705515"></p><p>同理权重向量α2依赖于q2以及所有的k向量， k1， k2， …, km</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206165854360.png" alt="image-20231206165854360"></p><p>依次类推， 计算出所有的权重α， 一共有m个α向量， 每个向量都是m维的。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206170211842.png" alt="image-20231206170211842"></p><p>现在开始计算context vector c， c1是所有m维v向量的加权平均， 权重都是α。 c1依赖于权重向量α:1以及所有m个v向量。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206170448475.png" alt="image-20231206170448475"></p><p>依次类推得到m个c向量， 得到c1， c2， …， cm</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206170542090.png" alt="image-20231206170542090"></p><p>这m个c向量就是Self-Attention层的输出。</p><p>第j个输出cj是这样计算出来的，它依赖于矩阵V， 矩阵K以及向量qj。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206170841530.png" alt="image-20231206170841530"></p><p>因为cj依赖于所有的k和所有的v， 所以cj依赖于所有m个x向量x1到xm。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206171031066.png" alt="image-20231206171031066"></p><p>上面介绍了Self-Attention， 输入是一个序列。Self-Attention层有三个参数矩阵， 三个矩阵把每个x映射到q， k， v三个向量， 输出也是一个序列， 从c1到cm这m个向量。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206171243465.png" alt="image-20231206171243465"></p><h4 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h4><p>1、attention最早出现在2015年的论文中， 用于改进seq2seq。</p><p>2、如果只有一个RNN网络， attention就叫做Self-Attention。 Self-Attention最早出现在2016年的论文中。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206172038846.png" alt="image-20231206172038846"></p><p>3、再后来发现去掉RNN单独用attention效果反而更好。2017年的论文attention is all you need中提到了transformer模型。</p><p>4、首先从RNN+attention下手， 剥离RNN， 只保留attention。这样就搭建了attention层， 可以代替RNN做机器翻译。然后把attention变成Self-Attention， 不局限于seq2seq。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206172049386.png" alt="image-20231206172049386"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;transformer模型完全基</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://guudman.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Self_Attention</title>
    <link href="https://guudman.github.io/2023/12/08/Self-Attention/"/>
    <id>https://guudman.github.io/2023/12/08/Self-Attention/</id>
    <published>2023-12-08T00:56:46.000Z</published>
    <updated>2023-12-08T00:57:22.420Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>seq2seq模型有两个RNN网络， 一个encoder， 一个decoder。这里介绍self-attention， 把attention用在一个RNN网络上。</p><p>attention的第一篇文章发表在2015年， 用来改进seq2seq模型， 解决RNN的遗忘问题。其实attention并不局限于seq2seq模型， attention可以用在所有的RNN上。下面介绍Self-Attention， 文章发表在2016年， 比第一篇attention论文晚一年。</p><p>Self-Attention的原始论文把attention用在LSTM上， 把它们的论文做了简化。为了方便理解， 把LSTM换成simple RNN。</p><p>现在开始simple RNN与Self-Attention的结合，初始的时候状态向量h与context vector c都是全零向量。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206110900013.png" alt="image-20231206110900013"></p><p>RNN读入第一个输出x1， 需要更新状态h。把x1的信息压缩到新的状态h里面。标准的simple RNN是这样计算状态h1的。新的状态h1依赖于旧的状态h0与新的输入x1。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206111151354.png" alt="image-20231206111151354"></p><p>上面的公式中， 新的输入x1与旧的状态h做concatination， 然后把得到的向量乘到参数矩阵A上。用tanh做激活函数， 对括号中的向量做非线性变换， 输出向量h1就是新的状态。</p><p>用Self-Attention的话， RNN这样更新状态。用context vector c0取代h0， 其余的都一样， 唯一的区别就是h0换成了c0。可以用其他的方法来更新状态h， 比如可以把x1， c0还有旧的状态h0这三个向量做concatination， 而不是x1和c0这两个向量。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206111730500.png" alt="image-20231206111730500"></p><p>算出新的状态h1， 下一步就是计算新的context vector c1。</p><p>新的context vector c1是已有状态h的加权平均。 由于初始状态h0是全零向量， 我们忽略到h0， 那么已有状态的加权平均就等于h1， 所以新的context vector c1就等于h1。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206112256331.png" alt="image-20231206112256331"></p><p>有了状态h1与context vector c1。 下一步要计算新的状态h2， 用这个公式来计算状态h2， h2是c1和x2的函数。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206112453746.png" alt="image-20231206112453746"></p><p>下面计算新的context vector c。要计算c， 首先要计算权重α， 权重向量αi是状态向量h1与h2的相关性， 拿当前状态与已有状态做对比， 包括h2跟自己做对比。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206112745041.png" alt="image-20231206112745041"></p><p>这样可以计算出两个权重α。每个α对应一个h， 对已有的两个状态h1和h2做加权平均来计算c。 由于h0是全零向量， 以后就忽略h0。</p><p>加权平均的结果就是新的context vector c2。c2=α1h1 + α2h2。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206113059697.png" alt="image-20231206113059697"></p><p>输入新的向量x3， RNN就会更新状态h。 新的状态h3依赖于x3， c2， 用同样的公式计算新的状态h3。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206113309763.png" alt="image-20231206113309763"></p><p>现在有了状态h3， 该计算新的context vector c3。首先计算权重α， 拿当前状态h3与已有状态h1、h2、h3三个向量做对比，计算出权重α1， α2， α3。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206113559698.png" alt="image-20231206113559698"></p><p>每个权重α对应一个h， 对三个状态h做加权平均， 加权平均的结果就是context vector c3。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206113703609.png" alt="image-20231206113703609"></p><p>重复这个过程， 以此类推。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206113749707.png" alt="image-20231206113749707"></p><h4 id="2、总结"><a href="#2、总结" class="headerlink" title="2、总结"></a>2、总结</h4><p>Self-Attention每一轮更新状态前都会用context vector c看一遍之前的所有状态， 这样就不会遗忘之前的状态。</p><p>Self-Attention可以用在所有的RNN上， 不会局限于seq2seq。</p><p>除了避免遗忘， Self-Attention还可以帮助RNN关注相关的信息。 下图是论文中的插入， RNN从左往右读入一句话， 红色是当前的输入， 高亮标注的是权重α很大的位置。这些权重α说明全文中最相关的词是哪几个。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206114151066.png" alt="image-20231206114151066"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;seq2seq模型有两个RNN网</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://guudman.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Attention</title>
    <link href="https://guudman.github.io/2023/12/07/Attention/"/>
    <id>https://guudman.github.io/2023/12/07/Attention/</id>
    <published>2023-12-07T11:48:39.000Z</published>
    <updated>2023-12-07T11:49:41.568Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>先回顾seq2seq模型， seq2seq模型有一个encoder和一个decoder。encoder的输入是英语， decoder把英语翻译成德语。encoder每次读入一个英语向量x， 在状态h中积累输入的信息。最后一个状态hm中积累了所有词向量x的信息。encoder输出最后ig状态hm， 把之前的状态向量全部扔掉。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205170348294.png" alt="image-20231205170348294"></p><p>decoder RNN的初始状态s0等于encoder RNN的最后一个状态hm。hm包含了输入英语句子的信息， 通过hm， decoder就知道了这句英语。然后decoder就像文本生成器一样， 逐字生成一句德语。这句德语就是模型生成的翻译。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205170659672.png" alt="image-20231205170659672"></p><p>但seq2seq模型有一个明显的缺陷， 就是如果输入的句子很长， 那么encoder会记不住完整的句子。encoder最后一个状态可能会漏掉一些信息。假如输入的英语里有个别词被忘记了， 那么decoder就无从得知完整的句子， 也就不可能产生正确的翻译。</p><p>如果你拿seq2seq模型做机器翻译， 你会得到这样的结果。 横轴是输入句子的长度。纵轴是BLEU， BLEU score是评价机器翻译好坏的标准。BLEU score越高， 说明机器翻译越准确。如果不用attention， 当输入的句子超过20个单词的时候， BLEU score就会往下掉。这是因为LSTM会遗忘， 造成翻译出错。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205171208705.png" alt="image-20231205171208705"></p><p>用attention的话会得到这条红色的线。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205171251689.png" alt="image-20231205171251689"></p><p>即便输入的句子很长， attention曲线中的BLEU score也不会往下掉。用attention改进seq2seq模型， 解决seq2seq遗忘问题。</p><p>attention是2015年提出的， 用attention， decoder每次更新状态时， 都会看一遍encoder所有状态，这样不会遗忘。</p><p>attention还会告诉decoder应该关注encoder哪个状态， 这就是attention名字的由来。</p><p>attention可以大幅度提高准确率， 但是attention的缺点是计算量非常大。</p><h4 id="2、attention原理"><a href="#2、attention原理" class="headerlink" title="2、attention原理"></a>2、attention原理</h4><p>在encoder已经结束工作后， attention与decoder同时开始工作， 回顾一下decoder的初始状态s0是encoder的最后一个状态hm。</p><p>encoder的所有状态h1、h2一直到hm都保留下来， 这里计算s0与每一个h的相关性。用align公式表示相关性， 计算encoder第i个状态hi与decoder当前状态s0的相关性。把结果记作αi， αi被称为权重weight。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205174131738.png" alt="image-20231205174131738"></p><p>encoder一共有m个状态， 所以一共算出m个α。从α1一直到αm， 都是介于0到1之间的实数， 所以α加起来等于1。</p><p>下面看一下如何进行计算。有多种方法计算hi与s0的相关性。第一种方法是这样的， 把hi与so做concatination得到更高的向量，然后计算矩阵w与这个向量的乘积，再用tanh应用到向量的每个元素上。把每个元素都压缩到-1和1之间， tanh的输出还是一个向量。 最后计算向量v与刚计算出来的向量的内积， 两个向量内积是个实数， 记作αi。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205174849776.png" alt="image-20231205174849776"></p><p>这里的向量v与矩阵w都是参数， 需要从训练数据中学习。</p><p>计算出α~i到α~m这m个实数之后， 对它们做softmax变换。把输出结果记为α1， α2， …， αm。由于α是softmax的输出， α1到αm都大于0， 而且相加等于1。这种计算权重α的方法是attention第一篇论文提出的。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205175259698.png" alt="image-20231205175259698"></p><p>之后很多论文提出过其他计算权重的方法。</p><p>第二种计算权重的方法：</p><p>输入还是hi和s0， 第一步分别用两个参数矩阵wk和wq对两个输入向量做线性变换， 得到ki和q0这两个向量，这两个参数矩阵要从训练数据中学习。</p><p>第二步是计算ki与q0的内积， 把结果记作α~i， 由于有m个k向量， 所以得到m个α~i。</p><p>第三步对α’i到α‘m做softmax变换， 把输出记作α1到αm。 α1到αm都是正数， 而且相加等于1。这种计算权重的方法被transformer模型采用， transform模型是当前很多nlp问题的state of the art。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205175830385.png" alt="image-20231205175830385"></p><p>刚才讲了两种方法计算hi与s0的相关性， 随便用哪种方法都会得到m个α。这些α被称为权重， 每个αi对应一个encoder状态hi。</p><p>利用这些权重α， 可以对这m个权重向量h做加权平均， 计算α1h1， 一直到αmhm。把加权平均的结果记作c0， c0称为context vector。每一个context vector都会对应一个decoder状态， context vector c0对应decoder状态s0。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206091948919.png" alt="image-20231206091948919"></p><p>decoder读入向量x’1， 然后需要把状态更新为s1。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206092612386.png" alt="image-20231206092612386"></p><p>具体怎么计算新的状态s1？回顾一下， 如果不用attention， 那么simple RNN是这样更新状态的。新的状态s1是输入x’1与旧的状态s0的函数。看一下simple RNN的计算公式， 把x‘1与s0做concatination， 然后乘到参数矩阵A’上， 加上intercept向量b， 得到新的状态s1。simple RNN在更新状态时， 只需知道新的输入x‘1与旧的状态s0， simple RNN并不会看encoder的状态。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206093039714.png" alt="image-20231206093039714"></p><p>用attention的话， 更新decoder状态的时候， 需要用到context vector c0， 把x’t, s0, c0做concatination， 用它们来计算新的状态s1。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206093511051.png" alt="image-20231206093511051"></p><p>已经更新了decoder的状态s1， 回忆一下c0是encoder所有状态h1到hm的加权平均。所以c0知道encoder的输入x1到xm的完整信息。decoder新的状态s1依赖于context vector c0， 这样一来， decoder也知道encoder完整的输入， 于是RNN遗忘问题就解决了。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206093854163.png" alt="image-20231206093854163"></p><p>下一步计算context vector c1， 跟之前一样。</p><p>先计算权重α， αi是encoder第i个状态hi与decoder当前状态s1的相关性， 把decoder状态s1与encoder所有m个状态对比， 计算出m个权重记作α1到αm。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206094239683.png" alt="image-20231206094239683"></p><p>注意， 虽然上一轮计算C0的时候算出了m个权重α， 但我们现在不能用那些α，必须要重新计算α。上一轮计算的是h与s0的相关性， 这一轮计算的是h与s1的相关性。 这里用了相同的符号α， 然而现在这m个权重α跟上一轮计算出来的α不一样， 现在不能重复使用上一轮算出来的α。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206094829890.png" alt="image-20231206094829890"></p><p>有了权重α， 就可以计算新的context vector c1， c1是encoder的m个状态向量h1到hm的加权平均， c1等于α1h1一直加到αmhm。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206095359354.png" alt="image-20231206095359354"></p><p>decoder接收新的输入x’2， 然后把状态s1更新到s2。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206095616450.png" alt="image-20231206095616450"></p><p>s2是新输入x‘2、旧状态s1以及context vector c1的函数， 还是用这个公式来计算decoder新的状态。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206100033530.png" alt="image-20231206100033530"></p><p>已经算出了新的状态s2， 下一步是计算新的context vector c2。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206100356914.png" alt="image-20231206100356914"></p><p>把decoder当前状态s2与encoder所有状态h1到hm做对比， 计算出权重α1一直到αm。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206100526507.png" alt="image-20231206100526507"></p><p>有了α1到αm这些权重， 把encoder的状态h1到hm做加权平均， 把加权平均的结果记作c2。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206101626812.png" alt="image-20231206101626812"></p><p>然后再更新状态s3， 然后计算c3， 不断重复， 依次更新状态s， 计算context vector c， 再更新状态s再计算context vector c一直到结束。在计算context vector c的过程中， 一共计算了多少个α。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206101942514.png" alt="image-20231206101942514"></p><p>每计算一个context vector c, 需要把decoder当前状态s与encoder所有m个状态h做对比， 计算出m个权重α1一直到αm，所以decoder每一轮更新都需要重新计算m个权重α。</p><p>假设decoder运行了t步， 那么一共计算了mt个权重α。所以attention的时间复杂度是mt。也就是encoder与decoder状态数量的乘积。这个时间复杂度很高的， attention避免了遗忘， 大幅度提高预测准确率， 但是代价也是巨大的。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206102325594.png" alt="image-20231206102325594"></p><p>已经介绍完attention的原理， 现在用这个例子来说明权重α的实际意义。这张图下面encoder输入是英语， 上面是decoder。把英语翻译成德语， attention会把decoder每个状态与encoder每个状态做对比， 得到二者的相关性， 也就是权重α。</p><p>下图中用线连接每个decoder状态与encoder状态， 每条线对应一个权重α。粗的线表示α很大， 细的线表示α很小。线越粗， 说明相关性很大。</p><p>这条粗线可以这样解释， 法语里面的zone就是英语里面的area。所以这两个状态的相似度很高。每当decoder想要生成一个状态时， 都会看一遍encoder的所有状态。这些权重α告诉decoder应该关注什么地方， 这就是attention名字的由来。当decoder需要计算这个状态的时候， 权重α告诉decoder应该关注encoder的这个状态。 这帮助decoder产生正确的状态， 从而生成正确的法语单词。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206103127572.png" alt="image-20231206103127572"></p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231206105328780.png" alt="image-20231206105328780"></p><h4 id="3、总结"><a href="#3、总结" class="headerlink" title="3、总结"></a>3、总结</h4><p>如果使用标准的seq2seq模型， decoder基于当前状态产生下一个状态， 这样产生的新状态可能已经遗忘了encoder的部分输入。如果使用attention， decoder在产生下一个状态前， 会先看一遍encoder的所有状态， 于是decoder就知道encoder的完整信息， 并不会遗忘。除了解决遗忘问题， attention还能告诉decoder应该关注encoder的哪一个状态， 这就是attention名字的由来。</p><p>attention可以大幅度提升seq2seq模型的表现。但attention也是有缺点的， 缺点就是计算量太大了。假设输入encoder序列的长度为m， decoder输出的序列长度为t。标准的seq2seq模型只需要让encoder读一遍输入序列， 之后就不会再看encoder的输入或者状态了， 然后让decoder依次生成输出的序列， 所以时间复杂度是m+t。</p><p>但attention中， 时间复杂度会高很多。decoder每更新一个状态都会把encoder的状态先看一遍， 所以时间复杂度是m。decoder自己有t个状态， 所以总的时间复杂度是mt。使用attention可以提升准确率， 但需要付出更多的计算量。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;先回顾seq2seq模型， se</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://guudman.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Seq2Seq</title>
    <link href="https://guudman.github.io/2023/12/07/Seq2Seq/"/>
    <id>https://guudman.github.io/2023/12/07/Seq2Seq/</id>
    <published>2023-12-07T11:32:24.000Z</published>
    <updated>2023-12-07T11:33:34.952Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>使用RNN来做机器翻译， 机器翻译模型有很多种， 这里介绍sequence-2-sequence模型， 机器翻译是many-2-many多对多的问题。输入的英语长度大于1， 输出的德语长度也大于1，而且输入和输出的长度不固定。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205095703255.png" alt="image-20231205095703255"></p><p>做任何机器学习的应用， 第一步都是处理输出。这里就取一个小规模数据集就行。可用该网站<a href="[Tab-delimited Bilingual Sentence Pairs from the Tatoeba Project (Good for Anki and Similar Flashcard Applications">英语翻译语料</a> (manythings.org)](<a href="http://www.manythings.org/anki/))提供的小规模数据集来训练一个seq-2-seq模型。这个网站上有多种语言翻译的数据，">http://www.manythings.org/anki/))提供的小规模数据集来训练一个seq-2-seq模型。这个网站上有多种语言翻译的数据，</a> 这个文件是德语和英语的翻译。文件中， 坐标是英语句子， 右边是德语句子， 给定一句英语， 如何翻译结果呢， match其中一个德语句子就算完全正确。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205111531811.png" alt="image-20231205111531811"></p><p>处理数据时， 把这些句子用矩阵表示， 首先做预处理， 比如把大小变成小写， 去掉标点符号。</p><p>处理之后就要做tokenization， 把一句话变成很多个单词或者字符。做tokenization时， 要用两个不同的tokenizer， 英语用一个， 德语用一个。tokenization之后要建立两个字典， 一个英语字典， 一个德语字典。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205111946725.png" alt="image-20231205111946725"></p><p>tokenization可以是character level也可以是word level， 二者都可以。character level tokenization会把一句话分割成很多个字符， word level tokenization会把一句话分割成很多单词。</p><p>为了方便起见， 这里使用character level tokenization, 一句话变成一个list， list中每个元素变成一个字符。但实际的机器翻译都是用word level tokenization， 因为它们数据集足够大。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205112729018.png" alt="image-20231205112729018"></p><p>刚才提到为什么要用两个不同的tokenizer， 这里解释一下。在字符层面， 不同的语言通常有不同的alphabet字母表。 英语有26个拉丁字母， 如果区分大小写就有53个字母。</p><p>德语也有26个拉丁字母， 但还有4个不常用字母。中文没有字母， 而是有几千个汉字。日语字符更复杂， 有46个片假名， 还有几百个汉字。两种语言的字符通常是不同的。所以应该用两种不同的tokenizer， 各有各的字母表。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205113032240.png" alt="image-20231205113032240"></p><p>如果是word level tokenization， 就更应该用两种不同的tokenizer和不同的字典。英语和德语的词汇完全不一样， 绝大多数德语单词在英语字典里面找不到。此外， 不同的语言有不同的分词方法， 汉语、日语和欧洲语言的分词方法就不一样。</p><p>keras提供的库叫作tokenization， 会自动生成字典。左边是英语字典， 一共有27个字符，包含26个字母和一个空格。27个字符分别对应27个数字。右边是德语字典， 删除了不常用的字符， 只保留了26个字母和一个空格。任务是把英语翻译成德语。英语是原语言， 德语是目标语言。要往目标语言-德语的字典里添加两个符号。一个起始符\t， 一个终止符\n。拿什么做起止符和终止符都可以， 只要不跟字典里面已有的字符冲突即可。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205114322805.png" alt="image-20231205114322805"></p><p>tokenization结束之后， 每句话就变成一个字符的列表， 并且生成一个英语字典和一个德语字典。用这个字典就可以把每个字符映射成一个整数。这样一句话就变成了一个sequence， 每个元素是一个整数。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205114607373.png" alt="image-20231205114607373"></p><p>可以进一步把每个数字用one-hot向量表示， 做完one-hot encoding，每个字符用一个向量表示， 每句话用一个矩阵表示，这个矩阵就是RNN的输入。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205115058404.png" alt="image-20231205115058404"></p><h4 id="2、模型搭建"><a href="#2、模型搭建" class="headerlink" title="2、模型搭建"></a>2、模型搭建</h4><p>下面开始搭建一个seq2seq模型， 并且开始训练这个模型。 seq2seq模型有一个encoder编码器和一个decoder解码器。encoder是LSTM或其他的RNN模型。用来从输入的英语句子中提取特征。encoder最后一个状态就是从输入的句子中提取的特征， 包含这句话的信息。encoder其余的状态没有用， 都被丢弃， encoder的输出是LSTM最后的状态h以及最后的传输带C。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205115436983.png" alt="image-20231205115436983"></p><p>seq2seq还有一个decoder用来生成德语。这个decoder其实就是上次讲解的text generation。与上次的text generation文本生成器唯一的区别在于初始状态。上次用的文本生成器的初始状态是一个全零向量。这里的decoder的初始状态是encoder的最后一个状态。encoder最后一个状态h和c是从输入的英语句子中提取的特征向量。概括了输入的英语句子， decoder靠这个状态来知道这句英语是go away。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205115939703.png" alt="image-20231205115939703"></p><p>再强调一下， decoder的初始状态是encoder的最后一个状态。通过encoder最后一个状态， decoder得知输入的英语句子是go away。现在decoder开始生成德语句子， decoder是个LSTM模型， 它每次接受一个输入， 然后输出对下一个字符的预测。第一个输入必须是起始符， 用\t表示。这就是为什么要在德语字典中加入起始符。decoder会输出一个概率分布， 记作向量p。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205143042665.png" alt="image-20231205143042665"></p><p>起始符后面德语的第一个字母是m， 把m做one-hot encoder， 作为标签y， 用标签y和预测p的cross entropy作为损失函数。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205143231577.png" alt="image-20231205143231577"></p><p>希望预测p尽量接近y， 所以损失函数越小越好。有了损失函数就可以反向传播计算梯度。梯度会从损失函数传到decoder， 然后再从decoder一直传到encoder。然后用梯度下降来更新decoder和encoder的模型参数， 让损失函数减小。</p><p>然后输入是两个字符， 起始符与字母m。 decoder会输出对下一个字符的预测，记为向量p。输入的字符串的下一个字符是字母a。把a做one hot encoding, 作为标签y。损失函数是标签y与预测p的cross entropy， 然后用反向传播计算梯度， 然后更新decoder和encoder。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205144742691.png" alt="image-20231205144742691"></p><p>再下一个输入是3个字符， 起始符， 字母m和字母a。LSTM输出对下一个字符的预测记为向量p。真实的下一个字母是c， 所以标签y是字母c的one hot向量。同样的道理， 用反向传播计算梯度， 然后做梯度下降更新encoder和decoder。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205144931769.png" alt="image-20231205144931769"></p><p>不断重复这个过程， 直到德语的最后一个字符。</p><p>最后一轮， 把整句德语作为decoder输入， 所以用停止符的one-hot向量作为标签y。希望输出的预测尽可能接近标签， 也就是停止符。然后再做一次反向传播， 再更新一次模型参数，不断重复这个训练过程， 拿所有的英语德语二元组来训练decoder和encoder。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205145303874.png" alt="image-20231205145303874"></p><h4 id="3、训练"><a href="#3、训练" class="headerlink" title="3、训练"></a>3、训练</h4><p>如果用keras、pytorch或TensorFlow等来搭建一个seq2seq模型， 你需要这么做。encoder的输入是英文句子的one hot encoding，用一个矩阵表示。encoder网络有一层或多层LSTM用来从英文句子中提取特征， LSTM的输出是最后一个状态h与最终的传输带c。decoder网络的初始状态是h与c， 这样可以让encoder和decoder连起来。在做反向传播时， 梯度可以顺着这条线从decoder传播到encoder。decoder网络的输入是德语的上半句话，decoder输出当前状态h， 然后全连接层输出对下一个字符的预测。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205145946456.png" alt="image-20231205145946456"></p><p> 训练好了seq2seq模型， 可以拿它把英语翻译成德语。把一句英语的每个字符输入encoder， encoder会在状态h和c中积累这句话的信息。encoder输出最后的状态记作h0和c0， 它们是从这句话里面提取的特征， h0和c0被送给了decoder。</p><h4 id="4、推理"><a href="#4、推理" class="headerlink" title="4、推理"></a>4、推理</h4><p>encoder的输出h0和c0被记为decoder的初始状态。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205150620310.png" alt="image-20231205150620310"></p><p>这样decoder就知道输入的英文句子是go away。现在decoder就跟文本生成器一样工作。 首先把起始符输入decoder， 有了新的输入， decoder就会更新状态h和传输带c并预测下一个字符， 并输出一个概率分布。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205150911295.png" alt="image-20231205150911295"></p><p>根据概率分布来做抽样， 得到字符词， 然后把c记录下来。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205153418706.png" alt="image-20231205153418706"></p><p>不断重复这个过程， 更新状态并且生成新的字符， 然后用新生成的字符作为下一轮的输入。</p><p>运行14轮之后的状态是h14和c14。上一轮生成的字符是字母e， 现在拿它作为输入， 根据decoder输出的概率分布做抽样， 可能碰巧抽到了终止符。一旦抽到终止符， 就终止文本生成并返回记录下来的字符串，这个字符串就是模型翻译得到的德语。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205153842144.png" alt="image-20231205153842144"></p><p>总结上述过程。</p><p>使用seq2seq模型做机器翻译模型， 模型有一个encoder网络和一个decoder网络。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205154704103.png" alt="image-20231205154704103"></p><p>encoder的输入是一句英语， 每输入一个词， RNN会更新状态。把输入的信息积累在encoder状态里。</p><p>encoder最后一个状态就是从英文句子里提取的特征， encoder只输出最后一个状态， 会扔掉之前的所有状态。</p><p>把最后一个状态传递给decoder网络。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205154933938.png" alt="image-20231205154933938"></p><p>把encoder的最后一个状态作为decoder的初始状态。初始化后， decoder网络就知道输入的英文句子了， 然后decoder作为一个文本生成器生成一句德语。首先把起始符作为decoder RNN的输入， decoder RNN会更新状态s1。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205155202969.png" alt="image-20231205155202969"></p><p>然后全连接层输出的预测概率记为p1，根据概率分布p1做抽样得到下一个字符记作z1。</p><p>decoder将z1作为输入更新状态s2， 并输出预测的概率为p2， 根据p2抽样得到新的字符z2。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205155713056.png" alt="image-20231205155713056"></p><p>同样的道理decoder网络拿z2作为输入更新状态s3。不断重复这个过程， 抽样得到新的字符记作z， 然后拿z作为下一轮输入更新状态s以及计算概率分布p。</p><p>如果抽到了停止符， 那么就终止文本生成， 返回生成的序列。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205155849425.png" alt="image-20231205155849425"></p><h4 id="5、seq2seq模型改进"><a href="#5、seq2seq模型改进" class="headerlink" title="5、seq2seq模型改进"></a>5、seq2seq模型改进</h4><p>针对上述的seq2seq模型， 如何做模型改进。</p><p>seq2seq模型的原理是这样的。encoder处理输入的英语句子， 把信息压缩到状态向量里面， encoder最后一个状态是整句话的一个概要。理想情况下， encoder最后一个状态包含整句英语的完整信息， 当然那是理想情况。</p><p>假如英语句子太长， LSTM就会遗忘。假如英语里面有些信息被遗忘了。那么encoder就不可能有英语句子的完整信息， decoder生成的德语肯定会有遗漏。</p><p>一种很显然的改进就是用双向LSTM代替单向LSTM。双向LSTM有两条链， 一条从左到右， 一条从右到左。两条链独立运行，分别从输入中提取特征， 从左到右的最后一个状态ht可能会遗忘最左边的输入。从右到左的这条链的最后一个状态ht’会记住最左边的信息， 把ht和ht‘结合起来，就能更好的记住所有的输入。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205161216936.png" alt="image-20231205161216936"></p><p>综上， 单向LSTM换成双向LSTM可以更好的记住输入的句子，但decoder必须是单向LSTM。decoder就是一个文本生成器， 必须按照顺序生成文本， 所以decoder不能用双向LSTM。</p><p>上面用了character level tokenization， 这样比较方便， 不需要用embedding层。但最好还是用word level tokenization。 原因是这样的， 英文平均每个单词有4.5个字母， 如果用单词代替字符，那么输入的序列就能缩短4.5倍， 序列更短就更不能遗忘。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205161624960.png" alt="image-20231205161624960"></p><p>但是想要用word level tokenization， 需要有足够大的数据集。用word level tokenization得到的vocabulary大约是1万。因此one hot向量的维度大约是1万， 必须用word embedding 得到更低维的向量。embedding层的参数量太大了， 用小的数据集无法得到很好的训练。embedding层会有overfitting问题，得有足够大的训练数据集，或者对embedding层做预训练，  才能避免overfitting。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205162306236.png" alt="image-20231205162306236"></p><p>另外一种改进方法是multi task learning， 多任务学习。encoder读入一句英语， 把英语句子概括成最终的状态向量h和c。decoder通过h和c获取英语句子的信息， 然后生成一句德语。训练时比较decoder的预测与真实的德语单词， 从而获得损失函数和梯度，用来更新decoder和encoder。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205162811779.png" alt="image-20231205162811779"></p><p>把英语翻译成德语是一个任务， 还可以多加几个任务。比如把英语句子翻译成英语句子本身。添加一个decoder， 让它根据h和c来生成英语句子。这样一来， encoder只有一个而训练数据多了一倍，所以encoder可以被训练得很好。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205163020849.png" alt="image-20231205163020849"></p><p>还可以添加其他很多任务，英文翻译成很多其他语言的数据集。可以利用这些数据更好的训练encoder。</p><p>比如还可添加更多任务，把英语翻译成法语， 西班牙语等， 添加更多的decoder， 让这些decoder生成各种语言。但encoder只有一个。如果用10种语言做训练， 那么训练encoder的数据就多了10倍， encoder可以训练的更好。目标是把英语翻译成德语， 通过借助其他语言可以把encoder变得更好。 虽然德语decoder没有改进， 但是翻译的效果还是会变好。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231205163630778.png" alt="image-20231205163630778"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;使用RNN来做机器翻译， 机器翻</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://guudman.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>TextGeneration</title>
    <link href="https://guudman.github.io/2023/12/03/TextGeneration/"/>
    <id>https://guudman.github.io/2023/12/03/TextGeneration/</id>
    <published>2023-12-03T06:53:05.000Z</published>
    <updated>2023-12-03T06:55:36.238Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>可以训练一个RNN， 它它自动生成文本， 生成的文本就像人写的一样。如果用莎士比亚的书来训练RNN， 生成的文本就像莎士比亚写的一样， 举个例子来解释文本生成。</p><p>输入本句话the cat sat on the ma, 要求训练一个神经网络来预测下一个字符。训练数据是很多文本， 把文本分割成字符， 用one-hot encoding来表示字符，这样每个字符就表示成一个one hot向量。把这些one-hot向量依次输入到RNN， RNN的状态向量h会积累看到的信息， RNN返回最后一个状态向量h。RNN层上面是一个softmax分类器， 把h与参数矩阵w相乘， 得到一个向量，经过softmax函数的变换， 最终输出是一个向量。向量每个元素都在0~1之间，元素全加起来等于1， softmax分类器的输出其实就是一个概率分布。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231203094511669.png" alt="image-20231203094511669"></p><p>假设这个神经网络已经训练好了， 我们把the cat sat on the mat输入到这个神经网络中， 神经网络最上层softmax分类器会输出这些概率值。每个字符对应一个概率值， 其中概率最大的是字符t， 大小约为0.175。有了这些概率值， 我们就可以预测下一个字符了。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231203095036762.png" alt="image-20231203095036762"></p><p>有了概率值就可以选择概率最大的字符t， 也可以按照概率值值随机抽样。假设我们抽到的是字符t， 然后就把t接到输入的文本末尾。</p><p>将得到的the cat sat on the mat这句话作为输入， 计算下一个字符的概率分布，从而生成下一个字符。</p><p>下一轮抽到的字符可能是句号。于是这句话就变成了the cat sat on the mat. 再重复这个过程， 下一次可能抽到空格， 再下次可能会抽到一个字母， 不断重复下去可以生成一段话， 一篇文章甚至一本书。</p><hr><h4 id="2、训练"><a href="#2、训练" class="headerlink" title="2、训练"></a>2、训练</h4><p>现在看一下究竟如何训练RNN， 训练数据是文本， 比如英文维基百科的所有文章。 把文章划分成很多片段，这些片段可以有重叠， 比如红色片段作为输入的文本， 后面的蓝色字符a作为标签。把红色的这个片段输入到神经网络， 希望神经网络做出的预测是这个蓝色的字符a。假设设置的片段的长度segment length等于40，意思就是红色的片段有40个字符。设置的步长等于3， 意思是下一个片段会往右平移三个字符的长度。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231203100346156.png" alt="image-20231203100346156"> </p><p>平移三个字符后， 这是下一个片段。从字符h到空格， 中间这40个红色的字符会被用作输入文本。蓝色的字母i是标签。由于设置的stride等于3， 这个片段会往右移动三个字符的长度。假设文章大约有3000个字符， 那么会得到大约1000个红色的片段和1000个蓝色的标签。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231203100806843.png" alt="image-20231203100806843"></p><p>这些红色的片段会被用于神经网络的输入， 蓝色的字符会被作为标签。训练数据就是把这些片段和标签的pairs。假如一篇文章有3000个字符， 这篇文章会被切分成约1000个pairs。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231203101405424.png" alt="image-20231203101405424"></p><p>训练神经网络的目的是给定输入的片段， 神经网络可以预测下一个字符，其实就是一个多分类的问题。 加入包括字母空格标点在内， 一共有50个不同的字符，那么分类类别的数量就是50。输入一个片段， 输出50个概率值， 每个类别对应一个概率值。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231203101650618.png" alt="image-20231203101650618"></p><p>文本生成可以用来做一些有趣的事情， 有人拿8000个英文名字做训练数据， 用来生成新的英文名字。这些全都是神经网络生成的名字。绝大多数都是新的，没有出现在训练数据里面， 这些名字看起来很像英文名， 但是绝大多数都没有人用过。这说明文本生成器并不是记住训练数据， 然后重复训练数据。相反， 它能生成新的东西， 能生成新的东西就是一个好的性质。</p><h4 id="3、代码实现"><a href="#3、代码实现" class="headerlink" title="3、代码实现"></a>3、代码实现</h4><p>随便拿一本书作为训练数据， 用python读取这本书得到一个很长的字符串。大约有60万个字符， 这是前1000个字符。字符主要是拉丁字母， 空格， 标点以及换行符号。为了方便把所有字母都变成小写字母。</p><p>首先来准备训练数据， 原始数据长度是60多万的字符串， 每次取长度为60的片段作为输入， 片段后面的字符作为label。将这个片段存到segment列表中， 字符存到next_chars列表中。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231203103458120.png" alt="image-20231203103458120"></p><p>完成文本切分后， 需把一个字符变成一个向量。于是一个片段就变成了一个矩阵。首先建立一个字典， 把每个字符映射到一个整数， 比如a映射到1， b映射到2。在之前的讲解中， 一个token就是一个word， 这里一个token就是一个字符。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231203105145783.png" alt="image-20231203105145783"></p><p>然后把每个正整数用one-hot向量表示。做完one-hot encoding， 一个字符串就被编码成一个数值矩阵。如果这个字符串的长度是8个字符， 那么这个矩阵就有8行。之前对于word level在得到one hot向量之后还要进一步做word embedding，用一个低维向量表示一个词。这里面不需要embedding层， 原因是这样的， 之前用word level tokenization把一句话分成多个单词， 英语里大约有1万个常用词。所以vocabulary设置为1万。那么one-hot encoding得到的one-hot向量是1万维。维度太高了， 所以要用word embedding把高维的one hot向量变成低维的词向量。而这里用的是character level tokenization， 把一句话切成很多字符， 文本里通常也就是几十个常用字符， 比如字母， 数字， 标点。所以字典里面的vocabulary最多也就是100个。做one hot encoding得到的one hot向量最多也就只有100维。维度已经够低了， 所以不需要进一步做embedding。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231203112637419.png" alt="image-20231203112637419"></p><p>训练时， vocabulary里面的v设置为57， 意思是字典里面一共有57个不同的字符。包括26个小写字母、空格、标点符号等等。之前设置每个滑动窗口的长度是60， 每个片段里面有60个字符。于是长度为60的片段就变成了60×57的矩阵。每个字符用57维的one hot向量表示， 标签是这个片段的下一个字符， 这个字符编码成了57维的one hot向量。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231203113149654.png" alt="image-20231203113149654"></p><p>接下来搭建一个神经网络， 第一层是LSTM， 输入是60×57的矩阵， 每个片段被编码成了60×57的矩阵。LSTM层状态向量h设置为128。注意这里只用了单向LSTM， 不能用双向LSTM。文本生成需要预测下一个字符， 必须是从前往后，这是一个单向问题。LSTM之后再加上一个全连接层， 用softmax激活函数。这一层的输出是57维的向量， 向量的每一个元素是一个字符的概率值， 字典里面有57个不同的字符， 于是输出的向量是57维的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.LSTM(<span class="number">128</span>, input_shape=(window_size, <span class="built_in">len</span>(char_dict))))</span><br><span class="line">model.add(layers.Dense(<span class="built_in">len</span>(char_dict), activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">optimizer = keras.optimizers.RMSprop(lr=<span class="number">0.001</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, optimizer=optimizer)</span><br><span class="line"></span><br><span class="line">model.fit(x, y, batch_size=<span class="number">32</span>, epochs=<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>介绍三种选择字符的方法， 一种是greedy selection。哪个字符的概率最大，就选择哪个字符。python中的np.argmax()就得到下一个字符的index， 根据index就能得到该字符。这种方法是确定的， 没有随机性。给定初始的几个字符， 后面生成的文本完全是确定的，完全取决于初始的输入。这种确定性的文本生成并不好， 我们希望生成的文本尽量多元化，这样才能得到很多有意思的结果。</p><p>第二种方法是多项式随机抽取。随机抽样需要用到函数np.random.multionmial()。假如一个字符的概率等于0.3， 那么它被选中的概率就是0.3， 这样抽样过于随机生成的文本会犯很多拼写和语法错误。</p><p>第三种方法介于二者之间。它有随机性， 但随机性并不是很大。具体做法是使用temperature调整概率值。这里要用到介于0到1之间的数temperature。把概率值做这种逆变换， 然后再归一化，把概率值做这种变换后大的概率值会变大，小的概率值会变小。极端情况下最大的概率值会变成1， 其余的概率值会变成0， 这就相当于第一种确定性的选择。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231203144042774.png" alt="image-20231203144042774"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;可以训练一个RNN， 它它自动生</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://guudman.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>make_RNN_more_efficient</title>
    <link href="https://guudman.github.io/2023/12/02/make-RNN-more-efficient/"/>
    <id>https://guudman.github.io/2023/12/02/make-RNN-more-efficient/</id>
    <published>2023-12-02T07:56:23.000Z</published>
    <updated>2023-12-02T08:22:40.189Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、stacked-RNN"><a href="#1、stacked-RNN" class="headerlink" title="1、stacked RNN"></a>1、stacked RNN</h4><p>3个技巧提升RNN的效果。</p><p>第一个是Stacked RNN。stack RNN为多层RNN， 可以把很多全连接层堆叠起来， 构成一个multilayer percepter， 也可把很多卷积层堆叠起来， 构成一个深度卷积网络。同样的道理也可以把很多RNN堆叠起来构成一个多层RNN网络。神经网络的每一步都会更新状态h， 新算出来的h有两个copy， 一份送到下一个时刻， 另外一份作为输出。这一层输出的状态h成为了上一层的输入。再解释一下， 最底层的RNN的输入是词向量x， 这层RNN会输出每一步的状态向量h， 这些输出的状态向量又成为了第二次RNN的输入。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201172900627.png" alt="image-20231201172900627"></p><p>第二次RNN有自己的模型参数， 会更新和输出自己的状态向量h， 第二层输出的状态向量h又成为了第三层RNN的输入。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201173043957.png" alt="image-20231201173043957"></p><p>一共有三层RNN， 最上层的状态向量是最终的输出， 可以用最后一个状态ht看成是从最底层的输入I love the movie so much中提取的特征向量。</p><p>使用keras实现多层LSTM， 这里用了三层LSTM层。第一层的输出会成为第二层的输入， 所以第一层的return_sequences要设置为true， 要输出所有的状态向量h。 同样的道理第二次的return sequence也要设置为true， 要输出所有的状态向量h，这些状态向量会成为下一层LSTM的输入。第三层也是最后一层LSTM的return_sequence设置为false， 只需要输入最后一个状态向量就行， 可以把前面所有的状态向量都扔掉，最后一层是全连接层， 拿第三层LSTM的状态向量作为输入， 这一层输出分类结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> LSTM, Embedding, Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line">vovabulary = <span class="number">10000</span></span><br><span class="line">embedding_dim = <span class="number">32</span></span><br><span class="line">word_num = <span class="number">500</span></span><br><span class="line">state_dim = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(vovabulary, embedding_dim, input_length=word_num))</span><br><span class="line">model.add(LSTM(state_dim, return_sequences=<span class="literal">True</span>, dropout=<span class="number">0.2</span>))</span><br><span class="line">model.add(LSTM(state_dim, return_sequences=<span class="literal">True</span>, dropout=<span class="number">0.2</span>))</span><br><span class="line">model.add(LSTM(state_dim, return_sequences=<span class="literal">False</span>, dropout=<span class="number">0.2</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201174343499.png" alt="image-20231201174343499"></p><p>上图所示为神经网络的概要， embedding层的输出是500乘以32的矩阵， 500的意思是每条电影评论有500个单词， 每个词用32维的向量表示， 这个500×32的句子成为了第一层LSTM的输入第一层LSTM的输出是500×32的句子， 这里的32是状态向量h的维度。由于设置return sequence为true， 所以第一层LSTM的输出看所有的500个状态， 这个500×32的矩阵又成为了第二层LSTM的输入。第二层LSTM输出了500×32的句子， 它又成了第三层LSTM的输入。设置第三层LSTM的return sequence为false， 所以第三层LSTM的输出是一个32维的向量， 它是第三层LSTM的最后一个状态。它相当于从输入的500个单词里提取的特征向量。这4个数字都是32， 其实只是一个巧合， 完全可以让这4个32分别取不同的值。</p><h4 id="2、Bidirectional-RNN"><a href="#2、Bidirectional-RNN" class="headerlink" title="2、Bidirectional RNN"></a>2、Bidirectional RNN</h4><p>RNN跟人的阅读习惯一模一样， 从左往右逐个单词阅读， 人阅读的过程在大脑里面积累提取出的信息。RNN阅读的过程在状态向量h中积累信息，读完一段电影评论就知道是正面还是负面评论。人类总是从左往右， 从前往后阅读， 但这只是我们的阅读习惯而已。对RNN来说， 从前往后或从后往前阅读并没有太大的区别。</p><p>所以很自然的想法就是训练两条RNN， 一条从左往右， 另一条从右往左，两条RNN完全独立， 不共享参数，也不共享状态。两条RNN各自输出自己的状态向量， 然后把它们的状态向量做concatination， 记作向量y。如果有多层RNN， 就把输出的这些向量y作为上一层RNN的输入。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231202143233217.png" alt="image-20231202143233217"></p><p>如果只有一层， 把这些y向量丢掉就可以了。只保留两条链最后的状态向量， 分别是ht和ht‘。把这两个状态向量的concatination作为从输入文字中提取的特征向量，根据它来判断电影评论是正面还是负面的。双向RNN总比单向RNN的效果好。原因可能是这样的， 不管是simpleRNN还是LSTM， 这些RNN模型都会或多或少的忘掉早先的输入。如果让RNN从左往右阅读， 那么最后一个状态ht可能会遗忘掉左边的输入， 如果让RNN从右往左阅读， 最后一个状态ht‘往往可能会记作靠左边的输入。将ht和ht’结合起来， 这样RNN就不会遗忘最开始看到的词了。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231202143852599.png" alt="image-20231202143852599"></p><p>下面是双向LSTM的编程实现， 具体实现只需要在标准的LSTM层外面包裹一个Bidrectional层。这样LSTM就变成双向的了。这就是实现双向LSTM唯一需要改变的地方。双向LSTM只会保留两条链最后的状态， 输出两个状态向量的concatination， 其余状态向量都被扔掉了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> LSTM, Embedding, Dense, Bidirectional</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vocabulary = <span class="number">10000</span></span><br><span class="line">embedding_dim = <span class="number">32</span></span><br><span class="line">word_num = <span class="number">500</span></span><br><span class="line">state_dim = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(vocabulary, embedding_dim, input_length=word_num))</span><br><span class="line">model.add(Bidirectional(LSTM(state_dim, return_sequences=<span class="literal">False</span>, dropout=<span class="number">0.2</span>)))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p>下面是打印出来的神经网络的概要。这里设置状态向量h的维度是32， 而且return_sequence为false， 所以这一层的输出是两条链最后的两个状态向量。两个都是32维， 叠加起来就是64维。可以算一下这一层的参数的数量， 参数数量比单向LSTM要多一倍。这是因为两条链各自输出自己的参数模型。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231202144636221.png" alt="image-20231202144636221"></p><p>其实我们可以发现embedding层的参数量远大于LSTM以及Dense层的参数量。再多的改进似乎对整体的模型效果并没有带来太大的提升， 因此另外一个提升LSTM的方法是pretrain预训练。</p><h4 id="3、预训练"><a href="#3、预训练" class="headerlink" title="3、预训练"></a>3、预训练</h4><p>预训练在深度学习中非常常用。比如在训练卷积网络时， 如果网络太大而训练集不够大， 那么可以在imagenet等大数据上做预训练， 这样可以让神经网络有比较好的初始化， 也可以避免overfitting。训练RNN的时候也是一样的道理。比如这个神经网络中的embedding层有32万个参数， 而我们只有2万个训练样本， 这个embedding层太大了， 会导致模型overfitting， 解决方法就是对embedding层做预训练。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231202150052309.png" alt="image-20231202150052309"></p><p>预训练是这样做的， 首先找一个更大的数据集， 可以是情感分析的数据， 也可以是其他类型的数据。但是任务最好是接近情感分析的任务，也就是说最好学出来的词向量带有正面或负面的情感。两个任务越相似， 预训练之后的transfer效果就越好。有了大数据集之后要搭建一个神经网络， 这个神经网络的结构是什么样的都可以， 甚至不用是RNN只有这个神经网络有embedding层就行， 然后就是在大数据集上训练这个神经网络。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231202150652649.png" alt="image-20231202150652649"></p><p>训练完成之后， 把上面的层全部丢掉， 只保留embedding层和训练好的模型参数。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231202150938101.png" alt="image-20231202150938101"></p><p>然后再搭建我们自己的RNN网络。这个新的RNN网络跟之前用于预训练的神经网络可以有不同的结构。搭好之后， 新的RNN层和全连接层都是随机初始化的，而下面的embedding层的参数是预训练出来的。要把embedding层的参数固定住，不要训练这个embedding层， 只训练其他的层。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231202151700094.png" alt="image-20231202151700094"></p><p>总结：</p><p>SimpleRNN和LSTM都是两种不同的RNN， 通常使用LSTM而不是SimpleRNN。</p><p>尽可能使用Bi-RNN而不是RNN</p><p>Stack RNN效果往往比单一的RNN效果要好</p><p>embedding层采用预训练的方式获取。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、stacked-RNN&quot;&gt;&lt;a href=&quot;#1、stacked-RNN&quot; class=&quot;headerlink&quot; title=&quot;1、stacked RNN&quot;&gt;&lt;/a&gt;1</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://guudman.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>LSTM</title>
    <link href="https://guudman.github.io/2023/12/02/LSTM/"/>
    <id>https://guudman.github.io/2023/12/02/LSTM/</id>
    <published>2023-12-02T07:49:02.000Z</published>
    <updated>2023-12-02T08:23:08.762Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>LSTM是一种RNN模型， 是对Simple RNN的改进， LSTM可以避免梯度消失的问题， 可以有更长的记忆。<a href="https://mp.weixin.qq.com/cgi-bin/[(PDF">原论文</a> Long Short-term Memory (researchgate.net)](<a href="https://www.researchgate.net/publication/13853244_Long_Short-term_Memory))是1997年发表的。下图是LSTM原论文中的截取的图。">https://www.researchgate.net/publication/13853244_Long_Short-term_Memory))是1997年发表的。下图是LSTM原论文中的截取的图。</a></p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201115008764.png" alt="image-20231201115008764"></p><p>LSTM也是一种循环神经网络， 原理跟Simple RNN差不多。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201120002469.png" alt="image-20231201120002469"></p><p>每当读取一个新的输入x就会更新状态h， lstm的结构比simple RNN要复杂很多。Simple RNN只有一个参数矩阵， LSTM有4个参数矩阵， 下面看一下LSTM内部的具体结构。</p><h4 id="2、LSTM结构"><a href="#2、LSTM结构" class="headerlink" title="2、LSTM结构"></a>2、LSTM结构</h4><p>LSTM最重要的设计是这个传输带， 即为向量C。过去的信息通过传输带送到下一个时刻，不会发生太大的变化， LSTM就是通过传输传输带来避免梯度消失。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201141239230.png" alt="image-20231201141239230"></p><p>LSTM中有很多gate可以有选择的让信息通过， 先来看一下forget gate遗忘门。遗忘门由sigmoid function和Elementwise multiplication两部分组成。输入sigmoid是一个向量a， sigmoid作用到向量a的每个元素上， 把每个元素都压到0和1之间。sigmoid处理后的向量维度与输入sigmoid的向量维度相同。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201141457555.png" alt="image-20231201141457555"></p><p>计算出f之后需要计算传输带向量C和遗忘门向量f的elementwise multiplication。elementwise multiplication是这样计算的， 举个例子， c和f都是4维的向量， 把它们的每个元素分别乘起来就行， 所以elementwise multiplication的结果也是一个四维的向量。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201142533071.png" alt="image-20231201142533071"></p><p>遗忘门有选择的让传输带C的值通过， 假如f的一个元素都是零， 那么C对应的元素就不能通过， 对应的输出也是零。假如f的一个元素是1， 那么C对应的元素就全部通过， 对应的输出就是C本身。可以这里理解： f向量中元素的值可以看做向量C中对应元素的权重。</p><p>遗忘门f具体是怎么计算出来的， 看下面的结构图， ft是上一个状态ht-1和当前输入xt的函数。把状态ht-1与输入xt做concatination得到更高的向量， 然后计算矩阵wf与这个向量的乘积得到一个向量，再用sigmoid函数得到向量ft。向量ft的每一个元素都介于0到1之间， 遗忘门有个参数矩阵wf需要经过反向传播从训练数据中学习。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201143302177.png" alt="image-20231201143302177"></p><p>上面是遗忘门forget gate， 下面解释一下input gate输入门。输入门it依赖于旧的向量ht-1和新的输入xt。输入门it的计算与遗忘门很类似， 把旧的状态ht-1与输入xt做concatination， 得到更高维度的向量。然后计算矩阵Wi与这个向量的乘积得到一个向量， 最后再用sigmoid函数得到向量it。it的每一个元素都介于0到1之间， 输入门也有自己的参数矩阵wi， wi也需要通过反向传播从训练数据中学习得到。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201143923011.png" alt="image-20231201143923011"></p><p>还要计算new value C~t， C~t是个向量， 计算跟遗忘门和输入门都很像， 也是把旧的状态ht-1与新的状态xt做concatination， 再乘到参数矩阵Wc上， 区别在于激活函数不是sigmoid，而是tanh。所以计算得到的Ct向量都介于-1到1之间。计算new value C~t也需要一个单独的参数矩阵记作Wc。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201145112251.png" alt="image-20231201145112251"></p><p>上面已经计算出了遗忘门ft、new value C~t以及输入门it，我们还知道传输带旧的值Ct-1, 现在就可以更新传输带C了。用遗忘门ft和传输带旧的值Ct-1计算elementwise multiplication。 遗忘门ft和传输带旧的值Ct-1的维度是相同的向量，计算的乘积也是一个向量， 遗忘门ft可选择性的遗忘Ct-1中的一些元素。如果ft中的一个元素是0，那么Ct-1对应的元素也是0， 也就是Ct-1的元素选择性遗忘掉了。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201150105584.png" alt="image-20231201150105584"></p><p>上面选择性的遗忘掉Ct-1中的一些元素， 现在需要往传输带上添加新的信息， 计算输入门it和新的值C~t的elementwise multiplication。输入门it和新的值C~t都是维度相同的向量。它们的乘积也是维度相同的向量， 把乘积加到传输带上就行。 这样就完成了对传输带的一轮更新。用遗忘门删除了传输带上的一些信息， 然后加入新的信息， 得到传输到新的值Ct。</p><p>上面已经更新完传送带C了， 最后一步是计算LSTM的输出，也就是状态向量ht。ht是这么计算的， 首先计算output gate输出门Ot， 输出门Ot跟前面的遗忘门的计算基本一致。输入是旧的状态ht-1和当前输入xt， 通过concatination拼接得到更高维的向量。然后计算矩阵Wo与这个向量的乘积得到一个向量， 最后再利用sigmoid函数得到向量Ot。Ot的每个元素都介于0到1之间， 输出门也有自己的参数矩阵Wo， Wo也需要从训练数据中学习。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201150938968.png" alt="image-20231201150938968"></p><p>现在计算状态向量ht， 对传输带Ct的每个元素求tanh， 把元素压缩到-1和1之间， 然后求这两个元素的elementwise multiplication。红色向量是刚刚求解出来的输出门Ot， 将其与传输带输出的Ct做tanh后的结果相乘就得到了状态向量ht。</p><p>下面的结构图中显示，ht有两份copy， 一份copy传输到了下一步， 另外一份copy成了lstm的输出。到第t步位置，一共有7个向量输入到了LSTM， 可以认为所有这些x向量的信息都积累在状态ht里面。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201153919329.png" alt="image-20231201153919329"></p><h4 id="3、LSTM的参数量"><a href="#3、LSTM的参数量" class="headerlink" title="3、LSTM的参数量"></a>3、LSTM的参数量</h4><p>LSTM有遗忘门，输入门， new value以及输出门4个模块组成。这四个模块都有各自的参数矩阵W， 所以一共有4个参数矩阵。矩阵的行数是h的维度， 列数是h的维度加上x的维度。所以LSTM参数的数量是4乘以h的维度再乘以h的维度加上x的维度。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201161857150.png" alt="image-20231201161857150"></p><h4 id="4、LSTM代码实现"><a href="#4、LSTM代码实现" class="headerlink" title="4、LSTM代码实现"></a>4、LSTM代码实现</h4><p>LSTM的结构虽然复杂， 但是用keras实现起来非常简单， 跟simple RNN的实现几乎一样。用LSTM判断电影评论是正面还是负面， 跟simple RNN一样。让LSTM只输出最后一个状态向量ht， ht就是从一段500个词的电影评论中提取的特征向量， 然后输入线性分类器来判断评论是正面的还是负面的。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201163212223.png" alt="image-20231201163212223"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> LSTM, Embedding, Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line"><span class="comment"># unique words in the dictionary</span></span><br><span class="line">vocabulary = <span class="number">10000</span></span><br><span class="line"><span class="comment"># shape(x) = 32</span></span><br><span class="line">embedding_dim = <span class="number">32</span></span><br><span class="line"><span class="comment"># sequence length 每条评论500个单词， 去长补短</span></span><br><span class="line">word_num = <span class="number">500</span></span><br><span class="line"><span class="comment"># shape(h) = 32</span></span><br><span class="line">state_dim = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面是return_sequences=False的情况</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(vocabulary, embedding_dim, input_length=word_num))</span><br><span class="line"><span class="comment"># return_state=False表示只输出最后的ht</span></span><br><span class="line">model.add(LSTM(units=state_dim, return_sequences=<span class="literal">False</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p>打印的模型概要为：</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201170343815.png" alt="image-20231201170343815"></p><p>参数量8320是这样计算的，每个参数矩阵h的维度是32， 乘以h的维度32 再 加上x的维度32。keras默认使用intercept， 所有参数里面还有32个维度的向量， 一个矩阵和一个向量加起来一共有2080个参数。LSTM一共有4个参数矩阵和4个intercept的向量， 所以参数的数量等于2080×4， 一共8320个参数。</p><p> 8320 = 2080 × 4， 2080 = 32×（32 + 32）+ 32</p><p>dropout也可以用在LSTM层上，具体的方法比较复杂。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201171440573.png" alt="image-20231201171440573"></p><p>总结， LSTM与simple RNN的区别就是用了一条传输带让过去的信息很容易传输到下一个时刻， 这样就有了更长的记忆， LSTM的表现总是比simple RNN更好。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;LSTM是一种RNN模型， 是对</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://guudman.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>RNN</title>
    <link href="https://guudman.github.io/2023/12/01/RNN/"/>
    <id>https://guudman.github.io/2023/12/01/RNN/</id>
    <published>2023-12-01T09:50:00.000Z</published>
    <updated>2023-12-01T10:04:34.438Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>RNN循环神经网络在nlp领域有些过时了， 训练数据足够多时， RNN的效果不如transformers模型， 但在小规模问题上， RNN还是很有用。机器学习中经常用到的文本、语音等时序数据， 思考一下， 如何对时序数据建模。在上面的基础部分讲到把一段文字整体输入到一个logistics regression模型， 让模型做二分类， 这属于one-to-one模型。 一个输入对应一个输出， 全连接神经网络和卷积网络都是one-to-one模型，但人类并不会把一整段文字全部输入到大脑中。 人类阅读时会从左到右阅读一段文字， 阅读时逐渐在大脑中积累文本的信息， 阅读一段话后脑中积累了整段文字的大意。one-to-one模型要求一个输入对应一个输出， 比如输入一张图片， 输出每一类的概率值。one-to-one模型很适合图片的问题， 但不太适合文本的问题。</p><p>对于文本问题， 输入和输出长度并不固定。 一句话可长可短， 所以输入的长度并不固定， 输出的长度也不固定。 比如把英文翻译成汉语， 英语可能有10个单词， 但翻译成的汉语可能有10个也可能有8个， 输出汉语的字数并不固定。由于输入和输出的长度不固定， one-to-one模型就不太适合了。</p><p>对于时序数据， 更好的模型是many-to-one或者many-to-many模型， RNN就是这样的模型， 输入和输出都不需要固定， RNN很适合文本、语音等时序数据。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129152146509.png" alt="image-20231129152146509" style="zoom:67%;"></p><p>RNN跟人的阅读习惯很类似，人每次看一个词， 逐渐在大脑中积累信息。RNN每看一个词， 用状态向量h来积累阅读过的信息。我们把输入的一个词用word embedding变成一个向量x， 每次把一个词向量输入到RNN中， 然后RNN会更新状态h， 把新的内容更新到状态h中。h0包含了第一个词the的信息， h1包含了前两个词the cat的信息， 以此类推， 最后一个状态ht包含了整句话的信息。</p><p>可把ht看成是从输入的这句话抽取得到的特征向量， 更新状态h时需要用到参数矩阵A。注意整个RNN只有一个参数A， 不管这条链路有多长， 参数A只有一个， A随机初始化， 然后利用训练数据来学习A。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129152706261.png" alt="image-20231129152706261" style="zoom:67%;"></p><hr><h4 id="2、Simple-RNN"><a href="#2、Simple-RNN" class="headerlink" title="2、Simple RNN"></a>2、Simple RNN</h4><p>simple RNN的结构如下图所示。</p><p>首先来看一下Simple RNN怎么把输入的词向量x结合到状态h里面? 上一个状态记住的是h_t-1, 新输入的词向量为xt， 把这两个向量做concatination， 得到一个更高维的向量。矩阵A是RNN的模型参数， 这里计算矩阵A和向量的乘积。 矩阵和向量的乘积是个向量， 然后把激活函数用到该向量的每个元素上。 激活函数是双曲正切函数tanh， 输入是任意实数， 输出在-1到1之间。把激活函数的输出作为新的状态向量ht。由于使用了tanh激活函数， 所以向量ht的每个元素都在-1到+1之间。</p><p>RNN神经网络的结构图可以这样理解。新的状态ht是旧的状态ht-1和新的输入xt的函数， 神经网络的模型模型参数是矩阵A， 新的状态ht依赖于旧的状态向量ht-1, 向量xt以及矩阵A。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129153119779.png" alt="image-20231129153119779" style="zoom:67%;"></p><p>双曲正切函数的曲线图如下所示。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129154031226.png" alt="image-20231129154031226" style="zoom:67%;"></p><p>思考一下， 为什么需要双曲正切函数tanh， 能否将其去掉。 </p><p>假设输入的词向量x全部都是0（这里考虑极端情况）， 这等同于把输入的词向量x给去掉， 把矩阵A右边那一半也去掉， 这样第100个状态向量h100就等于矩阵A乘以h99， 一直等于矩阵A的100次方乘以h0。加入矩阵A最大的特征值略小于1， 比如最大的特征值等于0.9。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231130100739550.png" alt="image-20231130100739550" style="zoom:67%;"></p><p>那么会发生什么， 0.9的100次方非常接近0， 那么新的状态向量h100几乎也是全零的向量。</p><script type="math/tex; mode=display">Suppose X_0 = ... = x_{100} = 0</script><script type="math/tex; mode=display">h_{100} = Ah_{99} = A^2h_{98} = ... = A^{100}h_0</script><p>假如矩阵A最大的特征值略大于1， 同理矩阵A的100次方会超级大， 那么新的状态向量h100的每个元素也都非常巨大。假如循环的次数更多，状态向量h就会爆炸或消失。由此可见， 如果每个这个激活函数， 数值计算可能会出现问题， 要么计算结果全为0， 要么计算结果都超级大。</p><p>what will happen if λmax(A) = 0.9 or 1.2 ?</p><p>所以需要使用tanh激活函数更新h， 而且更新之后会做一个normalization， 让h恢复到-1和+1这个合适的区间里。</p><p>首先针对ht-1与xt拼接后的向量， 这个向量的维度是h的维度加上x的维度， 所以A必须有h的维度加上x的维度这么多列， A的行数等于向量ht的维度， 所以矩阵A的大小就是矩阵h的维度乘以（h+x的维度）, 这个乘积就是simple rnn的参数。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231130165042224.png" alt="image-20231130165042224" style="zoom:67%;"></p><h4 id="3、模型搭建"><a href="#3、模型搭建" class="headerlink" title="3、模型搭建"></a>3、模型搭建</h4><p>之前是通过logistics regression来判断电影评论是正面的还是负面的， 下面通过RNN来完成这个分类任务。</p><p>首先最底层是word embedding, 它可以把词映射到向量x， 词向量的维度可自由设置， 可以使用cross validation交叉验证来选择最优的维度， 这里设置x的维度是32，然后搭建simple rnn层， 输入的词向量x， 输出的状态h， h的维度也是也可自由设置， 应用用corss validation交叉验证来选择最优的维度，这里设置h的维度是32， 所以x和h的维度都是32， 但这里只是一个巧合而已， 通常h和x的维度不一样。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231130165417313.png" alt="image-20231130165417313" style="zoom:67%;"></p><p>之前介绍过， 状态向量h积累输入的信息， 比如h0包含第一个单词i的信息， h1包含前两个词的信息， 最后一个ht积累了整句话的信息。</p><p>可以让keras输出所有的状态向量， 也可以让keras只输出最后一个向量ht， ht积累了整句话的信息。所以使用ht这一个向量就够了。</p><p>只使用ht， 而把ht前面的状态h全都丢掉， ht相当于从文本中提取的特征向量，把ht输入到分类器， 分类器就会输出一个0~1之间的数值， 0代表负面评价， 1代表正面评价。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231130171244821.png" alt="image-20231130171244821" style="zoom:67%;"></p><p>然后设置这些超参数， 设置vocabulary是1万， 意思是词典中有1万个词汇， embedding_dim是32， 意思是词向量x的维度是32， word_num是500， 意思是每个电影评论有500个单词， 如果超过了500个， 超过部分就会被截掉。如果不到500个， 就用zero padding补成长度等于500。state_dim是32， 意思就是状态向量h的维度等于32。</p><p>下面开始使用keras搭建网络。 首先添加embedding 层， 把它映射成向量， 然后是simpleRnn层。 SimpleRNN层需要指定状态向量h的维度， 其中return_sequences=False， 意思是RNN只输出最后一个状态向量， 而把之前的状态向量全部扔掉。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> SimpleRNN, Embedding, Dense</span><br><span class="line"></span><br><span class="line"><span class="comment"># unique words in the dictionary</span></span><br><span class="line">vocabulary = <span class="number">10000</span></span><br><span class="line"><span class="comment"># shape(x) = 32</span></span><br><span class="line">embedding_dim = <span class="number">32</span></span><br><span class="line"><span class="comment"># sequence length 每条评论500个单词， 去长补短</span></span><br><span class="line">word_num = <span class="number">500</span></span><br><span class="line"><span class="comment"># shape(h) = 32</span></span><br><span class="line">state_dim = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(vocabulary, embedding_dim, input_length=word_num))</span><br><span class="line"><span class="comment">#  return_sequences=False表示只输出最后的ht</span></span><br><span class="line">model.add(SimpleRNN(units=state_dim, return_sequences=<span class="literal">False</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p>打印的模型概要如下：</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231130173214390.png" alt="image-20231130173214390" style="zoom:67%;"></p><p>SimpleRNN层的参数量计算公式=h*(h + x) = 32×(32 + 32) + 32 = 2080， 最后面加的32表示偏置。</p><p>搭建模型后开始编译模型， 然后用训练数据拟合模型， 编译模型时指定算法为RMSprop， 损失函数是crossentropy， 评价标准是acc， 然后用训练数据来拟合模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line"><span class="comment"># Early stoping alleviates overfitting</span></span><br><span class="line">epochs = <span class="number">3</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">              loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line">history = model.fit(x_train, y_train, epochs=epochs,</span><br><span class="line">                    batch_size=<span class="number">32</span>, validation_data=(x_valid, y_valid))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用数据评价模型的表现</span></span><br><span class="line">scores = model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure><p>上面搭建模型时只使用了最后一个状态ht， 把ht之前的状态丢掉了。</p><p>想要h0到ht所有的状态也可以， 如果让keras返回所有的状态， RNN的输出就是个矩阵。矩阵的每一行是一个状态向量。如果使用所有的状态， 需要加上一个flatten层， 把所有状态变成一个向量， 然后这些向量作为分类器的输入来判断电影是正面的还是负面的， 只需要把前面的网络结构稍作改动即可。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201094445824.png" alt="image-20231201094445824" style="zoom:67%;"></p><p>如果返回所有的状态， 对应的代码做如下的修改。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> SimpleRNN, Embedding, Dense, Flatten</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line"><span class="comment"># unique words in the dictionary</span></span><br><span class="line">vocabulary = <span class="number">10000</span></span><br><span class="line"><span class="comment"># shape(x) = 32</span></span><br><span class="line">embedding_dim = <span class="number">32</span></span><br><span class="line"><span class="comment"># sequence length 每条评论500个单词， 去长补短</span></span><br><span class="line">word_num = <span class="number">500</span></span><br><span class="line"><span class="comment"># shape(h) = 32</span></span><br><span class="line">state_dim = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面是return_sequences=True的情况</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(vocabulary, embedding_dim, input_length=word_num))</span><br><span class="line"><span class="comment">#  return_state=False表示只输出最后的ht</span></span><br><span class="line">model.add(SimpleRNN(units=state_dim, return_sequences=<span class="literal">True</span>))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p>打印的模型参数如下所示：</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201101052199.png" alt="image-20231201101052199" style="zoom:67%;"></p><p>上面是改动模型后打印的概要， 当return_sequence=False时只输出最后一个状态ht， 所以RNN层的输出是32维的向量。当return_sequences=True时， RNN输出所有的状态向量， 所以RNN的输出是500×32的矩阵。500的意思是每条电影评论中有500个单词， 所以一共有500个状态向量， 每个状态向量都是32维。</p><h4 id="4、RNN模型的缺陷"><a href="#4、RNN模型的缺陷" class="headerlink" title="4、RNN模型的缺陷"></a>4、RNN模型的缺陷</h4><p>举个例子， 现在有这样一个问题， 给定半句话要求预测下一个单词。比如clouds are in the _正确的输出是sky。现在如果在大量文本上训练RNN， 应该是有能力做出这种预测的。在这个例子中， RNN只需要看最近的几个单词。RNN并不需要更多的上下文， 并不需要看得更远， 这个例子对simple RNN有利， simple RNN很适合做这种short term dependence。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201102334418.png" alt="image-20231201102334418" style="zoom:67%;"></p><p>Simple RNN的缺点是不擅长long-term dependence。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201102839929.png" alt="image-20231201102839929" style="zoom:67%;"></p><p>RNN中的状态h跟之前的所有输入的x都有函数依赖关系。 理论上，  若改变输入的单词x1， 那么之后所有的状态h都是发生变化，但实际上simpleRnn并没有这种性质，所以很不合理。如果把第100个向量h100关于输入x1求导， 会发现导数几乎等于0。导数等于0说明改变输入x1， h100几乎不会发生任何变化。也就是说状态h100跟100步之前的x1几乎没有关系了， 说明状态h100将很多步之前的输入给忘记了， 这显然不合理。</p><p>Simple RNN的遗忘会造成一些问题。举个例子， 这是一段话， 开始是I grew up in China, when I was a child,…， 说了很多之后，来了一句, I speak fluent _。然后Simple RNN并不会做出Chinese这个正确的预测， 因为RNN已经把前文忘记了， simple RNN很擅长short term dependence。RNN看到最近的单词是speak fluent， 所以RNN知道下一个单词应该是某种语言， 预测输出可能是任意一种语言。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201104615371.png" alt="image-20231201104615371" style="zoom:67%;"></p><h4 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h4><p>RNN是一种神经网络， 但是它的结构不同于全连接网络和卷积网络， RNN适合文本、语音、时序序列等数据。RNN按照顺序读取每一个词向量， 并且在状态向量h中积累看过的信息。ht只积累了之前所有x的信息。有一种错误的看法是ht只包含了xt的信息，这是不对的。可以认为ht就是从整个输入序列中抽取的特征向量，所以只需要ht向量就可以判断电影评论是正面的还是负面的。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201111508008.png" alt="image-20231201111508008" style="zoom:67%;"></p><p>RNN也有一个缺点， RNN的记忆比较短， 它会遗忘很久之前的输入x， 如果这个时间序列很长， 比如好几十步， 最终的ht已经忘记了早先的输入。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201111840310.png" alt="image-20231201111840310" style="zoom:67%;"></p><p>simple RNN有一个参数矩阵A， 它还有可能有一个intercept向量， 这里忽略了这个参数b， 这个参数矩阵的维度是h的维度乘以h加上输入x的维度。</p><p>参数矩阵A一开始是随机初始化的， 然后从训练数据中学习这个参数矩阵。注意simple RNN只有这一个参数矩阵， 不管这个序列有多长， 参数矩阵只有一个， 所有模块里面的参数都是一样的。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231201112048270.png" alt="image-20231201112048270" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;RNN循环神经网络在nlp领域有</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://guudman.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>NlpFoundation</title>
    <link href="https://guudman.github.io/2023/11/29/NlpFoundation/"/>
    <id>https://guudman.github.io/2023/11/29/NlpFoundation/</id>
    <published>2023-11-29T10:03:47.000Z</published>
    <updated>2023-11-29T10:14:36.133Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、基础"><a href="#1、基础" class="headerlink" title="1、基础"></a>1、基础</h4><p>如何将计算机不认识的文本特征转化为数字特征</p><p>常见的特征分类有类别特征（categorical feature）和数值特征（numeric feature）。 类别特征一般是有限集合， 没有大小之分， 是并列权重。数值特征如：年龄， 工资等， 有数值大小之分。计算机只能处理数值型特征，因此需要将非数值特征转化为计算机能识别的数字， 如下表中的gender和nationality。gender为二元特征， nationality为多元的类别特征。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128093427069.png" alt="image-20231128093427069" style="zoom:67%;"></p><p>国籍表示成1-197之间的整数(全球大概有197个国家)， 但这些整数只是一个类别， 它们之间无法比较大小， 因为这些整数只是类别而已， 并不是真正的数值， 所以需要对国籍做one-hot Embedding。需要注意的是我们这里用数字表示的类别特征从1开始， 因为0要用来保留未知或缺失的国籍，数据库中经常会有缺失数据， 这些缺失的国籍就用0来表示。</p><p>对于性别， 用0表示女性， 1表示男性。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128095539327.png" alt="image-20231128095539327"></p><p>上面提到， 1-197只代表一个类别， 它们之间无大小之分， 因此需要对国籍进一步做one-hot Embedding。</p><h5 id="类别特征转化数字流程"><a href="#类别特征转化数字流程" class="headerlink" title="类别特征转化数字流程"></a>类别特征转化数字流程</h5><p>上面我们将每个国籍与一个数字进行了映射， 因此会得到这样的一个字典{‘US’-1, ‘China’:2, ‘India’:3, ‘German’: 4,…}， 针对197种国籍， 每个用数字代表的国籍又可以映射为197×1的向量， 使用上面的字典查找国籍的index（如index(China) = 2）, 则该197×1的向量的第二维为1， 其他为0。</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;US&quot;:1-&gt;[1, 0, 0, 0,...]</span><br><span class="line">&quot;China&quot;:2-&gt;[0, 1, 0, 0,...]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>nationality转成one-hot向量后得到的表格如下。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128104946062.png" alt="image-20231128104946062" style="zoom:67%;"></p><p>针对表格中人的三个特征（age, gender, nationality）转化为数字后， 可以得到1+1+197=199维的向量，向量中每个数值的含义如下图。</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(35, Male, Chine) = (35, 1, 0, 1, 0, ...)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128104608977.png" alt="image-20231128104608977" style="zoom:67%;"></p><p>为什么要使用one-hot向量来表示呢， </p><p>如果不使用one-hot向量而用1， 2， 3， …分别代表US, China, India…， 则在此列运算时，”US” + “China” = “India”， 这显然不合理， 而用one-hot向量表示时， “US” + “China”  = [1, 0, 0, …] + [0, 1, 0, …] = [1, 1, 0,…]。 向量[1, 1, 0, 0…]表示既包含US也包好China的类别信息， 可以更好的表示二者类别特征相加的意思，显然one-hot表示更为合理。</p><h5 id="文本特征处理"><a href="#文本特征处理" class="headerlink" title="文本特征处理"></a>文本特征处理</h5><p>在自然语言处理中， 数字都是文本， 文本可以分割成很多单词， 需要把单词表示成数值向量， 每个单词就是一个类别， 如果字典中有1万个单词， 那么就有1万个类别。很显然， 单词就是categorical features, 用categorical features的方法把单词变成数值向量。</p><p>文本处理的第一步就是把文本处理成单词， 一段话、一篇文章或一本书都可以表示成一个字符串， 可以把文本分成很多单词，这个操作叫作tokenization。</p><p>第一步：文本-&gt;单词： 分词。 tokenization就是把文本变成单词列表。</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">before:</span><br><span class="line">S = &quot;... to be or not to be...&quot;.</span><br><span class="line">after:</span><br><span class="line">L = [..., to, be, or, not, to, be, ...]</span><br></pre></td></tr></table></figure><p>第二步： 计算词频， 也就是每个单词出现的次数， 可以用一个hash表来计数， 开始时hash表是空的， 然后按照下面的方式更新hash表。</p><p>如果单词w不在hash表中，  这就说明到目前为止文本中只出现过一次单词w， 所以把w加入hash表中， 让它的词频等于1。假如单词w在hash表中， 说明之前文本中就已经出现过单词w了， 需要将它的词频加1。</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">总结如下：</span><br><span class="line">若词w不在hash表中，则add(w, 1)</span><br><span class="line">否则dict[w] = dict[w] + 1</span><br></pre></td></tr></table></figure><p>如下图所示</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128111523105.png" alt="image-20231128111523105" style="zoom:67%;"></p><p>第三步： 排序， 将字典中的词频从大到小排序</p><p>表中最前面的是词频最高的词， 表最后是词频最低的。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128112721467.png" alt="image-20231128112721467" style="zoom:67%;"></p><p>然后把词频换成index， 转化完成的hash表称为词汇表vocabulary。词频最高的index是1。</p><p>统计词频的目的是保留常用词， 去掉词频词。比如你可以设置保存最常用的前10k个单词。为什么要去掉地频词呢， 有如下三点原因。</p><p>1、很多地频词是name entities, 在大多数的应用中， name entities没有任何意义。</p><p>2、拼写过程难免出现错误， 低频词还有可能是拼写错误造成的。</p><p>3、不希望词汇表vocabulary太大， vocabulary越大， one-hot向量维度就越高。</p><p>第一步的tokenization把文本分割成单词的列表， 第二步建立了一个字典（也可为hash表）， 将每个单词映射到一个正整数。下面开始第三步， 对单词做one-hot encoding。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128115000838.png" alt="image-20231128115000838" style="zoom:67%;"></p><p>如果有需要的话， 可以将每个index转换成一个one-hot向量。one-hot向量的维度就是字典vocabulary的维度。</p><p>上面提到， 字典中的低频词会被删除， 所以有些词在字典中找不到。假如有个错误拼写的单词bi， 这个单词在字典中找不到， 做one-hot encoding时可以忽略这些词， 也可以将其编码为0.</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128115214398.png" alt="image-20231128115214398" style="zoom:67%;"></p><h4 id="2、文本处理与词嵌入"><a href="#2、文本处理与词嵌入" class="headerlink" title="2、文本处理与词嵌入"></a>2、文本处理与词嵌入</h4><p>应用的数据集是IMDB<a href="[Sentiment Analysis (stanford.edu">(数据集下载地址)</a>](<a href="http://ai.stanford.edu/~amaas/data/sentiment/))电影评论数据集，">http://ai.stanford.edu/~amaas/data/sentiment/))电影评论数据集，</a> 该数据集包含了5万条偏向明显的评论， 其中2.5万条作为训练集， 2.5万条作为测试机。label为pos和neg，是一个二分类问题。</p><p>1、文本变成序列， text to sequence， 也就是tokenization， 把文本分割成单词， 一个token就是一个单词， 有些应用中， token可以是一个符号。</p><p>会有很多考虑， 1、是否将大写变成小写， 正常情况下， 大小写意义相同， 但有些特例，比如Apple表示苹果公司， 而apple表示水果。2、有些应用会去掉停用词stop word。 3、会存在拼写错误的情况， 因此需要typo correction文本纠错。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128175224685.png" alt="image-20231128175224685" style="zoom:67%;"></p><p>2、建立字典。首先统计词频， 去掉地频词， 然后每个单词对应一个正整数。 有了字典， 可以将每个单词映射成一个整数， 这样一句话可以用正整数的列表表示， 这个列表就是sequences。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231128175445939.png" alt="image-20231128175445939" style="zoom:67%;"></p><p>3、Encoding编码</p><p>将字典中的单词映射到索引，索引列表就是一个sequence。如果有必要的话， 还可以进一步做one-hot encoding, 将单词表示成one-hot向量。在IMDB电影评论的例子中， 数据是50k条， 每条电影评论可是表示成一个字符串， tokenization（句子切分成单词列表）之后， 每条电影评论都被转化成一个sequence。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129092836076.png" alt="image-20231129092836076" style="zoom:67%;"></p><p>这样的话每一条评论都被转化成一个sequence序列。 注意每个单词的索引是在25k条评论tokenization之后放在一个列表中， 经过排序， 截断之后的索引。排名越靠前， 该单词被使用的频率越高。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129093349986.png" alt="image-20231129093349986" style="zoom:67%;"></p><p>但是电影评论有长有短</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129095156849.png" alt="image-20231129095156849" style="zoom:67%;"></p><p>这样就造成一个问题， 训练数据没有对齐，每条sequence都有不同的长度， 做机器学习训练时需要把数据存储在张量中， 这就要求吧序列对齐， 让每条序列都有相同的长度。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129095701552.png" alt="image-20231129095701552" style="zoom:67%;"></p><p>解决方法就是可以固定长度w， 假如这个序列太长， 可切掉前面的单词，当然也可以保留前面的单词。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129095832639.png" alt="image-20231129095832639" style="zoom: 67%;"></p><p>如果序列太短的话， 就用padding把长度增加到w。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129095939431.png" alt="image-20231129095939431" style="zoom:67%;"></p><p>这样处理之后， 所以序列的长度都是w。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129100018776.png" alt="image-20231129100018776" style="zoom:67%;"></p><p>对齐之后的sequence就可以存储到一个矩阵中， 文本处理的过程就是每个词用一个正整数来表示， 下一步就是word embedding。</p><p>keras中文本处理流程实例。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129102921411.png" alt="image-20231129102921411"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : func.py</span></span><br><span class="line"><span class="string"># @Time       ：2023/11/29 9:54</span></span><br><span class="line"><span class="string"># @version    ：python 3.9</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 剔除html标签</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rm_tags</span>(<span class="params">text</span>):</span><br><span class="line">    re_tag = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;[^&gt;]+&gt;&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> re_tag.sub(<span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_file</span>(<span class="params">filetype</span>):  <span class="comment"># 读取文件</span></span><br><span class="line">    path = <span class="string">r&quot;./aclImdb&quot;</span></span><br><span class="line">    file_list = []</span><br><span class="line"></span><br><span class="line">    positive_path = os.path.join(path, filetype, <span class="string">&#x27;pos&#x27;</span>)  <span class="comment"># 正面评价的文件路径</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(positive_path):</span><br><span class="line">        file_list += [os.path.join(positive_path, f)]  <span class="comment"># 存储到文件列表中</span></span><br><span class="line"></span><br><span class="line">    negative_path = os.path.join(path, filetype, <span class="string">&#x27;neg&#x27;</span>)  <span class="comment"># 负面评价的文件路径</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(negative_path):</span><br><span class="line">        file_list += [os.path.join(negative_path, f)]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;read&#x27;</span>, filetype, <span class="string">&#x27;files&#x27;</span>, <span class="built_in">len</span>(file_list))  <span class="comment"># 打印文件个数</span></span><br><span class="line"></span><br><span class="line">    all_labels = ([<span class="number">1</span>] * <span class="number">12500</span> + [<span class="number">0</span>] * <span class="number">12500</span>)  <span class="comment"># 前12500值正面都为1， 后12500是负面都为0</span></span><br><span class="line">    all_texts = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(file_list)</span></span><br><span class="line">    <span class="comment"># 读取所有文件</span></span><br><span class="line">    <span class="keyword">for</span> fi <span class="keyword">in</span> file_list:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fi, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file_input:</span><br><span class="line">            <span class="comment"># 先读取文件， 使用join连接所有字符串， 然后使用rm_tags删除tag最后存入列表all_texts</span></span><br><span class="line">            all_texts += [rm_tags(<span class="string">&quot; &quot;</span>.join(file_input.readlines()))]</span><br><span class="line">    <span class="keyword">return</span> all_labels, all_texts</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_text:  list[string], 一条一条的评论放在一个列表中</span></span><br><span class="line">y_train, train_text = read_file(<span class="string">&quot;train&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;train_text: \n&quot;</span>, train_text[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了方便打印， 这里只查看前2条评论的转化情况</span></span><br><span class="line">train_text = train_text[:<span class="number">2</span>]</span><br><span class="line">vocabulary = <span class="number">1000</span></span><br><span class="line">tokenizer = Tokenizer(num_words=vocabulary)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit_on_texts的过程就是将文本切分成一个一个单词， 建立单词与index之间的索引</span></span><br><span class="line">tokenizer.fit_on_texts(train_text)</span><br><span class="line"><span class="comment"># word_index： 单词与索引的映射字典。dict[(string, int)]</span></span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;word_index: \n&quot;</span>, word_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sequence_train: 将所有的评论中的每个单词映射成list列表， 然后将所有的列表放在一个大列表中。list[list[int]]</span></span><br><span class="line">sequences_train = tokenizer.texts_to_sequences(train_text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;sequence_train: \n&quot;</span>, sequences_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(sequences_train[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(sequences_train[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本对齐</span></span><br><span class="line">word_num = <span class="number">200</span></span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(sequences_train, maxlen=word_num)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(x_train[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(x_train[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">train_text: </span></span><br><span class="line"><span class="string"> Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as &quot;Teachers&quot;. My 35 years in the teaching profession lead me to believe that Bromwell High&#x27;s satire is much closer to reality than is &quot;Teachers&quot;. The scramble to survive financially, the insightful students who can see right through their pathetic teachers&#x27; pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I&#x27;m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn&#x27;t!</span></span><br><span class="line"><span class="string">word_index: </span></span><br><span class="line"><span class="string"> &#123;&#x27;the&#x27;: 1, &#x27;to&#x27;: 2, &#x27;a&#x27;: 3, &#x27;on&#x27;: 4, &#x27;of&#x27;: 5, &#x27;or&#x27;: 6, &#x27;as&#x27;: 7, &#x27;who&#x27;: 8, &#x27;is&#x27;: 9, &#x27;it&#x27;: 10, &#x27;that&#x27;: 11, &#x27;what&#x27;: 12, &#x27;bolt&#x27;: 13, &#x27;i&#x27;: 14, &#x27;and&#x27;: 15, &#x27;for&#x27;: 16, &#x27;like&#x27;: 17, &#x27;he&#x27;: 18, &#x27;where&#x27;: 19, &#x27;bromwell&#x27;: 20, &#x27;high&#x27;: 21, &#x27;in&#x27;: 22, &#x27;once&#x27;: 23, &#x27;if&#x27;: 24, &#x27;be&#x27;: 25, &#x27;streets&#x27;: 26, &#x27;you&#x27;: 27, &quot;it&#x27;s&quot;: 28, &#x27;with&#x27;: 29, &#x27;his&#x27;: 30, &#x27;other&#x27;: 31, &#x27;school&#x27;: 32, &#x27;such&#x27;: 33, &#x27;teachers&#x27;: 34, &#x27;can&#x27;: 35, &#x27;see&#x27;: 36, &#x27;their&#x27;: 37, &#x27;when&#x27;: 38, &#x27;everything&#x27;: 39, &#x27;homeless&#x27;: 40, &#x27;bet&#x27;: 41, &#x27;rich&#x27;: 42, &#x27;do&#x27;: 43, &#x27;comedy&#x27;: 44, &#x27;at&#x27;: 45, &#x27;about&#x27;: 46, &#x27;life&#x27;: 47, &#x27;my&#x27;: 48, &#x27;years&#x27;: 49, &#x27;me&#x27;: 50, &#x27;students&#x27;: 51, &#x27;all&#x27;: 52, &#x27;student&#x27;: 53, &#x27;one&#x27;: 54, &#x27;think&#x27;: 55, &quot;isn&#x27;t&quot;: 56, &#x27;has&#x27;: 57, &#x27;an&#x27;: 58, &#x27;but&#x27;: 59, &#x27;help&#x27;: 60, &#x27;street&#x27;: 61, &#x27;were&#x27;: 62, &#x27;did&#x27;: 63, &#x27;from&#x27;: 64, &#x27;work&#x27;: 65, &#x27;matter&#x27;: 66, &#x27;people&#x27;: 67, &#x27;while&#x27;: 68, &#x27;worrying&#x27;: 69, &#x27;next&#x27;: 70, &#x27;given&#x27;: 71, &#x27;live&#x27;: 72, &#x27;without&#x27;: 73, &#x27;luxuries&#x27;: 74, &#x27;home&#x27;: 75, &#x27;mel&#x27;: 76, &#x27;making&#x27;: 77, &quot;he&#x27;s&quot;: 78, &#x27;by&#x27;: 79, &#x27;molly&#x27;: 80, &#x27;before&#x27;: 81, &#x27;losing&#x27;: 82, &#x27;her&#x27;: 83, &#x27;used&#x27;: 84, &#x27;being&#x27;: 85, &#x27;they&#x27;: 86, &#x27;money&#x27;: 87, &#x27;maybe&#x27;: 88, &#x27;cartoon&#x27;: 89, &#x27;ran&#x27;: 90, &#x27;same&#x27;: 91, &#x27;time&#x27;: 92, &#x27;some&#x27;: 93, &#x27;programs&#x27;: 94, &#x27;35&#x27;: 95, &#x27;teaching&#x27;: 96, &#x27;profession&#x27;: 97, &#x27;lead&#x27;: 98, &#x27;believe&#x27;: 99, &quot;high&#x27;s&quot;: 100, &#x27;satire&#x27;: 101, &#x27;much&#x27;: 102, &#x27;closer&#x27;: 103, &#x27;reality&#x27;: 104, &#x27;than&#x27;: 105, &#x27;scramble&#x27;: 106, &#x27;survive&#x27;: 107, &#x27;financially&#x27;: 108, &#x27;insightful&#x27;: 109, &#x27;right&#x27;: 110, &#x27;through&#x27;: 111, &#x27;pathetic&#x27;: 112, &quot;teachers&#x27;&quot;: 113, &#x27;pomp&#x27;: 114, &#x27;pettiness&#x27;: 115, &#x27;whole&#x27;: 116, &#x27;situation&#x27;: 117, &#x27;remind&#x27;: 118, &#x27;schools&#x27;: 119, &#x27;knew&#x27;: 120, &#x27;saw&#x27;: 121, &#x27;episode&#x27;: 122, &#x27;which&#x27;: 123, &#x27;repeatedly&#x27;: 124, &#x27;tried&#x27;: 125, &#x27;burn&#x27;: 126, &#x27;down&#x27;: 127, &#x27;immediately&#x27;: 128, &#x27;recalled&#x27;: 129, &#x27;classic&#x27;: 130, &#x27;line&#x27;: 131, &#x27;inspector&#x27;: 132, &quot;i&#x27;m&quot;: 133, &#x27;here&#x27;: 134, &#x27;sack&#x27;: 135, &#x27;your&#x27;: 136, &#x27;welcome&#x27;: 137, &#x27;expect&#x27;: 138, &#x27;many&#x27;: 139, &#x27;adults&#x27;: 140, &#x27;age&#x27;: 141, &#x27;far&#x27;: 142, &#x27;fetched&#x27;: 143, &#x27;pity&#x27;: 144, &#x27;homelessness&#x27;: 145, &#x27;houselessness&#x27;: 146, &#x27;george&#x27;: 147, &#x27;carlin&#x27;: 148, &#x27;stated&#x27;: 149, &#x27;been&#x27;: 150, &#x27;issue&#x27;: 151, &#x27;never&#x27;: 152, &#x27;plan&#x27;: 153, &#x27;those&#x27;: 154, &#x27;considered&#x27;: 155, &#x27;human&#x27;: 156, &#x27;going&#x27;: 157, &#x27;vote&#x27;: 158, &#x27;most&#x27;: 159, &#x27;just&#x27;: 160, &#x27;lost&#x27;: 161, &#x27;cause&#x27;: 162, &#x27;things&#x27;: 163, &#x27;racism&#x27;: 164, &#x27;war&#x27;: 165, &#x27;iraq&#x27;: 166, &#x27;pressuring&#x27;: 167, &#x27;kids&#x27;: 168, &#x27;succeed&#x27;: 169, &#x27;technology&#x27;: 170, &#x27;elections&#x27;: 171, &#x27;inflation&#x27;: 172, &quot;they&#x27;ll&quot;: 173, &#x27;end&#x27;: 174, &#x27;up&#x27;: 175, &#x27;month&#x27;: 176, &#x27;had&#x27;: 177, &#x27;entertainment&#x27;: 178, &#x27;sets&#x27;: 179, &#x27;bathroom&#x27;: 180, &#x27;pictures&#x27;: 181, &#x27;wall&#x27;: 182, &#x27;computer&#x27;: 183, &#x27;treasure&#x27;: 184, &#x27;goddard&#x27;: 185, &quot;bolt&#x27;s&quot;: 186, &#x27;lesson&#x27;: 187, &#x27;brooks&#x27;: 188, &#x27;directs&#x27;: 189, &#x27;stars&#x27;: 190, &#x27;plays&#x27;: 191, &#x27;man&#x27;: 192, &#x27;world&#x27;: 193, &#x27;until&#x27;: 194, &#x27;deciding&#x27;: 195, &#x27;make&#x27;: 196, &#x27;sissy&#x27;: 197, &#x27;rival&#x27;: 198, &#x27;jeffery&#x27;: 199, &#x27;tambor&#x27;: 200, &#x27;thirty&#x27;: 201, &#x27;days&#x27;: 202, &#x27;succeeds&#x27;: 203, &#x27;wants&#x27;: 204, &#x27;future&#x27;: 205, &#x27;project&#x27;: 206, &#x27;more&#x27;: 207, &#x27;buildings&#x27;: 208, &quot;bet&#x27;s&quot;: 209, &#x27;thrown&#x27;: 210, &#x27;bracelet&#x27;: 211, &#x27;leg&#x27;: 212, &#x27;monitor&#x27;: 213, &#x27;every&#x27;: 214, &#x27;move&#x27;: 215, &quot;can&#x27;t&quot;: 216, &#x27;step&#x27;: 217, &#x27;off&#x27;: 218, &#x27;sidewalk&#x27;: 219, &#x27;nickname&#x27;: 220, &#x27;pepto&#x27;: 221, &#x27;vagrant&#x27;: 222, &#x27;after&#x27;: 223, &#x27;written&#x27;: 224, &#x27;forehead&#x27;: 225, &#x27;meets&#x27;: 226, &#x27;characters&#x27;: 227, &#x27;including&#x27;: 228, &#x27;woman&#x27;: 229, &#x27;name&#x27;: 230, &#x27;lesley&#x27;: 231, &#x27;ann&#x27;: 232, &#x27;warren&#x27;: 233, &#x27;ex&#x27;: 234, &#x27;dancer&#x27;: 235, &#x27;got&#x27;: 236, &#x27;divorce&#x27;: 237, &#x27;pals&#x27;: 238, &#x27;sailor&#x27;: 239, &#x27;howard&#x27;: 240, &#x27;morris&#x27;: 241, &#x27;fumes&#x27;: 242, &#x27;teddy&#x27;: 243, &#x27;wilson&#x27;: 244, &#x27;are&#x27;: 245, &#x27;already&#x27;: 246, &quot;they&#x27;re&quot;: 247, &#x27;survivors&#x27;: 248, &#x27;not&#x27;: 249, &#x27;reaching&#x27;: 250, &#x27;mutual&#x27;: 251, &#x27;agreements&#x27;: 252, &#x27;fight&#x27;: 253, &#x27;flight&#x27;: 254, &#x27;kill&#x27;: 255, &#x27;killed&#x27;: 256, &#x27;love&#x27;: 257, &#x27;connection&#x27;: 258, &#x27;between&#x27;: 259, &quot;wasn&#x27;t&quot;: 260, &#x27;necessary&#x27;: 261, &#x27;plot&#x27;: 262, &#x27;found&#x27;: 263, &#x27;stinks&#x27;: 264, &quot;brooks&#x27;&quot;: 265, &#x27;observant&#x27;: 266, &#x27;films&#x27;: 267, &#x27;prior&#x27;: 268, &#x27;shows&#x27;: 269, &#x27;tender&#x27;: 270, &#x27;side&#x27;: 271, &#x27;compared&#x27;: 272, &#x27;slapstick&#x27;: 273, &#x27;blazing&#x27;: 274, &#x27;saddles&#x27;: 275, &#x27;young&#x27;: 276, &#x27;frankenstein&#x27;: 277, &#x27;spaceballs&#x27;: 278, &#x27;show&#x27;: 279, &#x27;having&#x27;: 280, &#x27;something&#x27;: 281, &#x27;valuable&#x27;: 282, &#x27;day&#x27;: 283, &#x27;hand&#x27;: 284, &#x27;stupid&#x27;: 285, &quot;don&#x27;t&quot;: 286, &#x27;know&#x27;: 287, &#x27;should&#x27;: 288, &#x27;give&#x27;: 289, &#x27;instead&#x27;: 290, &#x27;using&#x27;: 291, &#x27;monopoly&#x27;: 292, &#x27;this&#x27;: 293, &#x27;film&#x27;: 294, &#x27;will&#x27;: 295, &#x27;inspire&#x27;: 296, &#x27;others&#x27;: 297&#125;</span></span><br><span class="line"><span class="string">sequence_train: </span></span><br><span class="line"><span class="string"> [[20, 21, 9, 3, 89, 44, 10, 90, 45, 1, 91, 92, 7, 93, 31, 94, 46, 32, 47, 33, 7, 34, 48, 95, 49, 22, 1, 96, 97, 98, 50, 2, 99, 11, 20, 100, 101, 9, 102, 103, 2, 104, 105, 9, 34, 1, 106, 2, 107, 108, 1, 109, 51, 8, 35, 36, 110, 111, 37, 112, 113, 114, 1, 115, 5, 1, 116, 117, 52, 118, 50, 5, 1, 119, 14, 120, 15, 37, 51, 38, 14, 121, 1, 122, 22, 123, 3, 53, 124, 125, 2, 126, 127, 1, 32, 14, 128, 129, 45, 21, 3, 130, 131, 132, 133, 134, 2, 135, 54, 5, 136, 34, 53, 137, 2, 20, 21, 14, 138, 11, 139, 140, 5, 48, 141, 55, 11, 20, 21, 9, 142, 143, 12, 3, 144, 11, 10, 56], [145, 6, 146, 7, 147, 148, 149, 57, 150, 58, 151, 16, 49, 59, 152, 3, 153, 2, 60, 154, 4, 1, 61, 11, 62, 23, 155, 156, 8, 63, 39, 64, 157, 2, 32, 65, 6, 158, 16, 1, 66, 159, 67, 55, 5, 1, 40, 7, 160, 3, 161, 162, 68, 69, 46, 163, 33, 7, 164, 1, 165, 4, 166, 167, 168, 2, 169, 170, 1, 171, 172, 6, 69, 24, 173, 25, 70, 2, 174, 175, 4, 1, 26, 59, 12, 24, 27, 62, 71, 3, 41, 2, 72, 4, 1, 26, 16, 3, 176, 73, 1, 74, 27, 23, 177, 64, 3, 75, 1, 178, 179, 3, 180, 181, 4, 1, 182, 3, 183, 15, 39, 27, 23, 184, 2, 36, 12, 28, 17, 2, 25, 40, 11, 9, 185, 186, 187, 76, 188, 8, 189, 8, 190, 7, 13, 191, 3, 42, 192, 8, 57, 39, 22, 1, 193, 194, 195, 2, 196, 3, 41, 29, 3, 197, 198, 199, 200, 2, 36, 24, 18, 35, 72, 22, 1, 26, 16, 201, 202, 73, 1, 74, 24, 13, 203, 18, 35, 43, 12, 18, 204, 29, 3, 205, 206, 5, 77, 207, 208, 1, 209, 4, 19, 13, 9, 210, 4, 1, 61, 29, 3, 211, 4, 30, 212, 2, 213, 30, 214, 215, 19, 18, 216, 217, 218, 1, 219, 78, 71, 1, 220, 221, 79, 3, 222, 223, 28, 224, 4, 30, 225, 19, 13, 226, 31, 227, 228, 3, 229, 79, 1, 230, 5, 80, 231, 232, 233, 58, 234, 235, 8, 236, 237, 81, 82, 83, 75, 15, 83, 238, 239, 240, 241, 15, 242, 243, 244, 8, 245, 246, 84, 2, 1, 26, 247, 248, 13, 56, 78, 249, 84, 2, 250, 251, 252, 17, 18, 23, 63, 38, 85, 42, 19, 28, 253, 6, 254, 255, 6, 25, 256, 68, 1, 257, 258, 259, 80, 15, 13, 260, 261, 2, 262, 14, 263, 47, 264, 2, 25, 54, 5, 76, 265, 266, 267, 19, 268, 2, 85, 3, 44, 10, 269, 3, 270, 271, 272, 2, 30, 273, 65, 33, 7, 274, 275, 276, 277, 6, 278, 16, 1, 66, 2, 279, 12, 28, 17, 280, 281, 282, 81, 82, 10, 1, 70, 283, 6, 4, 1, 31, 284, 77, 3, 285, 41, 17, 52, 42, 67, 43, 38, 86, 286, 287, 12, 2, 43, 29, 37, 87, 88, 86, 288, 289, 10, 2, 1, 40, 290, 5, 291, 10, 17, 292, 87, 6, 88, 293, 294, 295, 296, 27, 2, 60, 297]]</span></span><br><span class="line"><span class="string"># 对齐前</span></span><br><span class="line"><span class="string">138</span></span><br><span class="line"><span class="string">425</span></span><br><span class="line"><span class="string"># 对齐后</span></span><br><span class="line"><span class="string">200</span></span><br><span class="line"><span class="string">200</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Process finished with exit code 0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>以上的过程总结如下：以一条评论为例。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129110647827.png" alt="image-20231129110647827"></p><p>对于测试数据集同样完成tokenization-&gt;encoding-&gt;alignment这三步。但是需要注意的是， 训练集中的字典与测试集中的字典必须相同， 否则可能会出现同样的单词在测试集中的索引与训练集中的所有不同。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129111035359.png" alt="image-20231129111035359" style="zoom:67%;"></p><h4 id="3、Word-Embedding词嵌入-word-to-vector"><a href="#3、Word-Embedding词嵌入-word-to-vector" class="headerlink" title="3、Word Embedding词嵌入  word to vector"></a>3、Word Embedding词嵌入  word to vector</h4><p>将单词进一步表示成向量， 之前每个单词都用数字表示， 该如何把这些特征表示成向量呢。显然可以用如下图所示的one-hot编码。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129111713626.png" alt="image-20231129111713626" style="zoom:67%;"></p><p>但是如果字典中有1万个单词， 那么这个one-hot维度就太大了， 因此需要做word embedding， 将这些高维向量映射成低维向量。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129112205707.png" alt="image-20231129112205707" style="zoom:67%;"></p><p>上图中<strong>P</strong>是参数矩阵， 它的参数可以在训练过程中从训练数据中学习。<strong>ei</strong>是字典中第i个单词的one-hot向量。</p><p>矩阵<strong>P</strong>的大小是d×v， d是词向量的维度， 由用户自主定义。矩阵乘法的结果是<strong>Xi</strong>， <strong>Xi</strong>就是词向量， 维度是d。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129112644337.png" alt="image-20231129112644337" style="zoom:67%;"></p><p>训练好的词向量展示在坐标中， 发现相似的词在坐标中的距离比较近。</p><p>keras中提供的Embedding层， 用户需要指定vocabulary的大小和词向量的维度以及每个sequence的长度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Flatten, Dense, Embedding</span><br><span class="line"></span><br><span class="line">embedding_dim = <span class="number">8</span></span><br><span class="line">vocabulary = <span class="number">100000</span></span><br><span class="line">word_num = <span class="number">20</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Embedding(vocabulary, embedding_dim, input_length=word_num))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129114342009.png" alt="image-20231129114342009" style="zoom: 67%;"></p><p>上图中， 处理数据时， 词汇表的大小是10k，以及每个电影评论中保留最后20个词（不足就补齐）， 设置词向量的维度等于8。</p><p>Embedding层的输出是20x8的句子， 也就是word_num × Embedding_dim</p><p>embedding层的参数量等于80k， 80k是这样计算的，Embedding层中有一个参数矩阵p， 矩阵的行数等于vocabulary， 所以矩阵有10k行， 矩阵的列数d是词向量的维度embedding_dim设置为8， 所以矩阵的大小是10k×8=80k， 所以Embedding层一共有80k个参数。</p><p>上面已经完成了文本处理和word embed， 每条电影评论保留最后的20个单词， 每个单词用一个8维的词向量表示。现在用logistics regression来做二分类。</p><p>Logistic regression for binary classification， 判断电影评论是正面的还是负面的， 用这几行就可实现一个分类器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Flatten, Dense, Embedding</span><br><span class="line"></span><br><span class="line">embedding_dim = <span class="number">8</span></span><br><span class="line">model = Sequential()  <span class="comment"># 就把网络层按照顺序搭建起来</span></span><br><span class="line"><span class="comment"># 添加Embedding层， Embedding层的输出是20x8的句子， 每条电影评论中有20个单词， 每个单词用8维的向量表示</span></span><br><span class="line">model.add(Embedding(vocabulary, embedding_dim, input_length=word_num))</span><br><span class="line"><span class="comment"># 展平操作</span></span><br><span class="line">model.add(Flatten())</span><br><span class="line"><span class="comment"># 全连接层， 输出1维， 用sigmoid激活函数， 这一层的输出是介于0-1之间的浮点数。0代表负面评价， 1代表正面评价</span></span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"><span class="comment"># summary函数打印模型的概要</span></span><br><span class="line">model.summary()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129142507422.png" alt="image-20231129142507422" style="zoom:67%;"></p><p>下面是编译模型， 然后训练损失函数来拟合模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line">epochs = <span class="number">50</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=optimizer.RMSprop(lr=<span class="number">0.0001</span>), loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line">history = model.fit(x_train, y_train, epochs=epochs, </span><br><span class="line">                   batch_size=<span class="number">32</span>, validation_data=(x_valid, y_valid))</span><br></pre></td></tr></table></figure><p>epochs的意思是把2万条数据扫一遍叫作igepochs， 50个epochs的意思是把训练数据全部扫50遍。</p><p>训练得到的准确率和损失曲线如下。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129150432649.png" alt="image-20231129150432649" style="zoom:67%;"></p><p>上面的过程总结如下：</p><p>针对每条电影评论， 首先进行tokenization， 将电影评论分割成一个一个的单词， 然后把每个单词编码成一个一个数字， 这样一条电影评论就可以用正整数的序列来表示了， 这个正整数的序列叫作sequence， sequence就是神经网络中Embedding的输入。</p><p>由于电影评论长短不一， 得到的sequences长短也不一样， 无法存储在一个矩阵中， 解决方法就是alignment对齐， 假设长度大于20个， 就只保留最后20个， 假设长度小于20， 就用0补齐，将长度补到20个， 这样每个sequence都是20个单词长度， 后面就是word embedding.</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129144713366.png" alt="image-20231129144713366" style="zoom:67%;"></p><p>把长度为20的sequences输入embedding层， embedding层把每个单词映射到8维的词向量， 所以每个长度为20的句子用一个20x8的矩阵来表示</p><p>然后用flatten将20x8的矩阵展平， 变成160维的向量， 最后用logistics regression分类器做分类。</p><p>embedding层有一个参数矩阵， 大小是1万×8， 1万就是词典里面的单词个数， 8是词向量的维度， 每个单词被映射为8维的词向量， 这个logistics regression分类器有161个参数， 输入是160维的向量， 所以分类器有160维的参数向量， 分类器还有一个bias偏置量， 所以一共有161个参数。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231129144737270.png" alt="image-20231129144737270" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、基础&quot;&gt;&lt;a href=&quot;#1、基础&quot; class=&quot;headerlink&quot; title=&quot;1、基础&quot;&gt;&lt;/a&gt;1、基础&lt;/h4&gt;&lt;p&gt;如何将计算机不认识的文本特征转化</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://guudman.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>RepVGG</title>
    <link href="https://guudman.github.io/2023/11/27/RepVGG/"/>
    <id>https://guudman.github.io/2023/11/27/RepVGG/</id>
    <published>2023-11-27T10:55:52.000Z</published>
    <updated>2023-11-29T10:05:56.537Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>VGG网络是2014年牛津大学提出的， 在2014到2016年， VGG网络可以说是当时很火并广泛应用的backbone， 后面由于新网络的提出， 精度上VGG比不上ResNet， 速度和参数数量VGG比不过MobileNet等轻量级网络， 慢慢的VGG开始淡出人们的视线， 当VGG已经被大家遗忘时， 2021年清华，旷视等机构共同基础了RepVGG网络。</p><p>论文中， 作者提到了structural re-parameterization technique方法，即结构重参数化。实际上就是在训练时， 使用一个类似ResNet-style的多分支模型，而推理时转化成VGG-style的单路模型。 如下图， B表示RepVGG训练时采用的网络结构， 而在推理时采用图（C)的网络结构。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231124163718801.png" alt="image-20231124163718801"></p><hr><h4 id="RepVGG-Block详解"><a href="#RepVGG-Block详解" class="headerlink" title="RepVGG Block详解"></a>RepVGG Block详解</h4><p>其实关于RepVGG模型就是在不断堆叠Rep VGG Block。下面介绍一下RepVGG Blocks中的结构， 如下图针对训练时采用的RepVGG Block结构。其中（a)是进行下采样stride=2时使用的RepVGG Block结构，图(b)是正常的(stride=1) RepVGG Block结构。通过图b可以发现， 训练时的RepVGG Block并行了三个分支， 一个卷积核大小为3x3的主分支， 一个卷积核大小为1x1的shortcu分支以及一个只连接了BN的shortcut分支。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/RepVGG.jpg" alt="RepVGG"></p><p>为什么训练时采用多分支结构， 之前的Inception系列， ResNet以及DenseNet等模型， 可以发现这些模型都并行了多个分支， 根据现有的经验来看， 并行多个分支能够增加模型的表征能力。在论文中作者也简单做了消融实验， 在使用单路结构时，ACC大概为72.39, 在加上Identity branch以及1x1 branch后acc达到了75.14。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231124165331760.png" alt="image-20231124165331760"></p><p>为什么推理时将多分支模型转换成单路模型， 论文中剃刀， 单路模型更快， 更省内存。</p><ul><li>更快， 主要考虑到模型在推理时硬件的并行程度以及MAC(memory access cost)， 对于多分支模型， 硬件需要分别计算每个分支的结果， 有的分支计算的快， 有的分支计算的慢， 而计算快的分支计算完后只能等其他分支计算完成后才能做进一步融合，这样会导致硬件算力不能充分利用， 或者说并行度不高。而且每个分支都需要方位一次内存， 计算完后还需要将计算结果存入内存（不断地访问和写入内存会在IO上浪费很多时间）</li><li>更省内存， 论文的图3中， 作者举了个例子， 如图A所以得Residual模块， 假设卷积层不改变channel的数量， 那么在主分支和shortcut分支上都要保存各自的特征图或者称Activation, 那么在add操作前占用的内存大概是输入activation的两倍，而图B的Plain结构占用内存始终不变。</li></ul><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231124170330194.png" alt="image-20231124170330194"></p><ul><li>更加灵活， 作者再论文中提到了模型优化的剪枝问题， 对于多分支的模型， 结构限制较多剪枝较麻烦， 而plain结构的模型就相对灵活很多， 剪枝也更方便。</li></ul><p>其实除此之外， 在多分支转化成单路模型后很多算子进行了融合（比如conv2d和BN融合）， 使得计算量变小了， 而且算子减少后启动kernel的次数也减少了（比如在GPU中， 每一次执行一个算子就要启动一次kernel， 启动kernel也需要消耗时间）。而且现在的硬件一般对3x3的卷积核做了大量的优化， 转成单路模型后采用的都是3x3卷积， 这样也能进一步加速推理。下图多分支模型B转换成单路模型C。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231124171152465.png" alt="image-20231124171152465"></p><hr><h4 id="结构重参数化"><a href="#结构重参数化" class="headerlink" title="结构重参数化"></a>结构重参数化</h4><p>在简单了解RepVGG Block的训练结构后， 下面看看RepVGG Block转成推理时的模型结构，即structural re-parametrization technique过程。根据论文中的图4可以看到， 结构重参数化主要分为两步， 第一步主要将Conv2d算子和BN算法融合以及将只有BN的分支转换成一个Conv2d算子， 第二步将每个分支上的3x3卷积层融合成一个卷积层。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231124171647594.png" alt="image-20231124171647594"></p><h5 id="融合Conv2d和BN"><a href="#融合Conv2d和BN" class="headerlink" title="融合Conv2d和BN"></a>融合Conv2d和BN</h5><p>Conv2d和BN的融合对于网络的优化来讲已经是基本操作了。 因为conv2d和BN两个算子都是做线性运算， 所以可以融合成一个算子，这里需要强调一点， 融合是网络训练完之后做的， 所以现在讲的默认都是推理模型， 注意BN在训练以及推理时计算方式是不同的。对于卷积层， 每个卷积核的通道数与输入特征图的通道数相同， 卷积核的个数决定了输出特征图的通道个数。对于BN层（推理模式）, 主要包含4个参数， μ（均值）, σ2（方差）， γ和β， 其中 μ（均值）, σ2（方差）是训练过程统计得到的，  γ和β是训练过程学习得到的。对于特征图第i个通道BN的计算公式如下， 其中为防止分母为0加上了一个非常小的数。</p><script type="math/tex; mode=display">y_i = \frac{x_i - \mu_i}{\sqrt{\sigma_i^2 + \varepsilon}} \cdot \gamma_i + \beta_i</script><p>在论文3.3章节中， 作者给出了转换公式（对于通道i）， 其中M代表输入BN层的特征图（activation）， 这里忽略了上面分母加上的非常小的数。</p><script type="math/tex; mode=display">bn(M, \mu, \sigma, \gamma, \beta)_{:, i, :, :} = (M_{:,i, :, :} - \mu_i) \frac{\gamma_i}{\sigma_i} + \beta_i</script><p>所以转换后新的卷积层权重计算公式为（对于第i个卷积核）， W‘和b’是新的权重和偏置。</p><script type="math/tex; mode=display">W'_{i,:,:,:} = \frac{\gamma_i}{\sigma_i}W'_{i,:,:,:}, b'_i = \beta_i = \frac{\mu_i \gamma_i}{\sigma_i}</script><p>为解释上面的过程， 作图如下：</p><p>假设输入的特征图（input feature map）如下图所示， 输入通道数为2， 然后采用两个卷积核（图中只画了第一个卷积核对应的参数）</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231125142704644.png" alt="image-20231125142704644"></p><p>计算输出特征图通道1上的第一个元素， 即当卷积核1在输入特征图红色框区域卷积时得到的额值（为保证输入输出特征图高宽不变， 所以对input feature map进行了padding）， 其他位置的计算过程类似。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231125144331638.png" alt="image-20231125144331638"></p><p>然后再将卷积层输出的特征图作为BN层的输入， 这里计算一下输出特征图通道1上的第一个元素， 按照上述BN在推理时的计算公式即可得到如下的计算结果。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231125154852304.png" alt="image-20231125154852304"></p><p>将卷积层输出的特征图作为BN层的输入， 这里同样计算输出特征图通道1上的第一个元素。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231125160112602.png" alt="image-20231125160112602"></p><p>最后对上述公式进行变形， 得到转化后新卷积层只需在对应第i个卷积核的权重上乘以</p><script type="math/tex; mode=display">\frac{\gamma_i}{\sqrt{\sigma_i^2 + e}}</script><p>系数即可， 对应第i个卷积核新的偏置等于</p><script type="math/tex; mode=display">\beta_i - \frac{\mu_i \gamma_i}{\sqrt{\sigma^2_i + e}}</script><p>因为之前采用Conv2d+BN的组合中Conv2d默认不采用偏置或偏置为0</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231127173306537.png" alt="image-20231127173306537"></p><h5 id="Conv2d-BN融合实验"><a href="#Conv2d-BN融合实验" class="headerlink" title="Conv2d + BN融合实验"></a>Conv2d + BN融合实验</h5><p>参考作者提供的源码， 首先了一个module包含了卷积核BN模块， 然后按照上述转换公式将卷积层的权重和BN的权重进行融合转换， 接着载入到新建的卷积模块fused_conv中， 嘴周随机创建一个Tensor(f1)将它们分别输入到module以及fused_conv中， 通过对比二者的输出可以发现它们的结果相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># @File       : conv2dBN.py</span></span><br><span class="line"><span class="string"># @Time       ：2023/11/27 17:38</span></span><br><span class="line"><span class="string"># @Software   : PyCharm</span></span><br><span class="line"><span class="string"># @Description：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ================【功能：】====================</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    torch.random.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    f1 = torch.randn(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    module = nn.Sequential(OrderedDict(</span><br><span class="line">        conv=nn.Conv2d(in_channels=<span class="number">2</span>, out_channels=<span class="number">2</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">        bn=nn.BatchNorm2d(num_features=<span class="number">2</span>)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    module.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        output1 = module(f1)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;output1: \n&quot;</span>, output1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># fuse conv + bn</span></span><br><span class="line">    kernel = module.conv.weight</span><br><span class="line">    running_mean = module.bn.running_mean</span><br><span class="line">    running_var = module.bn.running_var</span><br><span class="line"></span><br><span class="line">    gamma = module.bn.weight</span><br><span class="line">    beta = module.bn.bias</span><br><span class="line"></span><br><span class="line">    eps = module.bn.eps</span><br><span class="line">    std = (running_var + eps).sqrt()</span><br><span class="line">    t = (gamma / std).reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># [ch] -&gt;[ch, 1, 1, 1]</span></span><br><span class="line">    kernel = kernel * t</span><br><span class="line">    bias = beta - running_mean * gamma / std</span><br><span class="line">    fused_conv = nn.Conv2d(in_channels=<span class="number">2</span>, out_channels=<span class="number">2</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    fused_conv.load_state_dict(OrderedDict(weight=kernel, bias=bias))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        output2 = fused_conv(f1)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;output2: \n&quot;</span>, output2)</span><br><span class="line"></span><br><span class="line">    np.testing.assert_allclose(output1.numpy(), output2.numpy(), rtol=<span class="number">1e-03</span>, atol=<span class="number">1e-05</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Convert module has been tested, and the result looks good!&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">output1: </span></span><br><span class="line"><span class="string"> tensor([[[[ 0.2554, -0.0267,  0.1502],</span></span><br><span class="line"><span class="string">          [ 0.8394,  1.0100,  0.5443],</span></span><br><span class="line"><span class="string">          [-0.7252, -0.6889,  0.4716]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 0.6937,  0.1421,  0.4734],</span></span><br><span class="line"><span class="string">          [ 0.0168,  0.5665, -0.2308],</span></span><br><span class="line"><span class="string">          [-0.2812, -0.2572, -0.1287]]]])</span></span><br><span class="line"><span class="string">output2: </span></span><br><span class="line"><span class="string"> tensor([[[[ 0.2554, -0.0267,  0.1502],</span></span><br><span class="line"><span class="string">          [ 0.8394,  1.0100,  0.5443],</span></span><br><span class="line"><span class="string">          [-0.7252, -0.6889,  0.4716]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 0.6937,  0.1421,  0.4734],</span></span><br><span class="line"><span class="string">          [ 0.0168,  0.5665, -0.2308],</span></span><br><span class="line"><span class="string">          [-0.2812, -0.2572, -0.1287]]]])</span></span><br><span class="line"><span class="string">Convert module has been tested, and the result looks good!</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="将1x1卷积换成3x3卷积"><a href="#将1x1卷积换成3x3卷积" class="headerlink" title="将1x1卷积换成3x3卷积"></a>将1x1卷积换成3x3卷积</h5><p>以1x1卷积层中的某一个卷积为例， 只需在原来权重周围补一圈零就行， 这样公式变成了3x3的卷积层， 为了保证输入输出特征图高宽不变， 此时需要将padding设置成1（原来卷积核大小为1x1时padding为0）</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231127175704591.png" alt="image-20231127175704591"></p><h5 id="将BN换成3x3卷积"><a href="#将BN换成3x3卷积" class="headerlink" title="将BN换成3x3卷积"></a>将BN换成3x3卷积</h5><p>对于只有BN的分支由于没有卷积层， 所以我们可以先构建一个卷积层， 如下图， 构建一个3x3的卷积层， 该卷积层只做了恒等映射， 即输入输出特征图不变， 按照上述的融合方式将卷积层与BN层进行融合。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231127181703746.png" alt="image-20231127181703746"></p><h5 id="多分支融合"><a href="#多分支融合" class="headerlink" title="多分支融合"></a>多分支融合</h5><p>上面介绍了如何将每个分支融合转换成一个3x3的卷积层， 下面需进一步将多分支转换成一个单路3x3卷积层。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/RepVGG_conv_bn.jpg" alt="RepVGG_conv_bn"></p><p>合并过程也很简单， 直接将这三个卷积层的参数相加即可。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231127183628979.png" alt="image-20231127183628979"></p><p>接下来看论文的图就很清楚了。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231124171647594.png" alt="image-20231124171647594"></p><h4 id="模型配置"><a href="#模型配置" class="headerlink" title="模型配置"></a>模型配置</h4><p>论文给中对模型进一步细分由RepVGG-A, RepVGG-B以及RepVGG-Bxgy三种配置。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231127184304855.png" alt="image-20231127184304855"></p><p>可以看出RepVGG-B比RepVGG-A更深。RepVGG-A中的base layers of each stage为1， 2， 4， 14， 1, 而RepVGG-B为1， 4， 6， 16， 1。更加详细的配置可以看表3。其中a代表模型stage2-4的宽度缩放因子， b代表模型最后一个stage的宽度缩放因子。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231127184908729.png" alt="image-20231127184908729"></p><p>而RepVGG-bxgy配置在RepVGG-B的基础上加入了组卷积（Group Convolution）， 其中gy表示组卷积采用的groups参数为y， 注意不是所有的卷积层都采用组卷积， 根据源码可知， 从stage2开始（索引从1开始）的第2， 4， 6， 8， 10， 12， 14， 16， 18， 20， 22， 24， 26的卷积层采用组卷积。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h4&gt;&lt;p&gt;VGG网络是2014年牛津大学提出的， 在201</summary>
      
    
    
    
    
    <category term="ComputureVision" scheme="https://guudman.github.io/tags/ComputureVision/"/>
    
  </entry>
  
  <entry>
    <title>YOLOX</title>
    <link href="https://guudman.github.io/2023/11/24/YOLOX/"/>
    <id>https://guudman.github.io/2023/11/24/YOLOX/</id>
    <published>2023-11-24T09:55:08.000Z</published>
    <updated>2023-11-29T10:06:41.100Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>前面讲介绍过yolov5， 这里再来介绍YOLOX, YOLOx是旷视科技在2021年发表的一篇论文， 当时对标的网络就是YOLOv5。YOLOx从YOLOv5中引入如下三点：decoupled head, anchor-free以及advanceed label assigning strategy（SimOTA)。</p><p>在自己的项目中YOLOv5和YOLOx到底如何选择呢， 如果你的数据集分辨率不是很高， 比如640x640， 二者都可以试试， 如果你的图像分辨率很高， 比如1280x1280， 那么建议使用yolov5， 因为yolov5官方仓库提供了更大尺度的预训练权重， 而YOLOx当前只有640x640的预训练权重。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231122105024189.png" alt="image-20231122105024189"></p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231122105142040.png" alt="image-20231122105142040"></p><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>下图是根据源码绘制的YOLOX-L网络结构， 因为它是基于YOLO v5构建的， 所以Backbone以及PAN部分和YOLO v5是一模一样的， 注意这里说的YOLO v5对应的是tag：v5.0版本， 而我们之前的yolov5对应的是tag:v6.1版本， 所以在backbone部分有细微区别。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolox.jpg" alt="yolox"></p><p>yolox与yolo v5在结构上有什么差异呢， 主要区别在于检测头head部分， 之前的检测头就是通过一个卷积核大小为1x1的卷积层实现的， 即这个卷积层要同时预测类别分数， 边界框回归参数以及object ness, 这种方式在文章中称为coupled detection head（耦合检测头）。作者说采用coupled detection head对网络是有害的， 如果将coupled detection head换成decoupled detection head （解耦的检测头）能够大幅提升网络的收敛速度。论文中也对比了yolov3中使用couple detection和decouple detection head的训练收敛情况， 明显采用decoupled detection head后收敛速度会更快。</p><p>原论文给出的decouple detection head结构到底是啥样的， 作图如下， 在decoupled detection head中对预测Cls， Reg以及IoU参数分别使用三个不同的分支， 这样就将三者进行了解耦， 需要注意的是， 在YOLOx中对不同的预测特征图采用不同的head， 即参数不共享。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231124111429132.png" alt="image-20231124111429132"></p><h4 id="Anchor-Free"><a href="#Anchor-Free" class="headerlink" title="Anchor-Free"></a>Anchor-Free</h4><p>近几年关于Anchor-Free的网络也较多， 比如之前的FCOS, YOLOx也是一个Anchor-Free的网络， 并且借鉴了FCOS中的思想。上面提到YOLOx中的decouple detection head，它对预测特征图（feature map/Grid网络）上的每个位置都预测了num_cls + 4 + 1参数， 其中num_cls代表检测的目标类别数， 4代表网络预测的目标边界框参数， 1代表object ness（图中标的是IoU）</p><p>由于YOLOX是Anchor-Free的网络， 所以head在每个位置处直接预测4个目标边界框参数[t_x, t_y, t_w, t_h]， 如下图所示， 这4个参数分别对应预测目标中心点相对Grid Cell左上角(c_x, c_y)的偏移量， 以及目标的宽度、高度因子，注意这些值都是相对预测特征图尺度上的， 如果要映射回原图需要乘上当前特征图相对原图的步距stride。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231124141905359.png" alt="image-20231124141905359"></p><h4 id="损失计算"><a href="#损失计算" class="headerlink" title="损失计算"></a>损失计算</h4><p>在网络的检测头中有Cls.分支， Reg.分支以及IoU.分支（其实是Obj.分支）, 所以损失由Lcls， Lreg以及Lobj三部分组成， 其中Lcls和Lobj采用的是二值交叉熵损失（BCELoss）而Lreg采用的是IoULoss， 还要注意的是Lcls以及Lreg只计算正样本的损失， 而Lobj既计算正样本也计算负样本的损失。 </p><script type="math/tex; mode=display">Loss = \frac{L_{cls} + \lambda L_{reg} + L_{obj}}{N_{pos}}</script><p>其中，</p><p>L_cls代表分类损失</p><p>L_reg代表定位损失</p><p>L_obj代表损失</p><p>λ代表定位损失的平衡系数， 源码中设置的是5.0</p><p>Npos代表被分为正样本的Anchor Point数</p><h4 id="正负样本匹配策略SimOTA"><a href="#正负样本匹配策略SimOTA" class="headerlink" title="正负样本匹配策略SimOTA"></a>正负样本匹配策略SimOTA</h4><p>训练网络时通过SimOTA来匹配正负样本， 而SimOTA是由OTA（Optimal Transport Assignment）简化得到， OTA也是旷视科技同年出的一篇文章（《Optimal transport assignment for object detection》）。在yolov3的基准上使用SimOTA能够给AP带来2.3个点的提升。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231124145903302.png" alt="image-20231124145903302"></p><p>什么是OTA， 下面是论文中的原话</p><blockquote><p>Specifically, OTA analyzes the label assignment from a global perspective and formulate the assigning processdure as an Optimal Transport (OT) problem, producing the SOTA performance among the current assigning startegies</p></blockquote><p>简单来说就是将匹配正负样本的过程看成一个最优传输过程。以给牛奶基地给城市配送牛奶为例。</p><p>在SimOTA正负样本匹配过程中， 城市对应的是每个样本（对应论文中的anchor point， 其实就是隔日达网格中的每个cell）， 牛奶基地对应的是标注好的GT Bbox, 那现在的目标是怎样以最低的成本（cost）将GT分配给对应的样本。根据论文公式， cost的计算公式如下， λ为平衡系数， 源码中设置为3.0。</p><script type="math/tex; mode=display">c_{ij} = L_{ij}^{cls} + \lambda L_{ij}^{reg}</script><p>可以看到损失由分类损失和回归损失两部分组成，并且网络预测的类别越准确cost越小，网络预测的目标边界框越准确cos越小。那么最小化cost可以理解为让网络以最小的学习成本学习到有用的知识。举例如下：</p><p>上面提到， 城市对应的是每个样本（对应论文中的anchor point， 其实就是grid网络中的每个cell）， 那是不是所有的样本都参与cost的计算呢， 当然不是。先回顾一下FCOS网络， 它是如何匹配正负样本？它是将那些落入GT中心sub-box范围内的样本视为正样本， 其他的为负样本。那么在SimOTA中， 也有类似的筛选过程， 通过源码可知它首先会将落入目标GT Bbox内或落入fixed center area内的样本给筛选出来， 在源码中作者将center_ratius设置为2.5， 即fixed center area是5x5大小的box， 如下图， feature map中所有打勾的位置都是通过筛选得到的样本（anchor point）， 注意， 这里将落入GT Bbox与fixed center area相交区域内的样本用橙色的勾表示。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231124153407123.png" alt="image-20231124153407123"></p><p>然后计算网络在这些样本（anchor point）位置处的预测值（目标类别以及目标边界框）和每个GT的Lcls以及Lij_reg（由于回归损失时IoULoss, 所以这里也知道每个杨和每个GT的IoU），然后再计算每个样本和每个GT之间的cost， 这里需要注意一下， 在代码中每个cost的过程如下， 和论文中的公式有一点点区别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cost = (</span><br><span class="line">            pair_wise_cls_loss</span><br><span class="line">            + <span class="number">3.0</span> * pair_wise_ious_loss</span><br><span class="line">            + <span class="number">100000.0</span> * (~is_in_boxes_and_center)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p>其中， pair_wise_cls_loss就是每个样本与GT之间的分类损失Lij_cls</p><p>pair_wise_ious_loss是每个样本与每个GT之间的回归损失Lij_reg</p><p>is_in_boxes_and_center代表那些落入GT Bbox与fixed center area交集内的样本， 即图中橙色对应的样本， 然后~取反表示不在GT Bbox与fixed center area交集内的样本。乘以10000.0也就是说对于GT Bbox与fixed center area交集外的样本cost加上了一个非常大的数， 这里在最小化cost过程中会优先选择GT Bbox与fixed center area交集内的样本。</p><p>下面介绍如何利用cost匹配正负样本</p><p>首先构建两个矩形框， 一个是之前筛选处的anchor point与每个GT之间的cost矩阵， 另一个是Anchor Point与每个GT之间的IoU矩阵， 接着计算n_candidate_k并结合IoU对Anchor Point做进一步筛选（保留IoU大的Anchor point）， n_candidate_K取10和Anchor point 数量之间的最小值，在下面给出的实力由于Anchor point的数量为6， 所以n_candidate_k=6。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231124155843424.png" alt="image-20231124155843424"></p><p>然后对每个GT计算剩下所有的anchor point的IoU之和然后向下取整得到针对每个GT所采用的正样本数量， 即代码中计算得到的dynamic_ks（这个计算过程对应论文中的Dynamic k Estimation Strategy）。对于下面的示例， GT1的所有Anchor Point的IoU之和为3.0向下取整就是3， 所以对于GT1有3个正样本， 同理GT2也有3个正样本。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231124160534109.png" alt="image-20231124160534109"></p><p>根据计算得到的dynamic_ks（每个GT对应几个正样本）和cost矩阵找出所有的正样本（根据cost的数值大小）。比如实例中的GT1, 刚计算采用3个正样本， 然后GT1和所有的Anchor point的cost按照从小到大的顺序将千3小的anchor point找出来，即实例中的A1, A2, A5。同理对于GT2, cost排前3的是A3, A4和A5。根据以上结果， 再构建一个anchor point分配矩阵， 记录每个GT对应哪些正样本， 对应的位置标记1， 其余位置标记0.</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231124161252070.png" alt="image-20231124161252070"></p><p>然后会发现GT1和GT2同时分配给了A5, 为解决这种带有歧义的问题， 又增加了一个判断， 如果多个GT同时分配给一个Anchor Point， 那么只选cost最小的GT， 所以只将GT2分配给A5。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h4&gt;&lt;p&gt;前面讲介绍过yolov5， 这里再来介绍YOLO</summary>
      
    
    
    
    
    <category term="ComputureVision" scheme="https://guudman.github.io/tags/ComputureVision/"/>
    
  </entry>
  
  <entry>
    <title>FCOS</title>
    <link href="https://guudman.github.io/2023/11/24/FCOS/"/>
    <id>https://guudman.github.io/2023/11/24/FCOS/</id>
    <published>2023-11-24T09:46:37.000Z</published>
    <updated>2023-11-29T10:05:22.167Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>论文原文：<a href="[arxiv.org/pdf/1904.01355.pdf](https://arxiv.org/pdf/1904.01355.pdf">论文原文</a>)</p><p>在之前的一些目标检测网络中， 比如Faster RCNN, SSD, yolov2~v5都是基于Anchor进行预测，即在原图上生成一堆密密麻麻的Anchor Boxes， 然后网络基于这些Anchor去预测它们的类别、中心点偏移量以及宽高缩放因子得到网络预测输出的目标， 最后通过NMS(no-max-suppression非极大值抑制)即可得到最终预测目标。针对基于Anchor网络存在的问题， 原作者总结如下四点：</p><p>1、检测器的性能和Anchor的size以及aspect ratio相关， 比如在RetinaNet中改变Anchor(论文中说这是个超参数hyper-parameters)能够产生约4%的AP变化， 也就是说Anchor需要设置的合适才可以。</p><p>2、一般Anchor的size和aspect ratio（ 宽度/高度 的比值）都是固定的， 所以很难处理哪些形状变化很大的目标（比如一本书横着放于竖着放久不一样）。而且迁移到其他任务中时，如果新的数据集目标和预训练数据集中的目标形状差别很多， 一般需要重新设计Anchor。</p><p>3、为了达到更高的召回率（查全率），一般需要再图片中生成非常密集的Anchor Boxes尽可能保证每个目标都会有Anchor Boxes和它相交。比如在FPN（Feature Pyramid Network）中会生成超过18万个Anchor Boxes（以输入图片最小边长800为例）， 那么在训练时绝大部分的Anchor Boxes都会被分成负样本， 这样会导致正负样本及其不均。</p><p>4、Anchor的引入使得网络在训练过程中更加的繁琐， 因为匹配正负样本时需要计算每个Anchor Boxes和每个GT BBoxes之间的IoU</p><p>虽然基于Anchor的目标检测网络存在如上所述的问题， 但并不能否认它的有效性， 比如现在常用的YOLO V3~V5， 它们都是基于Anchor的网络， 当然今天的主角是Anchor-Free， 现在有关Anchor-Free的网络也很多， 比如DenseBox, YOLOV1, CornerNet, FCOS以及CenterNet等等， 而今天要讲的FCOS不仅是Anchor-Free还是One-Stage， FCN-base。</p><p>FCOS是2019年发表在CVPR上的文章， 这篇文章的想法不仅简单而且有效， 它的思想是跳出Anchor的限制， 在预测特征图的每个位置上直接去预测该带你分别举例目标左侧， 上侧， 右侧以及下侧的距离。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231120194020661.png" alt="image-20231120194020661"></p><h4 id="FCOS网络结构"><a href="#FCOS网络结构" class="headerlink" title="FCOS网络结构"></a>FCOS网络结构</h4><p>下图是2020年发表的版本， 与2019年发表的版本有些不同， 区别在于Center-ness分支的位置， 2019年发表的版本中是将Center-ness分支和classification分支放在一起的，但在2020年论文的图中将Center-ness分支和Regression分支放在一起的， 论文中说后者的效果更好一点。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231121183512578.png" alt="image-20231121183512578"></p><p>下图是根据源码绘制的网络结构。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/fcos.jpg" alt="fcos"></p><p>上图中Backbone以ResNet50为例， FPN是在Backbone输出的C3, C4和C5上先生成P3, P4和P5， 然后在P5的基础上通过一个卷积核大小为3x3步距为2的卷积层得到P6， 最后在P6的基础上再通过一个卷积核大小为3x3步距为2的卷积层得到P7。</p><p>接着看Head部分（注意这里的Head是共享的， 即P3~P7都是共用一个Head), 有三个分支：Classification, Regression和Center-ness。其中Regression和Center-ness是同一个分支上的两个不同小分支。每个分支都会先通过4个Conv2d+GB+ReLU的组合模块， 然后再通过一个卷积核大小为3×3步距为1的卷积层得到最终的分类结果。 </p><p>对于classification分支， 在预测特征图的每个位置上都会预测80个score参数（MS COCO数据集目标检测任务的类别数为80）。</p><p>对于Regressio分支， 在预测特征图的每个位置上都会预测4个距离参数（距离目标左侧距离l， 上侧距离为t， 右侧距离为r以及下侧距离b， 这里预测的数值是相对特征图尺度上的）。假设对于预测特征图上某个点映射回原图的坐标是（c_x, c_v）, 特征图相对原图的步距为s， 那么网络预测该点对应的目标边界框坐标为：</p><script type="math/tex; mode=display">x_{min} = c_x - l * s, y_{min} = c_y - t * s, x_{max} = c_x + r \cdot s, y_{max} = c_y + b \cdot s</script><p>对于Center-ness分支， 在预测特征图的每个位置上都会预测1个参数， center-ness反映的是该点（特征图上的某一点）距离目标中心的远近程度， 它的值域在0~1之间， 距离目标中心越近center-ness越接近1， 下面是center-ness真实标签的计算公式（计算损失时只考虑正样本， 即预测点在目标内的情况）</p><script type="math/tex; mode=display">centerness^*=\sqrt{\frac{min(l^*, r^*)}{max(l^*, r^*)} \times \frac{min(t^*, b^*)}{max(t^*, b^*)}}</script><p>在网络后处理部分筛选高质量的bbox时， 会将预测的目标class score与center-ness相乘再开根号， 然后根据得到的结果对bbox进行排序， 只保留分数较高的bbox， 这样做的目的是筛掉哪些目标class score低且预测点距离目标中心较远的bbox， 最终保留下来的就是高质量的bbox。下表展示了使用和不使用<code>center-ness</code>对AP的影响，只看第一行和第三行，不使用<code>center-ness</code>时AP为33.5，使用<code>center-ness</code>后AP提升到37.1，说明<code>center-ness</code>对FCOS网络还是很有用的。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231121193354426.png" alt="image-20231121193354426"></p><h4 id="正负样本的匹配"><a href="#正负样本的匹配" class="headerlink" title="正负样本的匹配"></a>正负样本的匹配</h4><p>在计算损失之前， 我们需要进行正负样本的匹配， 在基于Anchor的目标检测网络中， 一般会通过计算每个Anchor Box与每个GT的IoU配合事先设定的IoU阈值去匹配， 比如某个Anchor Box与某个GT的IoU大于0.7， 那么我们就将该Anchor Box设置为正样本， 但对于Anchor-Free的网络根本没有Anchor， 那该如何匹配正负样本呢， 在2020版本的论文中有这样的一段话：</p><blockquote><p>Specifically, location (x, y) is considered as a positive sample if it falls into the center area of any ground-truth box, by following [42]. The center area of a box centered at (cx , cy ) is defined as the sub-box (cx − rs, cy − rs, cx + rs, cy + rs) , where s is the total stride until the current feature maps and r is a hyper-parameter being 1.5 on COCO. The sub-box is clipped so that it is not beyond the original box. Note that this is different from our original conference version, where we consider the locations positive as long as they are in a ground-truth box.</p></blockquote><p>最开始的一句话就是说， 对于特征图上的某一点（x, y)， 只要它落入GT box中心区域， 那么它就被视为正样本（其实在2019年的论文中最开始说的是重要落入GT内就算正样本）。但在2020年发表论文中， 新加了一条规则，在满足上述条件外， 还需满足点(x, y)在（c_x - rs, c_y - rs, c_x + rs, c_y + rs）这个sub-box范围内， 其中（c_x, c_y）就是GT的中心点， s是特征图相对原图的步距， r是一个超参数控制距离GT中心的远近， 在COCO数据集中r设置为1.5， 换句话说点（x, y)不仅要在GT的范围内， 还要离GT的中心点（c_x, c_y）足够近才能被视为正样本。</p><p>作图如下， 假设上面两个feature map对应的是同一个特征图， 将特征图上的每个店映射回原图就是下面图片中黑色的圆点， 根据2019年论文的匹配准则， 只要落入GT Box就算正样本， 所以左侧的feature map中打勾的位置都被视为正样本。根据2020年的论文， 不仅要落入GT Box还要在（c_x - rs, c_y - rs, c_x + rs, c_y + rs）这个sub-box范围内， 所以右侧的feature map中打勾的位置被视为正样本。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231121195818634.png" alt="image-20231121195818634"></p><p>这里肯定会有人问， 如果feature map上的某个点同时落入两个GT Box内（即两个GT相交区域）， 那么该点到底分配给哪个GT Box， 这就是论文中提到的Ambiguity问题， 下图中， 橙色对应的点同时落入到人和球拍两个GT Box中， 此时默认将该点分配给面积Area最小的GT Box， 即图中的球拍， 其实引入FPN后能后减少这种情况。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231121200356708.png" alt="image-20231121200356708"></p><h4 id="损失计算"><a href="#损失计算" class="headerlink" title="损失计算"></a>损失计算</h4><p>FCOS网络结构中提高， Head总共有三个输出分支：Classification，Regression和Center-ness， 所以损失由分类损失L_cls， 定位损失L_reg以及center-ness损失L_ctrness三部分组成。</p><script type="math/tex; mode=display">L(p_{x, y}, t_{x, y}, s_{x, y}, )=\frac{1}{N_{pos}} \sum_{x,y}(p_{x,y}, c^*_{x,y}) + \frac{1}{N_{pos}} \sum_{x,y}1_{c^*_{x,y}>0}L_{reg}(t_{x,y}, t*_{x,y}) +  \frac{1}{N_{pos}} \sum_{x,y}1_{c^*_{x,y}>0}L_{ctrness}(s_{x,y}, s*_{x,y})</script><p>上式子中：</p><p>P_x,y表示特征图（x, y)点处预测的每个类别的score</p><p>c*_x,y表示特征图（x, y)点对应的真实类别标签</p><p>1{c*_x,y&gt;0}表示当特征图(x, y)点被匹配为正样本时为1， 否则为0</p><p>t_x, y表示在特征图(x, y)点处预测的目标边界框信息</p><p>t*_x, y表示在特征图(x, y)点处真实的目标边界框信息</p><p>s_x, y表示在特征图(x, y)点处预测的center-ness</p><p>s*_x, y表示在特征图(x, y)点处真实的center-ness</p><p>对于分类损失L_cls采用bce_focal_loss， 即二值交叉熵损失配合focal_loss， 计算损失时所有样本都会参与计算（正样本和负样本）。定位损失L_reg采用giou_loss（在2019年版中采用iou_loss， 但在2020年版本中采用giou_loss）, 计算损失时只有正样本参数计算， center-ness损失采用二值交叉熵损失， 计算损失时只有正样本参数计算。 </p><p>在匹配正负样本过程中， 对于特征图（x, y)点处对应的GT信息c*x,y和t*x, y比较好得到一点， 只要匹配到某一GT目标则c*x,y对应GT的类别， t*x,y对应GT的bbox。而获得真实的center-ness（s*x,y）要复杂一点， 下面是s*x, y的计算公式。</p><script type="math/tex; mode=display">centerness^*=\sqrt{\frac{min(l^*, r^*)}{max(l^*, r^*)} \times \frac{min(t^*, b^*)}{max(t^*, b^*)}}</script><p>为方便理解， 作图如下， 假设对于特征图上的某一点（图中蓝色填充的cell）映射回原图， 对应图片中的黑色点， 然后计算该点距离GT box左侧， 上侧， 右侧， 下侧的距离就能得到l*, t*, t*, b*再套用上面的公式就能得到s*x,y 。（这里的l*, t*, t*, b*无论是计算特征图尺度上的还是原图尺度上的都无所谓， 因为centerness*对尺度不敏感）</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231122100035331.png" alt="image-20231122100035331"></p><h4 id="Ambiguity问题"><a href="#Ambiguity问题" class="headerlink" title="Ambiguity问题"></a>Ambiguity问题</h4><p>论文中专门有一部分内容用来分析ambiguous sample问题， 即在匹配正样本的时当特征图上的某一点同时落入多个GT Box内时， 应该分配给哪一个GT的问题。</p><blockquote><p>Another concern about the FCN-based detector is that it may have a large number of ambiguous samples due to the overlap in ground-truth boxes.</p></blockquote><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231121200356708.png" alt="image-20231121200356708"></p><p>当特征图上的某一点同时落入多个GT Box内时， 默认将该点分配给面积Area最小的GT Box， 当然这并不是ig很好的解决方法。ambiguous samples的存在始终会对网络的学习以及预测产生干扰。作者再COCO2017年的val数据上进行了分析，作者发现如果不使用FPN结构时（仅在P4特征层上进行预测）会存在大量的ambiguous samples（大概占23.16%）， 如果启用FPN结构ambiguous samples会大幅度降低（大概占7.24%)。因为在FPN中会采用多个预测特征图， 不同尺度的特征图负责预测不同尺度的目标， 比如P3负责预测小型目标， P5负责预测中等目标， P7负责预测大型目标。作图如下， 比如对于小型目标 球拍， 根据尺度划分准则， 它被划分到feature map1上， 对于大型目标 人， 根据尺度划分准则【后面会讲】被划分到feature map2上， 这样在匹配正负样本时能将部分重叠在一起的目标（这里主要指不同尺度的目标）给分开， 解决了ambiguous samples的问题。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231122102449632.png" alt="image-20231122102449632"></p><p>如果再采用center sampling匹配准则（在2020年更新的FCOS版本中， 匹配正样本时要求不仅要落入GT Box还要在（c_x - rs, c_y - rs, c_x + rs, c_y + rs）这个sub-box范围内）能够进一步降低ambiguous samples的比例（小于3%）。 </p><h4 id="Assigning-objects-to-FPN"><a href="#Assigning-objects-to-FPN" class="headerlink" title="Assigning objects to FPN"></a>Assigning objects to FPN</h4><p>上文提到， 使用FPN结构能够降低ambiguous samples的比例， 那么按照怎样的准则将目标划分到对应尺度的特征图上呢， 在FPN中采用如下公式计算分配的</p><script type="math/tex; mode=display">k = [k_0 + log_2(\sqrt{wh}/224)]</script><p>在FCOS中， 作者发现套用FPN中的公式效果并不是很好， 作者猜测是因为按照FPN中的分配准则， 不能确保目标在对应感受野范围内， 比如对于某个特征层， 每个cell的感受野为28x28， 但分配到该特征层上的目标为52×52.</p><p>最终采用的是max（l*, t*, r*, b*）策略， 其中l*, t*, r*, b*分别代表某点（特征图映射在原图上）相对GT Box左边界， 上边界， 有边界以及下边界的距离。关于这个策略在2020年版本的论文中介绍的很清楚， 对于不同的预测特征图只要满足以下公式即可， 比如说P4特征图只要max(l*, t*, r*, b*)在（64， 128）之间即为正样本：</p><script type="math/tex; mode=display">m_{i - 1} < max(l^*, t^*, r^*, b^*) < m_i</script>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h4&gt;&lt;p&gt;论文原文：&lt;a href=&quot;[arxiv.org</summary>
      
    
    
    
    
    <category term="ComputureVision" scheme="https://guudman.github.io/tags/ComputureVision/"/>
    
  </entry>
  
  <entry>
    <title>HRNet</title>
    <link href="https://guudman.github.io/2023/11/19/HRNet/"/>
    <id>https://guudman.github.io/2023/11/19/HRNet/</id>
    <published>2023-11-19T06:41:20.000Z</published>
    <updated>2023-11-29T10:05:35.998Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>论文原文： <a href="[arxiv.org/pdf/1902.09212.pdf](https://arxiv.org/pdf/1902.09212.pdf">论文原文</a>)</p><p>HRNet是由中国科学技术大学和亚洲微软研究院在2019年共同发表的， 这篇文章中的HRNet（High-Resolution Net）是针对2D人体姿态估计任务提出的， 并且该网络主要针对单一人体姿态估计。人体姿态估计主要应用在人体行为识别、人机交互、动画制作等。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231119093443759.png" alt="image-20231119093443759"></p><p>主要由两种深度学习方法处理Human Pose Estimation任务。</p><p>1、基于regression的方式， 即直接预测每个关键点的位置坐标</p><p>2、基于heatmap的方式， 即针对每个关键点预测一张热度图（预测出现在每个位置上的分数）</p><hr><h4 id="HRNet"><a href="#HRNet" class="headerlink" title="HRNet"></a>HRNet</h4><p>下图是根据源码绘制的HRNet-w32的模型结构简图， 文章中还提到了HENet-w48版本， 二者的区别在于每个模块所采用的通道个数不同， 网路的整体结构都一样， 而该论文的核心思想就是不断地去融合不同尺度上的信息， 也就是论文中说的Exchange Blocks。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/HRNet_1.jpg" alt="HRNet_1"></p><p>从上图可以看出，HRNet首先通过两个卷积核大小为3x3步距为2的卷积层（后面接着BN以及ReLU）共下采样了4倍， 然后通过Layer1模块， 这里的Layer1其实和之前的ResNet中的Layer1类似， 就是重复堆叠Bottleneck， 注意这里的Layer1只会调整通道个数， 并不会改变特征层大小。</p><p>然后是一系列Transition以及stage结构， 每通过一个transition结构都会新增一个尺度分支， 比如transition1， 它在layer1的输出基础上通过并行两个卷积核为3x3的卷积层得到两个不同尺度分支， 即下采样4倍的尺度以及下采样8倍的尺度。在Transition2中在原来的两个尺度分支基础上再新增加一个下采样16倍的尺度。</p><p>下面介绍stage结构， 为方便理解， 以stage3为例， 对于每个尺度分支， 首先通过4个Basic Block，这个就是ResNet中的Basic Block， 然后融合不同尺度上的信息。对于每个尺度分支上的输出都是由所有分支上的输出进行融合得到的。比如对于下采样4倍分支的输出， 它是分别将下采样4倍分支的输出（不做任何处理）、下采样8倍分支的输出通过Up x2上采样2倍以及下采样16倍分支的输出通道通过Up x4上采样4倍进行相加最后通过ReLU得到下采样4倍分支的融合输出，其他分支也类似。xn表示该模块（Basic Block和Exchange Block）重复堆叠n次。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231119111457818.png" alt="image-20231119111457818"></p><p>下面看一下Up和Down是如何实现的，对于所有的Up模块通过一个卷积核大小为1x1的卷积层后通过BN层最后通过Upsample直接放大n倍得到上采样后的结果（这里的上采样默认采用的是nearest最邻近插值）。Down模块相比于Up模块稍微麻烦一点， 每下采样2倍都要增加一个卷积核大小为3x3步距为2的卷积层（注意下图中Conv和Conv2d的区别， Conv2d是普通的卷积层， 而Conv包含了卷积， BN以及ReLU激活函数）。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231119111915278.png" alt="image-20231119111915278"></p><p>最后需要注意的是Stage4中的最后一个Exchange Block只输出下采样4倍分支的输出（即只保留分辨率最高的特征层）， 然后接上一个卷积核大小为1x1卷积核个数为17（因为coco数据集中每个人标注了17个关键点）的卷积层，最终得到的特征层（64x48x17)就是针对每个关键点的heatmap（热力图）。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231119112307305.png" alt="image-20231119112307305"></p><hr><h4 id="预测结果（heatmap）可视化"><a href="#预测结果（heatmap）可视化" class="headerlink" title="预测结果（heatmap）可视化"></a>预测结果（heatmap）可视化</h4><p>关于预测得到的heatmap听起来比较抽象，假设输入网络预测图片的大小为256x192。为保证原图像比例， 在两侧进行padding。右侧是从预测结果也及时heatmap（64x48x17）中提取出的部分关键点对应的预测信息（64x17x1）。上面提到过， 网络最终输出的heatmap的分辨率是原图的1/4，所以高宽分别是64和48，接着对每个关键点的预测信息求最大值的位置， 即预测score最大的位置，作为预测关键点的位置，映射回原图就能得到原图上关键点的坐标。</p><p>在原论文中， 对于每个关键点并不是直接取score最大的位置（直接取score最大的位置其实也没有多大的影响）。假设对于某一关键点heatmap如下图所示， 根据寻找最大score可以找到坐标（3， 3），接着对比该点两侧（x方向）， 上下两侧（y方向）的score。比如先看左右两侧， 明显右侧的score比左侧大（颜色越深，score越大）， 所以最终预测的x坐标偏移了0.25， 同理上下两侧也是一样的。所以x， y的预测分别为3.25和2.75。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231119114105735.png" alt="image-20231119114105735"></p><p>coco数据集中标注的17个关键点的顺序如下：</p><blockquote><p>“kps”: [“nose”,”left_eye”,”right_eye”,”left_ear”,”right_ear”,”left_shoulder”,”right_shoulder”,”left_elbow”,”right_elbow”,”left_wrist”,”right_wrist”,”left_hip”,”right_hip”,”left_knee”,”right_knee”,”left_ankle”,”right_ankle”]</p></blockquote><hr><h4 id="损失的计算"><a href="#损失的计算" class="headerlink" title="损失的计算"></a>损失的计算</h4><p>论文中说采用的是均方误差Mean Squared Error.</p><p>网络预测的最终结果是针对每个关键点的heatmap， 那么训练对应的GT又是什么呢， 根据标注信息我们知道每个关键点的坐标的（原图尺度）， 接着将坐标除以4（缩放到heatmap尺度）再进行四舍五入。针对每个关键点， 先生成一张全值为0的heatmap， 然后对应关键点坐标处填充1就得到下面左侧的图片。如果直接拿左侧的heatmap作为GT去训练网络的话， 会发现网络很难收敛（可以理解为针对每个关键点只有一个正样本， 其他64x48-1个都是负样本， 正负样本及其不均匀）， 为了解决这个问题一般都会以关键点坐标为中心应用一个2D的高斯分布（没有做标准化处理）得到右图所示的GT（数字是随便填充的）， 利用这个GT heatmap配合网络预测的heatmap就能计算MSE损失了。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231119115634460.png" alt="image-20231119115634460"></p><p>最后计算损失的时候， 并非将所有关键点的损失进行相加， 而是每个点的损失分别乘以不同的权重。下面是不同点以及对应的权重。</p><blockquote><p>“kps”: [“nose”,”left_eye”,”right_eye”,”left_ear”,”right_ear”,”left_shoulder”,”right_shoulder”,”left_elbow”,”right_elbow”,”left_wrist”,”right_wrist”,”left_hip”,”right_hip”,”left_knee”,”right_knee”,”left_ankle”,”right_ankle”]</p><p>“kps_weights”: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.2, 1.2, 1.5, 1.5, 1.0, 1.0, 1.2, 1.2, 1.5, 1.5] </p></blockquote><hr><h4 id="评价准则"><a href="#评价准则" class="headerlink" title="评价准则"></a>评价准则</h4><p>在目标检测中可以通过IoU作为预测bbox和真实bbox之间的重合程度或相似程度。在关键点检测任务中一般采用OKS(object keypoint similarity)来表示预测keypoint与真实keypoints的相似程度， 其值域在0到1之间，越靠近1越相似。</p><script type="math/tex; mode=display">OKS=\frac{\sum_{i}[e^{-dIi^2/2s^2k_i^2} \cdot \delta(v_i > 0)]}{\sum_{i}[\delta(v_i > 0)]}</script><p>其中， i表示第i个关键点</p><p>vi表示第i个关键点的可见性，这里的vi是GT提供的， vi=0表示该点在图像外无法标注，vi=1表示虽然该点不可见但大概能猜出位置。vi=2表示该点可见。</p><p>δ(x), 当x为true时值为1， x为false时， 值为0， 通过上面的公式， OKS只计算GT中标出的点， 即vi&gt;0的所有关键点。</p><p>di表示第i个关键点与对应GT之间的欧氏距离。</p><p>s为目标面积的平方根， 原话是：scale s which we define as the square root of the object segment area. 这里的面积应该是分割面积， 该数据在COCO数据集标注信息中都是有提供的。</p><p>ki是用来控制关键点类别i的衰减常数。这个常数是在5000张验证集上统计得到的。</p><hr><h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><p>论文中采用的数据增强有：随机旋转（-45°到45°之间）， 随机缩放（0.65到1.35之间）。随机水平翻转以及half body（有一定概率会对目标进行裁剪， 只保留半身关键点，上半身或者下半身）。</p><p>注意输入图片的比例。假设原始输入图片大小固定尺寸为256x192（height：width=4:3）， 但预测的人体目标的高宽比不是4:3， 此时千万不要简单粗暴的拉伸到256x192。正确的方法是保存目标原比例缩放到对应尺度然后再做padding， （由于高宽比&gt;4:3, 所以保持原比例将高缩放到256， 然后在图片width两侧padding得到256x192）。如果拥有原始图像的上下文信息的话可以直接在原图中固定height， 然后再调整width保证高：宽 = 4:3， 再重新裁剪目标并缩放到256x192。这样预测的结果才是准确的。如果直接简单粗暴的拉伸准确率会明显下降。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h4&gt;&lt;p&gt;论文原文： &lt;a href=&quot;[arxiv.or</summary>
      
    
    
    
    
    <category term="ComputureVision" scheme="https://guudman.github.io/tags/ComputureVision/"/>
    
  </entry>
  
  <entry>
    <title>Mask_R-CNN</title>
    <link href="https://guudman.github.io/2023/11/17/Mask-R-CNN/"/>
    <id>https://guudman.github.io/2023/11/17/Mask-R-CNN/</id>
    <published>2023-11-17T09:53:10.000Z</published>
    <updated>2023-11-29T10:05:40.588Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>Mask R-CNN是2017年发表的文章， 一作是何恺明， 该论文也获得了ICCV 2017年最佳论文奖。并且网络提出后，又霸榜了MS COCO的各项任务，包括目标检测、实例分割以及人体关键点检测任务。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117034359378.png" alt="image-20231117034359378"></p><p>Mask R-CNN是在Faster R-CNN的基础上增加了一个用于预测目标分割Mask的分支（即可预测目标的Bounding Boxes信息、类别信息以及分割Mask信息）</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117034835252.png" alt="image-20231117034835252"></p><p>Mask R-CNN不仅能够同时进行目标检测与分割， 还能很容易扩展到其他任务， 比如预测人体关键点信息。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117035116897.png" alt="image-20231117035116897"></p><p>Mask R-CNN的结构也很简单， 就是通过RoIAlign（在原Fast R-CNN中是RoIPool）得到RoI基础上并行添加一个Mask分支（小型的FCN), 见下图，之前Faster R-CNN是在RoI基础上接上一个Fast R-CNN检测头，即图中class, box分支， 现在又并行了一个Mask分支。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117034359378.png" alt="image-20231117034359378"></p><p>注意带和不带FPN结构的Mask R-CNN在Mask分支上略有不同， 对于带有FPN结构的Mask R-CNN它的class， box分支和Mask分支并不是共用一个RoIAlign。在训练过程中， 对于class， box分支RoIAlign将RPN（Region Proposal Network）得到的Proposals池化到7x7大小， 对于Mask分支RoIAlign将Proposals池化到14×14大小。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117061321377.png" alt="image-20231117061321377"></p><h4 id="RoI-Align"><a href="#RoI-Align" class="headerlink" title="RoI Align"></a>RoI Align</h4><p>在之前的Faster RCNN中， 会使用RoIPool将RPN得到的Proposal池化到相同大小， 这个过程会涉及到quantization或者说取整操作， 这会导致定位不是那么准确（文中称为misalignment问题）</p><p>下面的示意图就是RoIPool的执行过程， 其中会经历两次quantization， 假设通过RPN得到了一个Proposal, 它在原图上的左上角坐标是(10, 10), 右下角的坐标是（124， 124）， 对于要映射的特征层相对原图的步距为32， 通过 RoIPool期望输出为2×2大小。</p><ul><li>将Proposal映射到特征层上， 对于左上角坐标10/32四舍五入后等于0， 对于右下角坐标124/32四舍五入后等于4， 即映射在特征层上的左上角坐标为（0， 0）右下角坐标为（4， 4）。对应下图特征层上从第0行到第4行， 从第0列到第4列的区域（黑色矩形框）, 这是第一次quantization。</li><li>对于期望输出的2×2大小，所以需要经映射在特征层上的Proposal划分成2x2大小区域， 但现在映射在特征层上的Proposal是5×5大小， 无法均分， 所以强行划分后有的区域大有的区域小， 如下图所以，这是第二次quantization。</li><li>对划分后的每个子区域进行maxpool即可得到RoIPool的输出， 即下图中蓝色对应的四个数字。</li></ul><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117065102092.png" alt="image-20231117065102092"></p><p>为解决这个问题， 作者提出了RoIAlign方法代替RoIPool, 以获得更加精细的空间定位信息。</p><p>作者提到将RoI替换成RoIAlign后，分割的Mask准确率相对提升了10%到50%, 并且将预测Mask和class进行了解耦， 解耦后也带来了很大的提升。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117071100480.png" alt="image-20231117071100480"></p><p>下图就是RoIAlign的执行过程，同样假设通过RPN得到了一个Proposal， 它在原图上的左上角坐标是（10， 10）， 右下角的坐标是（124， 124）， 对应要映射的特征层相对原图的步距是32， 通过RoIAlign期望的输出为2×2大小。</p><p>将Proposal映射到特征层上， 左上角作为（0.3125， 0.3125）(不进行四舍五入)， 右下角坐标为（3.875， 3.875）（不进行四舍五入）。为了方便理解， 将特征层上的每个元素都用一个点表示， 就能得到图中下方的gri网格， 图中蓝色的框就是Proposal。</p><p>由于期望输出大小为2x2， 故将Porposal划分成2×2四个子区域，接着根据sampling_ratio在每个子区域中设置采样点， 原论文中默认设置的sampling_ratio为4， 这里为了方便讲解， 将sampling_ratio设置为1.</p><p>然后计算每个子区域中每个采样点的值（利用双线性插值计算）， 最后对每个区域内的所有采样点取均值即为该子区域的输出。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117073133360.png" alt="image-20231117073133360"></p><p>以第一个子区域为例， 这里将sample_ratio设置为1， 所以每个子区域只需设置一个采样点， 第一个子区域的采样点为图中黄色的点（即该子区域的中心点）， 坐标为（1.203， 1.203），然后找到离该采样点最近的四个点（即图中用红色箭头标出的四个黑点），然后利用双线性插值即可计算得到采样点对应的输出-0.8546, 又由于该子区域只有一个采样点， 故该子区域的输出就为-0.8546。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117075332516.png" alt="image-20231117075332516"></p><h3 id="Mask-Branch-FCN"><a href="#Mask-Branch-FCN" class="headerlink" title="Mask Branch(FCN)"></a>Mask Branch(FCN)</h3><p>对于带有FPN和不带有FPN的Mask R-CNN, 它们的Mask分支不太一样， 下图是左边不带有FPN结构的Mask分支， 右侧是带有FPN结构的Mask分支（灰色部分为原Faster R-CNN预测的box， class信息的分支，白色部分为Mask分支）</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117061321377.png" alt="image-20231117061321377"></p><p>下图为带有FPN的Mask分支结构示意图。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/Mask_r-cnn_fpn.jpg" alt="Mask_r-cnn_fpn"></p><p>之前的FCN中提到过， FCN是对每一个像素对每个类别都会预测一个分数， 然后通过softmax得到每个类别的概率， 哪个概率高就将该像素分配给哪个类别， 但在Mask R-CNN中， 作者将预测Mask和class进行了解耦， 即对输入的RoI针对每个类别单独预测一个Mask， 最终根据box， cls分支预测的classes信息来选择对应类别的Mask。解耦后得到很大的提升， 下表是原论文中给出的消融实验结果，其中softmax代表原FCN方式， sigmoid代表Mask R-CNN采取的方式（Mask和class进行了解耦）。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117081349378.png" alt="image-20231117081349378"></p><p>在训练网络的时候输入Mask分支的目标是由RPN提供的， 即Proposals， 但在预测的时候输入Mask分支的目标是由Fast R-CNN提供的（即预测的最终目标）。并且训练时采用的是Proposals全部是Fast R-CNN阶段匹配到的正样本。在训练时Mask利用RPN提供的目标信息能够扩充训练样本的多样性（因为RPN提供的目标边界框并不是很准确， 一个目标可以呈现出不同的情景，类似于围着目标做随机裁剪。从另一个方面来看， 通过Fast R-CNN得到的输出一般都比较准确了， 再通过NMS后剩下的目标就更少了）。在预测时为了获得更加准确的目标分割信息以及减少计算量（通过Fast R-CNN后的目标数会更少）， 此时利用的是Fast R-CNN提供的目标信息。</p><h4 id="Mask-R-CNN损失"><a href="#Mask-R-CNN损失" class="headerlink" title="Mask R-CNN损失"></a>Mask R-CNN损失</h4><p>Mask R-CNN损失就是在Faster R-CNN的基础上加了Mask分支上的损失， 即：</p><script type="math/tex; mode=display">Loss = L_{rpn} + L_{fast_rcnn} + L_{mask}</script><p>关于mask分支上的损失就是二值交叉熵损失（Binary Cross Entropy）</p><h4 id="Maskf分支损失"><a href="#Maskf分支损失" class="headerlink" title="Maskf分支损失"></a>Maskf分支损失</h4><p>在理解Mask分支损失计算之前， 要弄清楚logist（网络预测的输出）是什么， targets（对应的GT）是什么。前面有提到训练时输入Mask分支的目标是RPN提供的Proposals， 所以网络预测的logits是针对每个Proposal对应每个类别的Mask信息（注意预测的mask大小都是28×28）。并且这里输入的Proposals都是正样本（在Fast R-CNN阶段采样得到的）， 对应的GT信息（box， cls）也是知道的。</p><p>如下图所示， 假设通过RPN得到了一个Proposal（图中黑色的矩形框）， 通过RoIAlign后得到对应的特征信息（shape为14×14×c）。接着通过Mask  Branch预测每个类别的Mask信息得到图中的logits（logits通过sigmoid激活函数后， 所有的值都被映射到0和1之间）。通过Fast R-CNN分支正负样本匹配过程我们能够知道该Proposal的GT类别为猫（cat）， 所以将logits中对应类别猫的预测mask（shape为28×28）提取出来。然后根据Proposal在原图对应的GT上裁剪并缩放到28×28大小，得到图中的GT mask（对应目标区域为1， 背景区域为0），最后计算logits中预测类别为猫的mask与GT amsk的BCELoss（BinaryCrossEntropyLoss）即可。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/Mask_r-cnn_1.jpg" alt="Mask_r-cnn_1"></p><h4 id="Mask-Branch预测使用"><a href="#Mask-Branch预测使用" class="headerlink" title="Mask Branch预测使用"></a>Mask Branch预测使用</h4><p>在真正预测推理的时候， 输入Mask分支的目标是由Fast R-CNN分支提供的。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/Mask_r-cnn_1_1.jpg" alt="Mask_r-cnn_1_1"></p><p>如上图所示， 通过Fast R-CNN分支， 我们能够得到最终的预测目标框架框信息以及类别信息。接着将目标边界框信息提供给Mask分支就能预测得到该目标的logits信息。再根据Fast R-CNN分支提供的类别信息将logits对应类别的Mask信息提取出来，即针对该目标预测的Mask信息（shape为28x28， 由于通过sigmoid激活函数， 数值在0都1之间）， 然后利用双线性插值将Mask缩放到预测目标框大小， 并放到原图对应区域。接着通过设置的阈值(默认为0.5)将mask转换成一张二值图，比如预测值大于0.5的区域设置为前景剩下区域设置为背景。现在对预测的每个目标就可以在原图中绘制出边界框信息， 类别信息以及目标Mask信息。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117174543776.png" alt="image-20231117174543776"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h4&gt;&lt;p&gt;Mask R-CNN是2017年发表的文章， 一</summary>
      
    
    
    
    
    <category term="ComputureVision" scheme="https://guudman.github.io/tags/ComputureVision/"/>
    
  </entry>
  
  <entry>
    <title>YoloV5</title>
    <link href="https://guudman.github.io/2023/11/17/YoloV5/"/>
    <id>https://guudman.github.io/2023/11/17/YoloV5/</id>
    <published>2023-11-17T09:46:42.000Z</published>
    <updated>2023-11-29T10:06:36.976Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>YOLOv5项目的作者是<code>Glenn Jocher</code>并不是原<code>Darknet</code>项目的作者<code>Joseph Redmon</code>。并且这个项目至今都没有发表过正式的论文。<a href="https://github.com/ultralytics/yolov5">YOLOV5仓库</a>早在2020年5月就已创建， 如今已迭代多个版本。本文是针对V6.1版本展开的， 下表是V6.1版本中贴出的关于不同大小模型以及输入尺度对应的mAP、推理速度、参数数量以及理论计算的FLOPs。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Model</th><th>size</th><th style="text-align:center">mAPval</th><th>mAPval</th><th>Speed</th><th>Speed</th><th>Speed</th><th>params</th><th>FLOPs</th></tr></thead><tbody><tr><td style="text-align:center"></td><td>pixels</td><td style="text-align:center">0.5:0.95</td><td>0.5</td><td>CPU b1(ms)</td><td>V100 b1(ms)</td><td>V100 b32(ms)</td><td>(M)</td><td>@640 (B)</td></tr><tr><td style="text-align:center">YOLOv5n</td><td>640</td><td style="text-align:center">28</td><td>45.7</td><td>45</td><td>6.3</td><td>0.6</td><td>1.9</td><td>4.5</td></tr><tr><td style="text-align:center">YOLOv5s</td><td>640</td><td style="text-align:center">37.4</td><td>56.8</td><td>98</td><td>6.4</td><td>0.9</td><td>7.2</td><td>16.5</td></tr><tr><td style="text-align:center">YOLOv5m</td><td>640</td><td style="text-align:center">45.4</td><td>64.1</td><td>224</td><td>8.2</td><td>1.7</td><td>21.2</td><td>49</td></tr><tr><td style="text-align:center">YOLOv5l</td><td>640</td><td style="text-align:center">49</td><td>67.3</td><td>430</td><td>10.1</td><td>2.7</td><td>46.5</td><td>109.1</td></tr><tr><td style="text-align:center">YOLOv5x</td><td>640</td><td style="text-align:center">50.7</td><td>68.9</td><td>766</td><td>12.1</td><td>4.8</td><td>86.7</td><td>205.7</td></tr><tr><td style="text-align:center">YOLOv5n6</td><td>1280</td><td style="text-align:center">36</td><td>54.4</td><td>153</td><td>8.1</td><td>2.1</td><td>3.2</td><td>4.6</td></tr><tr><td style="text-align:center">YOLOv5s6</td><td>1280</td><td style="text-align:center">44.8</td><td>63.7</td><td>385</td><td>8.2</td><td>3.6</td><td>12.6</td><td>16.8</td></tr><tr><td style="text-align:center">YOLOv5m6</td><td>1280</td><td style="text-align:center">51.3</td><td>69.3</td><td>887</td><td>11.1</td><td>6.8</td><td>35.7</td><td>50</td></tr><tr><td style="text-align:center">YOLOv5l6</td><td>1280</td><td style="text-align:center">53.7</td><td>71.3</td><td>1784</td><td>15.8</td><td>10.5</td><td>76.8</td><td>111.4</td></tr><tr><td style="text-align:center">YOLOv5x6</td><td>1280</td><td style="text-align:center">55</td><td>72.7</td><td>3136</td><td>26.2</td><td>19.4</td><td>140.7</td><td>209.8</td></tr><tr><td style="text-align:center">+TTA</td><td>1536</td><td style="text-align:center">55.8</td><td>72.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></div><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>网络结构由以下几个部分组成</p><p>Backbone: New CSP-Darknet53</p><p>Neck: SPPF, New CSP-PAN</p><p>Head； YOLOv3 Head</p><p>下面是根据yolov5l.yaml绘制的网络整体结构， YOLOv5针对不同大小（n, s, m, l, x）的网络整体架构都一样， 只不过在每个子模块中采用不同的深度和宽度， 分别对应yaml文件中的depth_multiple和width_multiple参数。另外， 官方出了n, s, m, l, x版本外还有n6, s6, l6, x6，区别在于后者是针对更大分辨率的图片比如1280x1280， 当然结构上也有些差异， 后者会下采样64倍， 采用4个预测特征层， 而前者只会下采样到32倍且采用3个预测特征层。本文只讨论前者，下图为yolov5l。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov5.jpg" alt="yolov5"></p><p>对于YOLOV4， 其实YOLOV5在<strong>Backbone</strong>部分无太大变化。但YOLOV5在v6.0版本相比之前版本有一个很小的改动， 把网络的第一层（原来是Focus模块）换成了一个6×6大小的卷积层。<strong>二者在理论上是等价的</strong>，但对于现有的一些GPU设备（以及相应的优化算法）使用6×6大小的卷积才能够比使用Focus模块更加高效。</p><p>下图是原来的Focus模块（和之前的Swin Transformer中的Patch Merging类似）， 将每个2×2的相邻像素划分为一个patch， 然后将每个patch中相同位置（同一颜色）像素拼接在一起就得到了4个feature map， 然后再接上一个3×3大小的卷积层。这和直接使用一个6×6大小的卷积层等效。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov5_focus.jpg" alt="yolov5_focus"></p><p>在Neck部分的变化还是相比较大， 首先是将SPP换成了SPPF（Glenn Jocher自己设计的）， 二者的作用一样， 但是后者的效率更高。SPP结构如下图所示， 是将输入并行通过多个大小不同的Maxpool， 然后做进一步融合， 能够在一定程度上解决目标多尺度问题。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov5_spp.jpg" alt="yolov5_spp"></p><p>而SPPF结构是将输入串行通过多个5×5大小的MaxPool层， 这里需要注意的是串行两个5x5大小的MaxPool层是和一个9x9大小的MaxPoll层计算结果是一样的， 串行三个5x5大小的MaxPool层是和一个13x13大小的MaxPoll层计算结果是一样的</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/SPPF.jpg" alt="SPPF"></p><h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><p>YOLOv5中使用了多种数据增强</p><p><strong>Mosaic</strong>，将四张图片拼成一张图片</p><p><strong>Copy paste</strong>，将部分目标随机的粘贴到图片中，前提是数据要有<code>segments</code>数据才行，即每个目标的实例分割信息。</p><p><strong>Random affine(Rotation, Scale, Translation and Shear)</strong>，随机进行仿射变换，但根据配置文件里的超参数发现只使用了<code>Scale</code>和<code>Translation</code>即缩放和平移。</p><p><strong>MixUp</strong>，就是将两张图片按照一定的透明度融合在一起，具体有没有用不太清楚，毕竟没有论文，也没有消融实验。代码中只有较大的模型才使用到了<code>MixUp</code>，而且每次只有10%的概率会使用到。</p><p><strong>Albumentations</strong>，主要是做些滤波、直方图均衡化以及改变图片质量等等，我看代码里写的只有安装了<code>albumentations</code>包才会启用，但在项目的<code>requirements.txt</code>文件中<code>albumentations</code>包是被注释掉了的，所以默认不启用。</p><p><strong>Augment HSV(Hue, Saturation, Value)</strong>，随机调整色度，饱和度以及明度。</p><h4 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h4><p>yolov5中使用了多种训练策略， 这里简单总结几点</p><p><strong>Multi-scale training(0.5~1.5x)，</strong>多尺度训练，假设设置输入图片的大小为640 × 640 ，训练时采用尺寸是在0.5 × 640 ∼ 1.5 × 640 之间随机取值，注意取值时取得都是32的整数倍（因为网络会最大下采样32倍）。</p><p><strong>AutoAnchor(For training custom data)</strong>，训练自己数据集时可以根据自己数据集里的目标进行重新聚类生成Anchors模板。</p><p><strong>Warmup and Cosine LR scheduler</strong>，训练前先进行<code>Warmup</code>热身，然后在采用<code>Cosine</code>学习率下降策略</p><p><strong>EMA(Exponential Moving Average)</strong>，可以理解为给训练的参数加了一个动量，让它更新过程更加平滑</p><p><strong>Mixed precision</strong>，混合精度训练，能够减少显存的占用并且加快训练速度，前提是GPU硬件支持。</p><p><strong>Evolve hyper-parameters</strong>，超参数优化，没有炼丹经验的人勿碰，保持默认就好。</p><h4 id="损失计算"><a href="#损失计算" class="headerlink" title="损失计算"></a>损失计算</h4><p>YOLOv5的损失主要是三部分组成。</p><p>Classes loss, 分类损失， 采用的是BCE loss， 注意只计算正样本的分类损失</p><p>Objectness loss, obj损失， 采用的是BCE loss， 这里的obj表示网络预测的目标边界框与GT Box的CIoU， 这里计算的是所有样本的obj损失</p><p>Location loss: 定位损失， 采用的是CIoU loss, 注意只计算正样本的定位损失</p><script type="math/tex; mode=display">Loss = \lambda_1L_{cls} + \lambda_2L_{obj} + \lambda_3L_{loc}</script><p>其中lambda为平衡系数</p><h4 id="平衡不同尺度的损失"><a href="#平衡不同尺度的损失" class="headerlink" title="平衡不同尺度的损失"></a>平衡不同尺度的损失</h4><p>这里是指三个预测特征层（P3,P4,P5)上的obj损失采用不同的权重， 针对预测小目标的预测特征层（P3)采用的权重是4.0， 针对中等目标的预测特征层（P4）采用的权重是1.0, 针对预测大目标的预测特征层（P5）采用的权重是0.4， 这里说的是针对COCO数据集设置的超参数。</p><script type="math/tex; mode=display">L_{obj} = 4.0 \cdot L_{obj}^{medium} + 0.4 \cdot L^{large}_{obj}</script><h4 id><a href="#" class="headerlink" title=" "></a> </h4><h4 id="消除Grid敏感度"><a href="#消除Grid敏感度" class="headerlink" title="消除Grid敏感度"></a>消除Grid敏感度</h4><p>主要是调整目标中心点相对Grid网络的左上角偏移量，下图是YOLOv2， v3的计算公式。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117020720005.png" alt="image-20231117020720005"></p><p>其中</p><p>t_x是网络预测的目标中心x坐标偏移量（相对于网格的左上角）</p><p>t_y是网络预测的目标中心y坐标偏移量（相对于网格的左上角）</p><p>c_x是对应网格左上角的x坐标</p><p>c_y是对应网络左上角的y坐标</p><script type="math/tex; mode=display">\sigma是Sigmoid激活函数， 将预测的偏移量限制到0和1之间， 即预测的中心点不会超出对应的Grid Cell区域。关于预测目标中心点相对Grid网格左上角（c_x, c_y）偏移量为σ(t_x)， σ(t_y)， yolov4作者认为这样不是很合理， 比如当真实目标中心带你非常靠近网格的左上角点【意味着σ(t_x)， σ(t_y)趋近于0】或者右下角点【意味着σ(t_x)， σ(t_y)趋近于1】时， 网络预测值需要负无穷或者正无穷时才能取到，而这种极端的值网络一般无法达到。为解决这个问题， 作者对偏移量进行了缩放，从原来的（0， 1）缩放到（-0.5， 1.5）, 这样网络预测的偏移量就能方便达到0或者1， 因此最终预测的目标中心点b_x, b_y的计算公式为： $$b_x = (2 \cdot \sigma(t_x) - 0.5) + c_x  \\ b_y = (2 \cdot \sigma(t_y) - 0.5) + c_y</script><p>在YOLOv5中除了调整预测Anchor相对Grid网格左上角（c_x, c_y)偏移量外， 还调整了预测目标高宽的计算公式， 之前是：</p><script type="math/tex; mode=display">b_w = p_w \cdot e^{tw} \\ $b_h = p_h \cdot e^{th}</script><p>在YOLOv5调整为：</p><script type="math/tex; mode=display">b_w = p_w \cdot (2 \cdot \sigma(t_w)) ^ 2  \\ b_h = p_h \cdot (2 \cdot \sigma(t_h)) ^ 2</script><p>原来的计算公式并没有对预测目标宽高做限制，这样可能出现梯度爆炸，训练不稳定等问题。</p><h4 id="匹配正样本（Buidl-Targets"><a href="#匹配正样本（Buidl-Targets" class="headerlink" title="匹配正样本（Buidl Targets)"></a>匹配正样本（Buidl Targets)</h4><p>yolov5与yolov4类似， 主要区别在于GT Box与Anchor Templates模板的匹配方式，在yolov4中直接将每个GT Box与对应的Anchor Template模板计算IoU， 只要IoU大于设定的阈值就算匹配成功， 但在YOLOv5中， 作者先去计算每个GT Box与对应的Anchor Template目标的高宽比例， 即</p><script type="math/tex; mode=display">r_w = w_{gt} / w_{at} \\ r_h = h_{gt} / h_{at}</script><p>然后统计这些比例和它们倒数之间的最大值， 这里可以理解成计算GT Box和Anchor Templates分别在宽高一级高度方向的最大差异（当相等的时候比例为1， 差异最小）</p><script type="math/tex; mode=display">r^{max}_{w} = max(r_w, 1/r_w) \\ r^{max}_{h} = max(r_h, 1/r_h)</script><p>接着统计$$r^{max}_w, r^{max}_h之间的最大值， 即宽度和高度方向差异最大的值：</p><script type="math/tex; mode=display">r^{max} = max(r^{max}_w, r^{max}_h)</script><p>如果GT Box和对应的Anchor Template的r^{max}小于阈值anchor_t（在源码中默认设置为4.0）， 即GT Box和对应的Anchor Template的高，宽比例相差不算太大，则将GT Box分配给该Anchor Template模板。为方便理解，做图如下。假设对某个GT Box而言， 其实只要GT Box满足在某个Anchor Template高和宽的x0.25和x4.0倍之间就算匹配成功。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117024711216.png" alt="image-20231117024711216"></p><p>剩下的步骤和YOLOv4中的一致</p><p>将GT投影到对应预测特征层上， 根据GT的中心点定位到对应的cell， 注意图中有三个对应的cell， 因为网络预测中心点的偏移范围已经调整到了（-0.5， 1.5），所以按理说只要Grid Cell左上角点距离GT中心点在（-0.5， 1.5）范围内它们对应的Anchor都能回归到GT的位置处，这样就能让正样本的数量得到大量的扩充。 </p><p>则这个三个Cell对应的AT2和AT3都为正样本。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117030514848.png" alt="image-20231117030514848"></p><p>还需要注意的是， yolov5源码中扩展Cell时只会往上、下、左、右四个方向扩展， 不会往左上、右下、左下、右下方向扩展。下面又给出了一些根据<script type="math/tex">GT_X^{center}, GT_y^{center}</script>的位置扩展的一些cell案例，其中%1表示会取余并保留一位小数。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231117030859200.png" alt="image-20231117030859200"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h4&gt;&lt;p&gt;YOLOv5项目的作者是&lt;code&gt;Glenn </summary>
      
    
    
    
    
    <category term="ComputureVision" scheme="https://guudman.github.io/tags/ComputureVision/"/>
    
  </entry>
  
  <entry>
    <title>Yolov4</title>
    <link href="https://guudman.github.io/2023/11/14/Yolov4/"/>
    <id>https://guudman.github.io/2023/11/14/Yolov4/</id>
    <published>2023-11-14T10:54:43.000Z</published>
    <updated>2023-11-29T10:06:32.621Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>论文原文：</p><p><a href="[arxiv.org/pdf/1506.02640.pdf](https://arxiv.org/pdf/1506.02640.pdf">yolov1原文</a>)</p><p><a href="[arxiv.org/pdf/1612.08242.pdf](https://arxiv.org/pdf/1612.08242.pdf">yolov2原文</a>)</p><p><a href="[arxiv.org/pdf/1804.02767.pdf](https://arxiv.org/pdf/1804.02767.pdf">yolov3原文</a>)</p><p><a href="[arxiv.org/pdf/2004.10934.pdf](https://arxiv.org/pdf/2004.10934.pdf">yolov4原文</a>)</p><p>YOLOV4是2020年AlexEY Bochkovskiy等人发表在CVPR上的一篇文章， 并不是Darknet的原始作者Joseph Redmon发表的， 但这个工作已经被Jose Redmon大佬认可了。如果将YOLOV4和原始的YOLOV3相比较确实有很大的提升， 但和Utralytics版的YOLOV3 SPP相比提升确实不大， 但毕竟Ultralytics的YOLOv3 SPP以及YOLOv5都没有发表过正式的论文， 所以这里先聊聊Alexey Bochkovskit的YOLOV4。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113093117660.png" alt="image-20231113093117660"></p><hr><p>YOLOv4的亮点</p><p>阅读原论文，会发信啊作者就是将当年所有的常用技术罗列了一遍，然后做了一堆消融实验。网络结构</p><p>在论文3.4章节中介绍了YOLOV4网络的具体结构</p><p>Backbone: CSPDarknet53</p><p>Neck: SPP, PAN</p><p>Head: YOLOv3</p><p>相比之前的yolov3, 改进了以下Backbone, 在Darknet53中引入了CSP模块（来自CSPNet）。在Neck部分， 采用了SPP模块（Utralytics版的YOLOv3 SPP就使用到了）以及PAN模块（来自PANet）， head部分使用的还是原来的检测头。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113094007031.png" alt="image-20231113094007031"></p><p>关于SSP（Spatial Pyramid Pooling）模块， SPP就是将特征层分别通过一个池化核大小为5×5， 9×9， 13×13的最大池化层， 最后在通道方向进行concat拼接在做进一步融合， 这样能够在一定程度上解决目标多尺度问题，如下图所示。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov4.jpg" alt="yolov4"></p><p>PAN（Path Aggregation Network）结构其实就是在FPN（从顶到底信息融合）的基础上加上了从底到顶的信息融合， 如下图所示。 </p><p><a href="[arxiv.org/pdf/1803.01534.pdf](https://arxiv.org/pdf/1803.01534.pdf">PAN路径聚合网络论文原文</a>)</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113100223861.png" alt="image-20231113100223861"></p><p>但YOLOv4的PAN结构和原始论文的融合方式又略有差异， 如下图， 图(a)是原始论文的融合方式， 即特征层之间融合时是直接通过相加的方式进行融合的， 但在YOLOV4中是通过在通道方向concat拼接的方式进行融合的。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113100523933.png" alt="image-20231113100523933"></p><hr><h4 id="优化策略"><a href="#优化策略" class="headerlink" title="优化策略"></a>优化策略</h4><p>这里直接说一下训练检测器时作者采用的一些方法。</p><h5 id="Eliminate-grid-sensitivity"><a href="#Eliminate-grid-sensitivity" class="headerlink" title="Eliminate grid sensitivity"></a>Eliminate grid sensitivity</h5><p>在原来的YOLOv3中， 关于计算预测目标中心坐标计算公式是：</p><script type="math/tex; mode=display">b_x = \sigma(t_x) +  c_x</script><script type="math/tex; mode=display">b_y = \sigma(t_y) +  c_y</script><p>其中， tx， ty分别是网络预测的目标中心x、y坐标偏移量（相对于网络的左上角）</p><p>cx, cy分别是对应网络左上角的x、y坐标</p><script type="math/tex; mode=display">\sigma是sigmoid激活函数， 将预测的偏移量控制在0到1之间， 即预测的中心点不会超出赌赢的Grid Cell区域。 ![image-20231113101648651](https://gitee.com/guudman/blog_images/raw/master/image-20231113101648651.png)YOLOv4中作者认为这样不太合理， 比如当真实目标中心点非常靠近网络的左上角或者右下角时， 即预测的偏移趋近于0或趋近于1时， 网络的预测值需要负无穷或正无穷时才能取到， 而这种极端的值网络一般无法达到， 为解决这个问题，作者引入了一个大于1 的缩放系数</script><p>b_x = (\sigma(t_x) \cdot scale_{xy} - \frac{scale_{xy} - 1}{2}) + c_x</p><script type="math/tex; mode=display"></script><p>b_y = (\sigma(t_y) \cdot scale_{xy} - \frac{scale_{xy} - 1}{2}) + c_y</p><p>$$</p><p>通过引入这个系数， 网络的预测值能够达到0或者1。</p><h5 id="Mosaic-data-augmentation"><a href="#Mosaic-data-augmentation" class="headerlink" title="Mosaic data augmentation"></a>Mosaic data augmentation</h5><p>在数据预处理时阿静四张图片拼成一张图片， 增加学习样本的多样性。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113102716181.png" alt="image-20231113102716181"></p><h5 id="IOU-threshold（正样本匹配）"><a href="#IOU-threshold（正样本匹配）" class="headerlink" title="IOU threshold（正样本匹配）"></a>IOU threshold（正样本匹配）</h5><p>在yolov3中针对每一个GT都只分配了一个Anchor, 但在YOLOv4包括之前的YOLOv3 SPP以及YOLOv5中一个GT可以同时分配多个Anchor， 它们是直接使用Anchor模块与GT Boxes进行粗略匹配， 然后在定位到对应cell的对应Anchor。</p><p>针对某个预测特征层采用如下的三种Anchor模块AT 1, AT 2, AT 3</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov4_1.jpg" alt="yolov4_1"></p><p>将每个GT Boxes与每个Anchor模板进行匹配（这里直接将GT和Anchor模板左上角对齐， 然后计算IoU</p><p>如果GT与某个Anchor模块的IoU大于给定的阈值， 则将GT分配给该Anchor模块， 如图中的AT 2</p><p>将GT投影到对应预测特征层上， 根据GT的中心点定位到对应的cell</p><p>则该cell对应的AT2为正样本</p><p>yolov4以及yolov5中关于匹配正样本的方法有些不同， 主要原因是引入了缩放因子scale_xy, 通过缩放后网络预测中心的偏移范围从原来的（0， 1）调整到了（-0.5， -1.5）, 所以对于同一个GT Boxes可以分配更多的Anchor， 即正样本的数量多了。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov4_1_2.jpg" alt="yolov4_1-第 2 页"></p><p>将每个GT Boxes与每个Anchor模板进行匹配（这里直接将GT和Anchor模板左上角对齐， 然后计算IoU, 在yolov4中IoU的阈值设置的是0.213</p><p>如果GT与某个Anchor模板的IoU大于给定的阈值， 则将GT分配给改Anchor模板， 如图中的AT 2</p><p>将GT投影到对应预测特征层上， 根据GT的中心点定位到对应的cell（注意图中有三个对应的cell）</p><p>则这三个cell对应的AT2都为正样本</p><p>为什么图中的GT会定位到3个cell。刚刚说了网络预测中心点的偏移范围已经调整到了（-0.5， 1.5）, 所以按理说只要Grid Cell左上角距离GT中心点在（-0.5， 1.5）范围内它们对应的Anchor都能回归到GT的位置上。在回过头看看上面的例子， GTx^center, GTy^center距离落入的Grid Cell左上角距离都小于0.5，  所以该Grid Cell上方的Cell以及左侧的Cell都满足条件， 即Cell左上角距离GT中心在（-0.5， 1.5）范围内。这样会让正样本的数量得到大量的扩充。但需要注意的是， yolov5源码中扩展cell时只会往上，下，左边，右边四个方向扩展，不会往左上， 右下， 左下， 右下方向扩展。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113114629932.png" alt="image-20231113114629932"></p><h5 id="Optimizer-Anchors"><a href="#Optimizer-Anchors" class="headerlink" title="Optimizer Anchors"></a>Optimizer Anchors</h5><p>在yolov3中使用anchor模板</p><div class="table-container"><table><thead><tr><th>目标类型</th><th>Anchors模板</th></tr></thead><tbody><tr><td>小尺度</td><td>（10×13), (16×30), (33×23)</td></tr><tr><td>中尺度</td><td>（30×61), (62×45), (59×119)</td></tr><tr><td>大尺度</td><td>（116×90), (156×198), (373×326)</td></tr></tbody></table></div><p>在yolov4中针对512×512尺度采用的anchor模板</p><div class="table-container"><table><thead><tr><th>目标类型</th><th>Anchors模板</th></tr></thead><tbody><tr><td>小尺度</td><td>（12×16), (19×36), (40×23)</td></tr><tr><td>中尺度</td><td>（36×75), (76×55), (72×146)</td></tr><tr><td>大尺度</td><td>（142×110), (192×243), (459×401)</td></tr></tbody></table></div><h5 id="CIoU-定位损失"><a href="#CIoU-定位损失" class="headerlink" title="CIoU(定位损失)"></a>CIoU(定位损失)</h5><p>在yolov3中定位损失采用的是MSE损失， 但在yolov4中采用的是CIoU损失。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231113150754303.png" alt="image-20231113150754303"></p><h4 id="CSPDarjnet53网络结构"><a href="#CSPDarjnet53网络结构" class="headerlink" title="CSPDarjnet53网络结构"></a>CSPDarjnet53网络结构</h4><p>CSPDarknet53就是将CSP结构融入了Darknet53中。CSP结构是在CSPNet（Cross Stage Partial Network）论文中提出的， CSP作者说在目标检测任务中使用了CSP结构有如下好处：</p><blockquote><ol><li>Strengthening learning ability of a CNN</li><li>Removing computational bottlenecks</li><li>Reducing memroy costs</li></ol></blockquote><p>即减少网络的计算量以及对显存的占用， 同时保证网络的能力不变或者略微提升。 CSP结构的思想参考原论文绘制的CSPDenseNet， 进入每个stage（一般在下采样后）先将数据划分成两部分， 如下图所示的Part1和Part2。但具体怎么划分呢， 在CSPNet中直接按照通道均分， 但在YOLOv4网络中通过两个1x1的卷积层来实现的。在Part2后跟一堆Blocks然后再通过1×1的卷积层（图中的Transition）。接着将两个分支的信息在通道方向进行Concat拼接， 最后通过1×1的卷积层进一步融合（图中的Transition）。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/cspnd.jpg" alt="cspnd"></p><p>接下来详细分析CSPDarknet53网络的结构， 图中</p><p>k表示卷积核的大小， s代表步距， c代表通过该模块输出的特征层channels， 注意CSPDarknet53 Backbone中所有的激活函数都是Mish激活函数。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov4_2.jpg" alt="yolov4_2"></p><h4 id="yolov4网络结构"><a href="#yolov4网络结构" class="headerlink" title="yolov4网络结构"></a>yolov4网络结构</h4><p>如下图为yolov4网络结构</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/yolov4_3-16999321127172.jpg" alt="yolov4_3"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h4&gt;&lt;p&gt;论文原文：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;[a</summary>
      
    
    
    
    
    <category term="ComputureVision" scheme="https://guudman.github.io/tags/ComputureVision/"/>
    
  </entry>
  
  <entry>
    <title>Grad-CAM</title>
    <link href="https://guudman.github.io/2023/11/12/Grad-CAM/"/>
    <id>https://guudman.github.io/2023/11/12/Grad-CAM/</id>
    <published>2023-11-12T07:23:48.000Z</published>
    <updated>2023-11-29T10:05:30.799Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>论文原文： <a href="https://arxiv.org/pdf/1610.02391.pdf">Grad_CAM</a></p><p>对于常用的深度学习网络（例如CNN， 普遍认为是个黑盒， 可解释性并不强）， 它为什么会这么预测， 它的关注点在哪里， 我们并不知道， 很多科研人员想方设法的探索其内在的联系， 也有很多相关的论文。 这篇Grad-CAM并不是最新的论文， 但是很有参考意义。通过Grad-CAM我们能够绘制出如下的热力图（对于给定类别， 网络到底关注哪些区域）。Grad-CAM（Gradient-weighted Classes Activation Mapping）是CAM（Class Activation Mapping）的升级版， Grad-CAM比CAM更具一般性。但CAM比较致命的问题是需要修改网络结构并重新训练， 而Grad-CAM完美避开了这些问题， 本文不对CAM进行讲解。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231112105641758.png" alt="image-20231112105641758"></p><p>Grad-CAM能够帮助我们分析网络对于某个类别的关注区域， 那么我们通过网络关注的区域能够反过来分析网络是否正确学习到正确的特征或者信息。例如， 作者训练了一个二分类网络， Nurse和Doctor， 如下图， 第一列是预测时输入的原图， 第二列是Biased model（具有偏见的模型）通过Grad-CAM绘制的热力图。 第三列是Unbiased model（不具偏见的模型）通过Grad-CAM绘制的热力图。通过对比发现， Biased model对于Nurse（护士）这个类别关注的是人的性别，可能模型认为Nurse都是女性，很明显这是带有偏见的。比如第二行第二列这个图， 明明是个女doctor， 但bias modek却认为她是Nurse（可能因为模型关注到它是女性）。而Unbiased model关注的是Nurse和Doctor使用的工作器具以及服装， 明显这个更合理。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231112110347160.png" alt="image-20231112110347160"></p><hr><h4 id="Grad-CAM介绍以及实验"><a href="#Grad-CAM介绍以及实验" class="headerlink" title="Grad-CAM介绍以及实验"></a>Grad-CAM介绍以及实验</h4><h5 id="理论介绍"><a href="#理论介绍" class="headerlink" title="理论介绍"></a>理论介绍</h5><p>作者的想法还是比较简单， 参见下图， 我们这里简单看一下Image Classification任务， 首先网络进行正向传播， 得到特征层A（一般指的是最后一个卷积层的输出）和网络预测值y（注意， 这里指的是softmax激活之前的数值）。假设我们想看一下网络针对Tiger Cat这个类别的感兴趣区域， 假设网络针对Tiger Cat类别预测的值为<script type="math/tex">y^c</script>。紧接着对<script type="math/tex">y^c</script>进行反向传播， 能够得到反向传播特征层A的梯度信息<script type="math/tex">A'</script>。通过计算得到针对特征层A每个通道额重要程度， 然后进行加权求和通过ReLU就行了， 最终得到的结果即是Grad-CAM。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231112111240261.png" alt="image-20231112111240261"></p><p>至于为什么这么做， 首先得到的特征层A是网络对原图进行特征提取得到的结果， 越往后的特征层抽象程度越高， 语义信息越丰富，而且利用CNN抽取得到的特能图是能够保留空间信息的（Transformer同样）。所以Grad-CAM在CNN中的A一般都指的是最后一个卷积层的输出（参考下图， 越往后的特征层效果越好）。当然特征层A包含了所有我们感兴趣的语义信息， 但具体哪些语义信息对应哪个我们并不清楚。紧接着通过对类别c的预测值<script type="math/tex">y^c</script>进行反向传播， 得到反传回特征层A的梯度信息A’, 那么A’就是<script type="math/tex">y^c</script>对A求得的偏导， 换句话说， A’代表A中每个元素对<script type="math/tex">y^c</script>的贡献， 贡献越大网络就认为越重要。然后对A’在w， h上求均值就能得到针对A每个通道的重要程度（这里针对类别c而言的）。最后进行简单的加权求和再通过ReLU就能得到文中说的Grad-CAM。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231112112738557.png" alt="image-20231112112738557"></p><p>关于Grad-CAM总结下来就是下面这个公式：</p><script type="math/tex; mode=display">L^c{Grad-CAM}=ReLU(\sum_{k}\alpha^c_kA^k)</script><p>A代表某个特征层， 论文中一般指的是最后一个卷积层输出的特征层</p><p>k代表特征层A中第k个通道（channel）</p><p>c代表类别c</p><script type="math/tex; mode=display">A^k$$代表特征层A中通道k的数据$$\alpha^c_k$$代表针对$$A^k$$的权重。 关于$$\alpha ^c_k</script><script type="math/tex; mode=display">\alpha ^c_k = \frac{1}{Z}\sum_i\sum_{j}\frac{\alpha y^c}{\alpha A^k_{ij}}</script><script type="math/tex; mode=display">y^c$$代表网络针对类别c预测的分数， 注意这里没有经过softmax激活$$A^k_{ij}$$代表特征层A在通道k中坐标ij位置处的数据Z代表特征层的高度×宽度通过上面的公式可知$$\alpha^c$$ 就是通过预测类别c的预测分数$$y^c$$进行反向传播， 然后利用反传到特征层A上的梯度信息计算特征层A每个通道k的重要程度， 然后通过$$\alpha$$对特征A每个通道的数据进行加权求和， 最后通过ReLU激活函数得到Grad-CAM（论文中说ReLU是为了过滤掉Negative pixls， 而Negative pixles很可能归属于其他类别的pixels）。当然一般还要通过一些后处理，插值等方法与原图叠加得到最终的可视化结果。 如下图所示， CNN Extractor代表CNN 特征提取器， GAP代码Global Average Pooling, FC代表全连接层。 ![Grad-cam1](https://gitee.com/guudman/blog_images/raw/master/Grad-cam1.jpg)假设网络正向传播得到的特征层A如图所示（这里为了方便只画了两个channel， 数据都是随机写的）， 针对类别cat的预测值进行反向传播得到针对特征层A的梯度信息A'，接着利用上面的公式（2）计算针对特征层A每个通道的权重， 就是求A'每个通道的均值。</script><p>\alpha ^c_k = \frac{1}{Z}\sum_i\sum_{j}\frac{\alpha y^c}{\alpha A^k_{ij}}</p><script type="math/tex; mode=display">那么有：</script><p>\alpha^{Cat} =\left(<br>\begin{matrix}<br>  \alpha_1^{Cat} \\<br>   \alpha_2^{Cat}<br> \end{matrix}<br> \right) = \left(<br>\begin{matrix}<br>  \frac{1}{3} \\<br>   -\frac{2}{3}<br> \end{matrix}<br> \right)</p><script type="math/tex; mode=display">然后我们再带人公式</script><p>L^c_{Grad-CAM} = ReLU(\sum_k \alpha^c_{k}A^k)</p><script type="math/tex; mode=display">得到对应类别cat的Grad-CAM</script><p>L^{Cat}_{Grad-CAM} = ReLU(\frac{1}{3}\cdot \left(<br>\begin{matrix}<br> 1 &amp; 0&amp;2\\<br> 3 &amp; 5&amp;0\\<br> 1 &amp; 1&amp;1<br> \end{matrix}<br> \right)) +</p><script type="math/tex; mode=display"></script><p> (-\frac{2}{3})\cdot \left(<br>\begin{matrix}<br> 0 &amp; 1&amp;0\\<br> 3 &amp; 1&amp;0\\<br> 1 &amp; 0&amp;1<br> \end{matrix}<br> \right)) \\ </p><script type="math/tex; mode=display"></script><p> =ReLU(\left(<br>\begin{matrix}<br> \frac{1}{3} &amp; -\frac{2}{3}&amp; \frac{2}{3}\\<br> -1 &amp; 1&amp;0\\<br>  -\frac{1}{3} &amp; \frac{1}{3}&amp; -\frac{1}{3}<br> \end{matrix}<br> \right)))</p><script type="math/tex; mode=display">##### 梯度计算实例上面在计算Grad-CAM时， 其实主要是计算正向传播过程中得到的特征层A和反向传播得到的A', 得到特征层A很简单， 大家也经常会提取某个特征层进行分析或特征融合等。但获取A'会相对麻烦点， 计算倒不是难点因为常用的深度学习框架会自动帮我们计算， 只是很少会用到反传的信息。 那么A’究竟是怎么去计算的。下面构建一个非常简单的神经网络， 重要结构是一个卷积层+一个全连接层， 通过这个例子来演示如何计算反向传播过程中某个特征层的梯度。 ![Grad-cam2](https://gitee.com/guudman/blog_images/raw/master/Grad-cam2.jpg)根据上图， 可得output第一个元素的计算公式如下：</script><p>y_1 = f_{fc}(f_{conv2d}(X,W_1),W^1_2)</p><script type="math/tex; mode=display">其中X代表输入(input), $$f_{conv2d}$$表示卷积的计算过程， $$f_{fc}$$表示全连接的计算， $$W_1$$代表卷积层对应的权重（为了方便， 不考虑偏置）， $$W^1_2$$代表全连接层中第一个节点对应的权重。这里先令$$f_{conv2d}$$即卷积层输出的结果为$$O=(O_{11}, O_{12}, O_{21}, O_{22})$$（为方便后续计算， 这里直接展平写成向量形式）分别对应图中的$$(4, 7, 5, 6)^T$$,这里的O是向量， 那么y1的计算公式为：</script><p>y_1 = f_{fc}(O, W_2^1) = O_{11}\cdot W^{11}_2 +  O_{12}\cdot W^{12}_2 +  O_{21}\cdot W^{13}_2 +  O_{22}\cdot W^{14}_2</p><script type="math/tex; mode=display">接着对O进行偏导：</script><p>\frac{\partial y_1}{\partial O} = \frac{\partial y_1}{\partial(O_{11}, O_{12}, O_{21}, O_{22})^T}</p><script type="math/tex; mode=display"></script><p> =(0, 1, 0, 1)^T</p><script type="math/tex; mode=display">接着将上面的结果reshape得到</script><p>\left(<br>\begin{matrix}<br> 0 &amp; 1 \\<br> 0 &amp; 1<br> \end{matrix}<br> \right)</p><p>$$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;论文原文： &lt;a href=&quot;h</summary>
      
    
    
    
    
    <category term="ComputureVision" scheme="https://guudman.github.io/tags/ComputureVision/"/>
    
  </entry>
  
  <entry>
    <title>ConvNeXt</title>
    <link href="https://guudman.github.io/2023/11/12/ConvNeXt/"/>
    <id>https://guudman.github.io/2023/11/12/ConvNeXt/</id>
    <published>2023-11-12T01:02:56.000Z</published>
    <updated>2023-11-29T10:05:02.892Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>论文原文： <a href="[arxiv.org/pdf/2201.03545.pdf](https://arxiv.org/pdf/2201.03545.pdf">ConvNeXt</a>)</p><p>自从Vit（Vision Transformer)在CV领域大放异彩， 越来越多的研究人员开始涌入Transformer中， 2021年在CV领域的文章绝大多数都是基于Transformer的， 比如2021年的ICCV的best paper （Swin transformer）， 而卷积神经网络已经开始慢慢淡出舞台中央。 卷积网络要被Transformer取代 了吗， 也许会在不久的将来。2022年一份月， Facebook AI Research和UC Berkeley一起发表了一篇文章A ConvNet for the 2020s, 在文章中提出了ConVNeXt纯卷积神经网络， 它对标的就是2021年非常火的Swin Transformer， 通过一系列实验对比， 在相同的FLOPs下， ConvNeXt相比Swin Transformer拥有更快的推理速度以及更高的准确率， 在ImageNet 22k上ConvNeXt-XL达到了87.8%的准确率。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110171124691.png" alt="image-20231110171124691"></p><p>仔细阅读这篇文章， 你会发现ConvNeXt使用的都是现有的结构和方法， 无任何结构或者方法上的创新，而且源码也非常精简， 100多行代码就能搭建完成。 相比Swin Transformer简直不要太简单。 之前看Swin transformer时，滑动窗口， 相对位置索引， 不光原理理解起来很吃力，源码也让人绝望（但不可否认Swin Transformer的成功以及巧妙的设计思想）。为什么基于Transformers结构的模型比卷积神经网络要好呢， 论文中的作者认为可鞥就是随着技术的不断发展， 各种新的架构以及优化策略促使Transformer模型的效果更好， 那么使用相同的策略去训练卷积神经网络也能达到相同的效果吗？</p><blockquote><p>In this work, we investigate the architectural distinctions between ConvNets and Transformers and try to identify the confounding variables when comparing the network performance. Our research is intended to bridge the gap between the pre-ViT and post-ViT eras for ConvNets, as well as to test the limits of what a pure ConvNet can achieve.</p></blockquote><h4 id="设计方案"><a href="#设计方案" class="headerlink" title="设计方案"></a>设计方案</h4><p>作者首先利用训练网络Vision Transformers的策略去训练原始的ResNet50模型， 发现比原始效果要好很多， 并将此结果作为后续实验额的基准baseline， 接下来的实验包含如下部分。</p><p>macro design</p><p>ResnetXt</p><p>inverted bottleneck</p><p>large kernel size</p><p>variout layer-wise micro designs</p><blockquote><p>Our starting point is a ResNet-50 model. We first train it with similar training techniques used to train vision Transformers and obtain much improved results compared to the original ResNet-50. This will be our baseline. We then study a series of design decisions which we summarized as 1) macro design, 2) ResNeXt, 3) inverted bottleneck, 4) large kernel size, and 5) various layer-wise micro designs.</p></blockquote><p>下图展现了每个方案对最终结果的影响。 很显然最后得到的ConNetX在相同FLOPs下准确率已经超过了Swin Transformer。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110172201944.png" alt="image-20231110172201944"></p><hr><h4 id="Macro-design"><a href="#Macro-design" class="headerlink" title="Macro design"></a>Macro design</h4><p>在这个部分主要研究两个方面</p><p>Changing stage compute ratio. 在原ResNet网络中， 一般conv4_x（即stage3）堆叠的block的次数是最多的， 如下图中的ResNet50中stage1到stage4堆叠block的次数是（3， 4， 6， 3）比例大概是1:1:2:1，但在Swin Transformer中， 比如Swin-T的比例是1:1:3:1， Swin-L的比例是1:1:9:1。很明显， 在Swin Transformer中， stage3堆叠block的占比更高。所以作者将ResNet50中的堆叠次数由(3, 4, 6, 3)调整成（3， 3， 9，3）,和Swin-T拥有相似的FLOPs。进行调整后， 准确率78.8%提升到了79.4%。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110174426233.png" alt="image-20231110174426233"></p><p>Changing stem to “Patchify”， 在之前的卷积神经网络中， 一般最初始的下采样模块stem一般都是通过一个卷积核大小为7×7步距为2的卷积层以及一个步距为2的最大池化下采样层共同组成。 高和宽都是下采样的4倍。 但在Transformer模型中一般都是通过一个卷积核非常大而且相邻窗口之间没有重叠的（即stride 等于kernel_size)卷积层进行下采样。比如在Swin transformer中采用的是一个卷积核大小为4×4步距为4的卷积层构成patchify, 同样是下采样4倍。所以作者将ResNet中的stem也换成了和Swin Transformer一样的patchify， 替换后准确率从79.4%提升到79.5%， 并且FLOPs也降低了一点。 </p><hr><h4 id="ResNeXt-ify"><a href="#ResNeXt-ify" class="headerlink" title="ResNeXt-ify"></a>ResNeXt-ify</h4><p>接下来作者借鉴了ResNeXt中的组卷积grouped convolution, 因为ResNeXt相比普通的ResNet而言在FLOPs以及accuracy之间做到了更好的平衡。 而作者采用的是更激进的depthwise convolution. 即group数和通道数channel相同。 之前在讲MobileNet时讲解过这个。这样做的另外一个原因是作者认为depthwise convolution和self-attention中的加权求和操作很相似。 </p><blockquote><p>We note that depthwise convolution is similar to the weighted sum operation in self-attention</p></blockquote><p>接着作者将最初的通道数由64调整成96和Swin Transformer保持一致， 最终准确率达到了80.5%。</p><hr><h4 id="Inverted-Bottleneck"><a href="#Inverted-Bottleneck" class="headerlink" title="Inverted Bottleneck"></a>Inverted Bottleneck</h4><p>作者认为Transformer block中的MLP模块非常像MobileNetV2中的Inverted Bottleneck模块， 即两头细中间粗。下图a是ResNet中采用的Bottleneck模块， b是MobileNetV2采用的Inverted Bottleneck模块。c是ConvNeXt采用的是Inverted Bottleneck模块。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/image-20231110175852997.png" alt="image-20231110175852997"></p><p>作者采用Inverted Bottleneck模块后， 在较小的模型上准确率由80.5%提升到了80.6%， 在较大的模型上准确率由81.9%提升到了82.6%。</p><hr><h4 id="Larget-Kernel-Sizes"><a href="#Larget-Kernel-Sizes" class="headerlink" title="Larget Kernel Sizes"></a>Larget Kernel Sizes</h4><p>在Transformer中一般都是对全局做self-attention， 比如Vision Transformer， 即使是Swin Transformer也有7×7大小的窗口。但现在主流的卷积神经网络都是采用3×3大小的窗口， 因为之前VGG论文中说通过堆叠多个3×3的窗口可以替代一个更大的窗口。而且现在的GPU设备针对3×3大小的卷积核做了很多的优化， 所以会更高效， 接着作者做了如下两个改动：</p><p>Moving up depthwise conv layer, 即将depthwise conv模块上移， 原来是1×1 conv-&gt;depthwise conv-&gt;1x1 conv， 现在变成了depthwise conv-&gt;1×1 conv-&gt;1x1 conv，这么做是因为在Transformer中， MAS模块是放在MLP模块之前的， 所以这里进行效仿， 将depthwise conv上移， 这里改动后， 准确率下降到了79.9%, 同时FLOPs也减少了。</p><p>Increasing the kernel size, 接着作者将depthwise conv的卷积核大小由3×3改成了7×7（和swin Transformers一样）， 当然作者也尝试了其他尺寸， 包括3, 5, 7, 9, 11发现取到7时准确率达到了饱和， 并且准确率从79.9%增长到80.6%(7×7)。</p><hr><p>Micro Design</p><p>接下来作者再聚集到更细小的差异， 比如激活函数以及Normalization.</p><p>Replacing ReLU with GELU, 在Transformer中激活函数基本用的都是GELU， 而在卷积神经网络中最常用的是ReLU， 于是作者又将激活函数替换成了GELU， 替换后发现准确率没发生变化。 </p><p>Fewer activation functions, 使用更少的激活函数， 在卷积神经网络中， 一般会在每个卷积层或全连接层后接上一个激活函数， 但在Transformer中并不是每个模块后都跟有激活函数， 比如MLP中只有第一个全连接层后跟了GELU激活函数， 接着作者再ConvNeXt中也减少了激活函数的使用， 如下图所示， 减少后发现准确率增长到了81.3%。</p><p><img src="https://gitee.com/guudman/blog_images/raw/master/SwinTransformerBlock.jpg" alt="SwinTransformerBlock"></p><p>Fewer normalization layers: 使用更少的Normalization. 同样在Transformer中， Normalization使用的也比较少， 作者也减少了ConvNeXt Block中的Normalization层， 只保留了depthwise conv后的Normalization层， 此时准确率达到了81.4%， 已经超过了Swin-T。</p><p>Substituting BN with LN, 将BN替换成LN。Batch Normalization（BN）在卷积神经网络中是非常常用的操作了，它可以加速网络的收敛并减少过拟合（但用的不好也是个大坑）。但在Transformer中基本都用的Layer Normalization（LN），因为最开始Transformer是应用在NLP领域的，BN又不适用于NLP相关任务。接着作者将BN全部替换成了LN，发现准确率还有小幅提升达到了81.5%。</p><p>Separate downsample layers. 单独的下采样层。在ResNet网络中stage2-stage4的下采样都是通过将主分支上3x3的卷积层步距设置成2，捷径分支上1x1的卷积层步距设置成2进行下采样的。但在Swin Transformer中是通过一个单独的Patch Merging实现的。接着作者就为ConvNext网络单独使用了一个下采样层，就是通过一个Laryer Normalization加上一个卷积核大小为2步距为2的卷积层构成。更改后准确率就提升到了82.0%。</p><hr><p>对于ConvNeXt网络， 作者提出了T/S/B/L四个版本， 计算复杂度刚好和Swin Transformer中的T/S/B/L相似。 </p><p>这四个版本的配置如下：</p><blockquote><p>ConvNeXt-T: C = (96, 192, 384, 768), B = (3, 3, 9, 3)<br>ConvNeXt-S: C = (96, 192, 384, 768), B = (3, 3, 27, 3)<br>ConvNeXt-B: C = (128, 256, 512, 1024), B = (3, 3, 27, 3)<br>ConvNeXt-L: C = (192, 384, 768, 1536), B = (3, 3, 27, 3)<br>ConvNeXt-XL: C = (256, 512, 1024, 2048), B = (3, 3, 27, 3)</p></blockquote><p>其中C代表4个stage中输入的通道数， B代表每个stage重复堆叠block的次数。</p><hr><h4 id="ConvNeXt-T结构图"><a href="#ConvNeXt-T结构图" class="headerlink" title="ConvNeXt-T结构图"></a>ConvNeXt-T结构图</h4><p>下图是根据源码绘制的网络结构图， 会发现ConvNeXt Block中还有一个Layer Scale操作， 其实它就是将输入的特征乘以上一个可训练的参数， 该参数就是一个向量， 元素个数与特征层channel相同， 即对每个channel的数据进行缩放。 </p><p><img src="https://gitee.com/guudman/blog_images/raw/master/ConvNeXt.jpg" alt="ConvNeXt"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;

&lt;h4 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h4&gt;&lt;p&gt;论文原文： &lt;a href=&quot;[</summary>
      
    
    
    
    
    <category term="ComputureVision" scheme="https://guudman.github.io/tags/ComputureVision/"/>
    
  </entry>
  
</feed>
